{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "items_set=pickle.load(open('../data/processed_data/item_set','rb'))\n",
    "bundle_item_map=pickle.load(open('../data/processed_data/bundle_item_map','rb'))\n",
    "user_bundle_map=pickle.load(open('../data/processed_data/user_bundle_map','rb'))\n",
    "user_item_map=pickle.load(open('../data/processed_data/user_item_map','rb'))\n",
    "bundle_diversity_map=pickle.load(open('../data/processed_data/bundle_diversity_map','rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "item_data=pickle.load(open('../data/processed_data/all_items','rb'))\n",
    "item_id_lookup = pickle.load(open('../data/processed_data/item_id_lookup','rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "COLD_ITEMS=2600"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "item_data_map=dict()\n",
    "tags_set=set()\n",
    "for item in item_data:\n",
    "    item_data_map[int(item['appid'])]=item\n",
    "    for tag in item['tags']:\n",
    "        tags_set.add(tag)\n",
    "tags_map=dict()\n",
    "for i,tag in enumerate(tags_set):\n",
    "    tags_map[tag]=i\n",
    "def get_feat(tags):\n",
    "    feat=np.zeros(len(tags_map))\n",
    "    for tag in tags:\n",
    "        feat[tags_map[tag]]=1\n",
    "    return feat\n",
    "\n",
    "from sklearn.metrics import jaccard_similarity_score\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "def compute_diversity(app_data):\n",
    "    l=len(app_data)\n",
    "    app_data=[item_id_lookup[d] for d in app_data]\n",
    "    count=0.0\n",
    "    similarity=0.0\n",
    "    for i in range(l):\n",
    "        if app_data[i] in item_data_map:\n",
    "            for j in range(i+1,l):\n",
    "                if app_data[j] in item_data_map:\n",
    "                    count+=1\n",
    "                    similarity+=jaccard_similarity_score(get_feat(item_data_map[app_data[i]]['tags']),\n",
    "                                                         get_feat(item_data_map[app_data[j]]['tags']))\n",
    "    if count>0:\n",
    "        return 1-(similarity/count)\n",
    "    else:\n",
    "        return 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "all_item_data=[]\n",
    "for user,items in user_item_map.items():\n",
    "    for item in items:\n",
    "        all_item_data.append((user,item))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "training_data=[]\n",
    "cold_test_data=[]\n",
    "\n",
    "bundle_set=set()\n",
    "for user,bundles in user_bundle_map.items():\n",
    "    for bundle in bundles:\n",
    "        bundle_set.add(bundle)\n",
    "        is_train=True\n",
    "        for item in bundle_item_map[bundle]:\n",
    "            if item>=COLD_ITEMS:\n",
    "                is_train=False\n",
    "                break\n",
    "        if is_train:\n",
    "            training_data.append((user,bundle))\n",
    "        else:\n",
    "            cold_test_data.append((user,bundle))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "# Training data for items for bpr_item model\n",
    "training_data_2=all_item_data[:int(0.8*len(all_item_data))]\n",
    "test_data_2=all_item_data[int(0.8*len(all_item_data)):]\n",
    "\n",
    "# Training data for bundles for cold bpr model\n",
    "training_data=training_data[:int(0.8*len(training_data))]\n",
    "test_data=training_data[int(0.8*len(training_data)):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def check_tuple(tuple_1, tuple_2, user_bundle_map):\n",
    "    return tuple_1[1] not in user_bundle_map[tuple_2[0]] and tuple_2[1] not in user_bundle_map[tuple_1[0]]\n",
    "\n",
    "def graph_sampling(n_samples, training_data, user_bundle_map):\n",
    "    sgd_users=[]\n",
    "    sgd_pos_items, sgd_neg_items = [], []\n",
    "    i=0\n",
    "    while n_samples>0:\n",
    "        if i%100000==0:\n",
    "            print i\n",
    "        i+=1\n",
    "        tuple_1=training_data[np.random.randint(len(training_data))]\n",
    "        tuple_2=training_data[np.random.randint(len(training_data))]\n",
    "        iteration=100\n",
    "        while not check_tuple(tuple_1, tuple_2, user_bundle_map):\n",
    "            tuple_2=training_data[np.random.randint(len(training_data))]\n",
    "            iteration-=1\n",
    "            if iteration == 0:\n",
    "                break\n",
    "        if iteration==0:\n",
    "            continue   \n",
    "        sgd_neg_items.append(tuple_2[1])\n",
    "        sgd_pos_items.append(tuple_1[1])\n",
    "        sgd_users.append(tuple_1[0])\n",
    "        \n",
    "        sgd_neg_items.append(tuple_1[1])\n",
    "        sgd_pos_items.append(tuple_2[1])\n",
    "        sgd_users.append(tuple_2[0])\n",
    "        n_samples-=2\n",
    "    return sgd_users, sgd_pos_items, sgd_neg_items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "100000\n",
      "200000\n",
      "300000\n",
      "400000\n",
      "500000\n",
      "600000\n",
      "700000\n",
      "800000\n",
      "900000\n",
      "1000000\n",
      "1100000\n",
      "1200000\n",
      "1300000\n",
      "1400000\n",
      "1500000\n",
      "1600000\n",
      "1700000\n",
      "1800000\n",
      "1900000\n",
      "2000000\n",
      "2100000\n",
      "2200000\n",
      "2300000\n",
      "2400000\n",
      "2500000\n",
      "2600000\n",
      "2700000\n",
      "2800000\n",
      "2900000\n",
      "3000000\n",
      "3100000\n",
      "3200000\n",
      "3300000\n",
      "3400000\n",
      "3500000\n",
      "3600000\n",
      "3700000\n",
      "3800000\n",
      "3900000\n",
      "4000000\n",
      "4100000\n",
      "4200000\n",
      "4300000\n",
      "4400000\n",
      "4500000\n",
      "4600000\n",
      "4700000\n",
      "4800000\n",
      "4900000\n",
      "5000000\n",
      "5100000\n",
      "5200000\n",
      "5300000\n",
      "5400000\n",
      "5500000\n",
      "5600000\n",
      "5700000\n",
      "5800000\n",
      "5900000\n",
      "6000000\n",
      "6100000\n",
      "6200000\n",
      "6300000\n",
      "6400000\n",
      "6500000\n",
      "6600000\n",
      "6700000\n",
      "6800000\n",
      "6900000\n",
      "7000000\n",
      "7100000\n",
      "7200000\n",
      "7300000\n",
      "7400000\n",
      "7500000\n",
      "7600000\n",
      "7700000\n",
      "7800000\n",
      "7900000\n",
      "8000000\n",
      "8100000\n",
      "8200000\n",
      "8300000\n",
      "8400000\n",
      "8500000\n",
      "8600000\n",
      "8700000\n",
      "8800000\n",
      "8900000\n",
      "9000000\n",
      "9100000\n",
      "9200000\n",
      "9300000\n",
      "9400000\n",
      "9500000\n",
      "9600000\n",
      "9700000\n",
      "9800000\n",
      "9900000\n",
      "10000000\n",
      "10100000\n",
      "10200000\n",
      "10300000\n",
      "10400000\n",
      "10500000\n",
      "10600000\n",
      "10700000\n",
      "10800000\n"
     ]
    }
   ],
   "source": [
    "sgd_train_users_items, sgd_train_pos_items, sgd_train_neg_items = graph_sampling(len(training_data_2)*30, training_data_2, user_item_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_test_data_items(test_data, train_data):\n",
    "    users=[]\n",
    "    pos_items=[]\n",
    "    neg_items=[]\n",
    "    train_dict, train_users, train_items  = data_to_dict(train_data)\n",
    "    test_dict, test_users, test_items = data_to_dict(test_data)\n",
    "    z = 0\n",
    "    for i,user in enumerate(test_dict.keys()):\n",
    "        if(i%1000==0):\n",
    "            print i\n",
    "\n",
    "        if user in train_users: \n",
    "            for pos_item in test_dict[user]:\n",
    "                if pos_item in train_items:\n",
    "                    for neg_item in train_items:\n",
    "                        if neg_item not in test_dict[user] and neg_item not in train_dict[user]:\n",
    "                            users.append(user)\n",
    "                            pos_items.append(pos_item)\n",
    "                            neg_items.append(neg_item)\n",
    "\n",
    "    return users, pos_items, neg_items\n",
    "\n",
    "\n",
    "def data_to_dict(data):\n",
    "    data_dict = defaultdict(list)\n",
    "    items = set()\n",
    "    for (user, item) in data:\n",
    "        data_dict[user].append(item)\n",
    "        items.add(item)\n",
    "    return data_dict, set(data_dict.keys()), items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1000\n",
      "2000\n",
      "3000\n",
      "4000\n",
      "5000\n",
      "6000\n",
      "7000\n",
      "8000\n"
     ]
    }
   ],
   "source": [
    "test_users_cold, test_pos_items_cold, test_neg_items_cold = get_test_data_items(test_data_2, training_data_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['THEANO_FLAGS']='mode=FAST_RUN,device=gpu,lib.cnmem=0.7,floatX=float32'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR (theano.sandbox.cuda): nvcc compiler not found on $PATH. Check your nvcc installation and try again.\n"
     ]
    }
   ],
   "source": [
    "# theano-bpr\n",
    "#\n",
    "# Copyright (c) 2014 British Broadcasting Corporation\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "# http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "\n",
    "import theano, numpy\n",
    "import theano.tensor as T\n",
    "import time\n",
    "import sys\n",
    "from collections import defaultdict\n",
    "\n",
    "class BPR_Item(object):\n",
    "\n",
    "    def __init__(self, rank, n_users, n_items, lambda_u = 0.0025, lambda_i = 0.0025, lambda_j = 0.00025, lambda_d = 0.0025, lambda_p = 0.00025, lambda_bias = 0.0, learning_rate = 0.05):\n",
    "        \n",
    "        self._rank = rank\n",
    "        self._n_users = n_users\n",
    "        self._n_items = n_items\n",
    "        self._lambda_u = lambda_u\n",
    "        self._lambda_i = lambda_i\n",
    "        self._lambda_j = lambda_j\n",
    "        self._lambda_d = lambda_d\n",
    "        self._lambda_p = lambda_p\n",
    "        self._lambda_bias = lambda_bias\n",
    "        self._learning_rate = learning_rate\n",
    "        self._configure_theano()\n",
    "        self._generate_train_model_function()\n",
    "        self._generate_test_model_function()\n",
    "\n",
    "    def _configure_theano(self):\n",
    "        \"\"\"\n",
    "          Configures Theano to run in fast mode\n",
    "          and using 32-bit floats. \n",
    "        \"\"\"\n",
    "        theano.config.mode = 'FAST_RUN'\n",
    "        theano.config.floatX = 'float32'\n",
    "\n",
    "    def _generate_train_model_function(self):\n",
    "        u = T.lvector('u')\n",
    "        i = T.lvector('i')\n",
    "        j = T.lvector('j')\n",
    "\n",
    "        h = numpy.random.random((self._n_items, self._rank))\n",
    "        b = numpy.random.random(self._n_items)\n",
    "        \n",
    "        self.W = theano.shared(numpy.random.random((self._n_users, self._rank)).astype('float32'), name='W')\n",
    "        self.H = theano.shared(h.astype('float32'), name='H')\n",
    "        self.B = theano.shared(b.astype('float32'), name='B')\n",
    "        \n",
    "        x_ui = T.dot(self.W[u], self.H[i].T).diagonal() + self.B[i]\n",
    "        x_uj = T.dot(self.W[u], self.H[j].T).diagonal() + self.B[j]\n",
    "        x_uij = T.nnet.sigmoid(x_ui-x_uj)\n",
    "        \n",
    "        obj = T.sum(T.log(x_uij) - self._lambda_u * (self.W[u] ** 2).sum(axis=1) - \n",
    "                    self._lambda_i * (self.H[i] ** 2).sum(axis=1) - self._lambda_j * \n",
    "                    (self.H[j] ** 2).sum(axis=1) - self._lambda_bias * \n",
    "                    (self.B[i] ** 2 + self.B[j] ** 2) )\n",
    "       \n",
    "    \n",
    "        cost = - obj\n",
    "\n",
    "        g_cost_W = T.grad(cost=cost, wrt=self.W)\n",
    "        g_cost_H = T.grad(cost=cost, wrt=self.H)\n",
    "        g_cost_B = T.grad(cost=cost, wrt=self.B)\n",
    "\n",
    "        updates = [(self.W, self.W - self._learning_rate * g_cost_W), (self.H, self.H - self._learning_rate * g_cost_H), \n",
    "                   (self.B, self.B - self._learning_rate * g_cost_B) ]\n",
    "\n",
    "        self.train_model = theano.function(inputs=[u, i, j], outputs=cost, updates=updates)\n",
    "\n",
    "    \n",
    "    def train(self, s_users=None, s_pos_items=None, s_neg_items=None, batch_size=1000):\n",
    "        \"\"\"\n",
    "          Trains the BPR Matrix Factorisation model using Stochastic\n",
    "          Gradient Descent and minibatches over `train_data`.\n",
    "\n",
    "          `train_data` is an array of (user_index, item_index) tuples.\n",
    "\n",
    "          We first create a set of random samples from `train_data` for \n",
    "          training, of size `epochs` * size of `train_data`.\n",
    "\n",
    "          We then iterate through the resulting training samples by\n",
    "          batches of length `batch_size`, and run one iteration of gradient\n",
    "          descent for the batch.\n",
    "        \"\"\"\n",
    "        if len(s_pos_items) < batch_size:\n",
    "            sys.stderr.write(\"WARNING: Batch size is greater than number of training samples, switching to a batch size of %s\\n\" % str(len(train_data)))\n",
    "            batch_size = len(s_pos_items)\n",
    "            \n",
    "        sgd_users, sgd_pos_items, sgd_neg_items = s_users, s_pos_items, s_neg_items\n",
    "        n_sgd_samples = len(s_users)\n",
    "        \n",
    "        z = 0\n",
    "        t2 = t1 = t0 = time.time()\n",
    "        while (z+1)*batch_size < n_sgd_samples:\n",
    "            self.train_model(\n",
    "                sgd_users[z*batch_size: (z+1)*batch_size],\n",
    "                sgd_pos_items[z*batch_size: (z+1)*batch_size],\n",
    "                sgd_neg_items[z*batch_size: (z+1)*batch_size]\n",
    "            )\n",
    "            z += 1\n",
    "            t2 = time.time()\n",
    "            sys.stderr.write(\"\\rProcessed %s ( %.2f%% ) in %.4f seconds\" %(str(z*batch_size), 100.0 * float(z*batch_size)/n_sgd_samples, t2 - t1))\n",
    "            sys.stderr.flush()\n",
    "            t1 = t2\n",
    "        if n_sgd_samples > 0:\n",
    "            sys.stderr.write(\"\\nTotal training time %.2f seconds; %e per sample\\n\" % (t2 - t0, (t2 - t0)/n_sgd_samples))\n",
    "            sys.stderr.flush()\n",
    "            \n",
    "    def _generate_test_model_function(self):\n",
    "        \"\"\"\n",
    "          Computes item predictions for `user_index`.\n",
    "          Returns an array of prediction values for each item\n",
    "          in the dataset.\n",
    "        \"\"\"\n",
    "        u = T.lvector('u')\n",
    "        i = T.lvector('i')\n",
    "        j = T.lvector('j')\n",
    "\n",
    "        x_ui = T.dot(self.W[u], self.H[i].T).diagonal() + self.B[i]\n",
    "        x_uj = T.dot(self.W[u], self.H[j].T).diagonal() + self.B[j]\n",
    "        x_uij = x_ui-x_uj\n",
    "        \n",
    "        self.test_model = theano.function(inputs=[u, i, j], outputs=x_uij)\n",
    "   \n",
    "    def test_bundle(self, sgd_users, sgd_pos_items, sgd_neg_items, batch_size=1000):\n",
    "        \"\"\"\n",
    "          Computes the Area Under Curve (AUC) on `test_data`.\n",
    "\n",
    "          `test_data` is an array of (user_index, item_index) tuples.\n",
    "\n",
    "          During this computation we ignore users and items\n",
    "          that didn't appear in the training data, to allow\n",
    "          for non-overlapping training and testing sets.\n",
    "        \"\"\"\n",
    "        \n",
    "        auc_values = []\n",
    "        z = 0\n",
    "        t2 = t1 = t0 = time.time()\n",
    "        n_sgd_samples = len(sgd_users)\n",
    "        while (z+1)*batch_size < n_sgd_samples:\n",
    "            pref_list=self.test_model(\n",
    "                sgd_users[z*batch_size: (z+1)*batch_size],\n",
    "                sgd_pos_items[z*batch_size: (z+1)*batch_size],\n",
    "                sgd_neg_items[z*batch_size: (z+1)*batch_size]\n",
    "            )\n",
    "            z += 1\n",
    "            t2 = time.time()\n",
    "            sys.stderr.write(\"\\rProcessed %s ( %.2f%% ) in %.4f seconds\" %(str(z*batch_size), 100.0 * float(z*batch_size)/n_sgd_samples, t2 - t1))\n",
    "            t1 = t2\n",
    "            \n",
    "            auc = np.sum([1.0 if a>0.0 else 0.0 for a in pref_list])\n",
    "            auc /= batch_size\n",
    "            \n",
    "            auc_values.append(auc)\n",
    "            sys.stderr.write(\"\\rCurrent AUC mean (%s samples): %0.5f\" % (str(z*batch_size), numpy.mean(auc_values)))\n",
    "            sys.stderr.flush()\n",
    "        \n",
    "        sys.stderr.write(\"\\n\")\n",
    "        sys.stderr.flush()\n",
    "        return numpy.mean(auc_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "bpr_item = BPR_Item(8, len(user_item_map.keys()), len(items_set))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed 21671000 ( 100.00% ) in 0.0113 seconds\n",
      "Total training time 265.33 seconds; 1.224321e-05 per sample\n"
     ]
    }
   ],
   "source": [
    "bpr_item.train(s_users=sgd_train_users_items, s_pos_items=sgd_train_pos_items, s_neg_items=sgd_train_neg_items)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Current AUC mean (66000 samples): 0.82629\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.82628787878787879"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bpr_item.test_bundle(test_users_cold, test_pos_items_cold, test_neg_items_cold)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bundle model begins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "100000\n",
      "200000\n",
      "300000\n",
      "400000\n",
      "500000\n",
      "600000\n",
      "700000\n",
      "800000\n",
      "900000\n"
     ]
    }
   ],
   "source": [
    "# Generting training data for bundles through graph sampling.\n",
    "sgd_users, sgd_pos_bundles, sgd_neg_bundles = graph_sampling(len(training_data)*30, training_data, user_bundle_map)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "88\n",
      "2819\n"
     ]
    }
   ],
   "source": [
    "# Determining max bundle size to create bins for N\n",
    "max_bundle_size=0\n",
    "for bundle,items in bundle_item_map.items():\n",
    "    if(len(items)>max_bundle_size):\n",
    "        max_bundle_size=len(items)\n",
    "print max_bundle_size\n",
    "print len(items_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_items(bundle_id, max_bundle_size, index):\n",
    "    item=list(bundle_item_map[bundle_id]);\n",
    "    for i in range(len(item),max_bundle_size):\n",
    "        item.append(index)\n",
    "    return item\n",
    "\n",
    "sgd_pos_items=[get_items(b_id, max_bundle_size, len(items_set)) for b_id in sgd_pos_bundles]\n",
    "sgd_neg_items=[get_items(b_id, max_bundle_size, len(items_set)) for b_id in sgd_neg_bundles]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_diversity_latent(app_data, H):\n",
    "    l=len(app_data)\n",
    "    count=0.0\n",
    "    similarity=0.0\n",
    "    for i in range(l):\n",
    "            for j in range(i+1,l):\n",
    "                    count+=1\n",
    "                    similarity+=cosine_similarity(H[i],H[j])[0,0]\n",
    "    if count>0:\n",
    "        return 1-(similarity/count)\n",
    "    else:\n",
    "        return 0.0\n",
    "\n",
    "bundle_diversity_map=dict()\n",
    "for bundle,items in bundle_item_map.items():\n",
    "    bundle_diversity_map[bundle]=compute_diversity_latent(list(items), bpr_item.H.eval())\n",
    "\n",
    "#bundle_diversity_map=pickle.load(open('../../data/pickle/training_data/game_aus/bpr/bundle_diversity_map','rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sgd_pos_len=[len(bundle_item_map[b_id]) for b_id in sgd_pos_bundles]\n",
    "sgd_neg_len=[len(bundle_item_map[b_id]) for b_id in sgd_neg_bundles]\n",
    "sgd_pos_diversity=[bundle_diversity_map[b_id] for b_id in sgd_pos_bundles]\n",
    "sgd_neg_diversity=[bundle_diversity_map[b_id] for b_id in sgd_neg_bundles]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_test_data_bundles(test_data, train_data, n_items):\n",
    "    users=[]\n",
    "    pos_items=[]\n",
    "    neg_items=[]\n",
    "    n1=[]\n",
    "    n2=[]\n",
    "    pos_diversity=[]\n",
    "    neg_diversity=[]\n",
    "    train_dict, train_users, train_items  = data_to_dict(train_data)\n",
    "    test_dict, test_users, test_items = data_to_dict(test_data)\n",
    "    auc_values = []\n",
    "    z = 0\n",
    "    for i,user in enumerate(test_dict.keys()):\n",
    "        if(i%1000==0):\n",
    "            print i\n",
    "\n",
    "        if user in train_users: \n",
    "            for pos_item in test_dict[user]:\n",
    "                if pos_item in train_items:\n",
    "                    for neg_item in train_items:\n",
    "                        if neg_item not in test_dict[user] and neg_item not in train_dict[user]:\n",
    "                            pos_diversity.append(bundle_diversity_map[pos_item])\n",
    "                            neg_diversity.append(bundle_diversity_map[neg_item])\n",
    "                            users.append(user)\n",
    "                            pos_items.append(pos_item)\n",
    "                            neg_items.append(neg_item)\n",
    "                            n1.append(len(bundle_item_map[pos_item]))\n",
    "                            n2.append(len(bundle_item_map[neg_item]))\n",
    "\n",
    "    pos_items=[get_items(b_id, max_bundle_size, n_items) for b_id in pos_items]\n",
    "    neg_items=[get_items(b_id, max_bundle_size, n_items) for b_id in neg_items]\n",
    "    return users, pos_items, neg_items, n1, n2, pos_diversity, neg_diversity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1000\n",
      "2000\n",
      "3000\n",
      "4000\n"
     ]
    }
   ],
   "source": [
    "test_users, test_pos_items, test_neg_items, test_n1, test_n2, test_pos_diversity, test_neg_diversity= get_test_data_bundles(test_data, training_data, len(items_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_test_data_cold_bundle(test_data, train_data):\n",
    "    users=[]\n",
    "    pos_items=[]\n",
    "    neg_items=[]\n",
    "    n1=[]\n",
    "    n2=[]\n",
    "    pos_diversity=[]\n",
    "    neg_diversity=[]\n",
    "    test_dict, test_users, test_items = data_to_dict(test_data)\n",
    "    auc_values = []\n",
    "    z = 0\n",
    "    for i,user in enumerate(test_dict.keys()):\n",
    "        if(i%500==0):\n",
    "            print i\n",
    "\n",
    "        for pos_item in test_dict[user]:\n",
    "            for neg_item in bundle_set:\n",
    "                if neg_item not in user_bundle_map[user]:\n",
    "                    pos_diversity.append(bundle_diversity_map[pos_item])\n",
    "                    neg_diversity.append(bundle_diversity_map[neg_item])\n",
    "                    users.append(user)\n",
    "                    pos_items.append(pos_item)\n",
    "                    neg_items.append(neg_item)\n",
    "                    n1.append(len(bundle_item_map[pos_item]))\n",
    "                    n2.append(len(bundle_item_map[neg_item]))\n",
    "\n",
    "    pos_items=[get_items(b_id, max_bundle_size, len(items_set)) for b_id in pos_items]\n",
    "    neg_items=[get_items(b_id, max_bundle_size, len(items_set)) for b_id in neg_items]\n",
    "    return users, pos_items, neg_items, n1, n2, pos_diversity, neg_diversity\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "500\n",
      "1000\n",
      "1500\n",
      "2000\n",
      "2500\n",
      "3000\n",
      "3500\n",
      "4000\n",
      "4500\n",
      "5000\n",
      "5500\n",
      "6000\n",
      "6500\n",
      "7000\n",
      "7500\n"
     ]
    }
   ],
   "source": [
    "cold_test_users, cold_test_pos_items, cold_test_neg_items, cold_test_n1, cold_test_n2, cold_test_pos_diversity, cold_test_neg_diversity = get_test_data_cold_bundle(cold_test_data, training_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2819, 8)\n",
      "(2820, 8)\n",
      "(2819,)\n",
      "(2820,)\n"
     ]
    }
   ],
   "source": [
    "print np.shape(bpr_item.H.eval())\n",
    "H_item=bpr_item.H.eval()\n",
    "H_item = np.concatenate((H_item,np.zeros((1,8))),axis=0)\n",
    "H_item=np.array(H_item).astype('float32')\n",
    "print np.shape(H_item)\n",
    "\n",
    "print np.shape(bpr_item.B.eval())\n",
    "B_item=bpr_item.B.eval()\n",
    "B_item = np.append(B_item,0)\n",
    "B_item=np.array(B_item).astype('float32')\n",
    "print np.shape(B_item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import theano, numpy\n",
    "import theano.tensor as T\n",
    "import time\n",
    "import sys\n",
    "from collections import defaultdict\n",
    "\n",
    "class BPR_Cold(object):\n",
    "\n",
    "    def __init__(self, rank, bundle_size, n_users, n_items, lambda_u = 0.0025, lambda_i = 0.0025, lambda_j = 0.00025, lambda_d = 0.0025, lambda_p = 0.00025, lambda_bias = 0.0, learning_rate = 0.05):\n",
    "        \n",
    "        self._rank = rank\n",
    "        self._bundle_rank = bundle_size + 1\n",
    "        self._n_users = n_users\n",
    "        self._n_items = n_items\n",
    "        self._lambda_u = lambda_u\n",
    "        self._lambda_i = lambda_i\n",
    "        self._lambda_j = lambda_j\n",
    "        self._lambda_d = lambda_d\n",
    "        self._lambda_p = lambda_p\n",
    "        self._lambda_bias = lambda_bias\n",
    "        self._learning_rate = learning_rate\n",
    "        self._configure_theano()\n",
    "        self._generate_train_model_item_function()\n",
    "        self._generate_test_model_function()\n",
    "\n",
    "    def _configure_theano(self):\n",
    "        theano.config.mode = 'FAST_RUN'\n",
    "        theano.config.floatX = 'float32'\n",
    "    \n",
    "    def _generate_train_model_item_function(self):\n",
    "        u = T.lvector('u')\n",
    "        i = T.lmatrix('i')\n",
    "        j = T.lmatrix('j')\n",
    "        n1 = T.lvector('n1')\n",
    "        n2 = T.lvector('n2')\n",
    "        di = T.dvector('di')\n",
    "        dj = T.dvector('dj')\n",
    "        \n",
    "        self.W1 = bpr_item.W\n",
    "        self.H1 = theano.shared(H_item.astype('float32'), name='H')\n",
    "        self.B1 = theano.shared(B_item.astype('float32'), name='B')\n",
    "        \n",
    "        \n",
    "        self.M1 = theano.shared(numpy.random.random((self._rank, self._rank)).astype('float64'), name='M1')\n",
    "        self.M2 = theano.shared(numpy.random.random((self._rank, self._rank)).astype('float64'), name='M2')\n",
    "        self.K = theano.shared(numpy.random.rand(), name='K')\n",
    "        self.D = theano.shared(numpy.random.rand(), name='D')\n",
    "        self.N = theano.shared(numpy.random.random(self._bundle_rank).astype('float32'), name='N')\n",
    "        \n",
    "        x_ui = T.dot(T.dot(self.W1[u],self.M2), T.dot(self.M1, self.H1[i].sum(axis=1).T/n1)).diagonal() + self.K*(self.B1[i].T/n1).T.sum(axis=1) + self.D*di  + self.N[n1] \n",
    "        x_uj = T.dot(T.dot(self.W1[u],self.M2), T.dot(self.M1, self.H1[j].sum(axis=1).T/n2)).diagonal() + self.K*(self.B1[j].T/n2).T.sum(axis=1)  + self.D*dj + self.N[n2] \n",
    "        \n",
    "        x_uij = T.nnet.sigmoid(x_ui-x_uj)\n",
    "        obj = T.sum(T.log(x_uij) - self._lambda_u * (self.M1 ** 2).sum() - \\\n",
    "                    self._lambda_u * (self.M2 ** 2).sum()  - self._lambda_d * (self.K**2) - self._lambda_d * (self.D**2)\n",
    "                    -self._lambda_p * (self.N[n2]**2) - self._lambda_p * (self.N[n1]**2))\n",
    "        cost = - obj\n",
    "\n",
    "        g_cost_M1 = T.grad(cost=cost, wrt=self.M1)\n",
    "        g_cost_M2 = T.grad(cost=cost, wrt=self.M2)\n",
    "        g_cost_K = T.grad(cost=cost, wrt=self.K)\n",
    "        g_cost_N = T.grad(cost=cost, wrt=self.N)\n",
    "        g_cost_D = T.grad(cost=cost, wrt=self.D)\n",
    "        \n",
    "        updates = [(self.M1, self.M1 - self._learning_rate * .001* g_cost_M1), (self.M2, self.M2 - self._learning_rate *.001* g_cost_M2), \n",
    "                   (self.K, self.K - self._learning_rate * .001*g_cost_K), (self.N, self.N - self._learning_rate *g_cost_N),\n",
    "                  (self.D, self.D - self._learning_rate * g_cost_D)]\n",
    "\n",
    "        self.train_model_item = theano.function(inputs=[u, i, j, n1, n2, di, dj], outputs=cost, updates=updates)\n",
    "\n",
    "    \n",
    "    def train(self, s_users=None, s_pos_items=None, s_neg_items=None, s_pos_len=None, s_neg_len=None,\n",
    "             s_pos_diversity=None, s_neg_diversity=None,batch_size=1000):\n",
    "        \n",
    "        if len(s_users) < batch_size:\n",
    "            sys.stderr.write(\"WARNING: Batch size is greater than number of training samples, switching to a batch size of %s\\n\" % str(len(train_data)))\n",
    "            batch_size = len(s_users)\n",
    "        \n",
    "        sgd_users, sgd_pos_items, sgd_neg_items = s_users, s_pos_items, s_neg_items\n",
    "        n_sgd_samples = len(s_users)\n",
    "\n",
    "        z = 0\n",
    "        t2 = t1 = t0 = time.time()\n",
    "        while (z+1)*batch_size < n_sgd_samples:\n",
    "            \n",
    "            self.train_model_item(\n",
    "                sgd_users[z*batch_size: (z+1)*batch_size],\n",
    "                sgd_pos_items[z*batch_size: (z+1)*batch_size],\n",
    "                sgd_neg_items[z*batch_size: (z+1)*batch_size],\n",
    "                s_pos_len[z*batch_size: (z+1)*batch_size],\n",
    "                s_neg_len[z*batch_size: (z+1)*batch_size],\n",
    "                s_pos_diversity[z*batch_size: (z+1)*batch_size],\n",
    "                s_neg_diversity[z*batch_size: (z+1)*batch_size],\n",
    "            )\n",
    "            z += 1\n",
    "            t2 = time.time()\n",
    "            sys.stderr.write(\"\\rProcessed %s ( %.2f%% ) in %.4f seconds\" %(str(z*batch_size), 100.0 * float(z*batch_size)/n_sgd_samples, t2 - t1))\n",
    "            sys.stderr.flush()\n",
    "            t1 = t2\n",
    "        if n_sgd_samples > 0:\n",
    "            sys.stderr.write(\"\\nTotal training time %.2f seconds; %e per sample\\n\" % (t2 - t0, (t2 - t0)/n_sgd_samples))\n",
    "            sys.stderr.flush()\n",
    "    \n",
    "    def _generate_test_model_function(self):\n",
    "        u = T.lvector('u')\n",
    "        i = T.lmatrix('i')\n",
    "        j = T.lmatrix('j')\n",
    "        n1 = T.lvector('n1')\n",
    "        n2 = T.lvector('n2')\n",
    "        di = T.dvector('di')\n",
    "        dj = T.dvector('dj')\n",
    "        \n",
    "        x_ui = T.dot(T.dot(self.W1[u],self.M2), T.dot(self.M1, self.H1[i].sum(axis=1).T/n1)).diagonal() + self.K*(self.B1[i].T/n1).T.sum(axis=1) + self.D*di + self.N[n1]\n",
    "        x_uj = T.dot(T.dot(self.W1[u],self.M2), T.dot(self.M1, self.H1[j].sum(axis=1).T/n2)).diagonal() + self.K*(self.B1[j].T/n2).T.sum(axis=1) + self.D*dj + self.N[n2] \n",
    "        \n",
    "        x_uij = x_ui-x_uj\n",
    "        self.test_model = theano.function(inputs=[u, i, j, n1, n2, di, dj], outputs=x_uij)\n",
    "        \n",
    "    def test_bundle(self, sgd_users, sgd_pos_items, sgd_neg_items, s_pos_len, s_neg_len, s_pos_diversity, s_neg_diversity, batch_size=1000):\n",
    "        \n",
    "        auc_values = []\n",
    "        z = 0\n",
    "        t2 = t1 = t0 = time.time()\n",
    "        n_sgd_samples = len(sgd_users)\n",
    "        while (z+1)*batch_size < n_sgd_samples:\n",
    "            pref_list=self.test_model(\n",
    "                sgd_users[z*batch_size: (z+1)*batch_size],\n",
    "                sgd_pos_items[z*batch_size: (z+1)*batch_size],\n",
    "                sgd_neg_items[z*batch_size: (z+1)*batch_size],\n",
    "                s_pos_len[z*batch_size: (z+1)*batch_size],\n",
    "                s_neg_len[z*batch_size: (z+1)*batch_size],\n",
    "                s_pos_diversity[z*batch_size: (z+1)*batch_size],\n",
    "                s_neg_diversity[z*batch_size: (z+1)*batch_size]\n",
    "            )\n",
    "            z += 1\n",
    "            t2 = time.time()\n",
    "            sys.stderr.write(\"\\rProcessed %s ( %.2f%% ) in %.4f seconds\" %(str(z*batch_size), 100.0 * float(z*batch_size)/n_sgd_samples, t2 - t1))\n",
    "            t1 = t2\n",
    "            \n",
    "            auc = np.sum([1.0 if a>0.0 else 0.0 for a in pref_list])\n",
    "            auc /= batch_size\n",
    "            \n",
    "            auc_values.append(auc)\n",
    "            sys.stderr.write(\"\\rCurrent AUC mean (%s samples): %0.5f\" % (str(z*batch_size), numpy.mean(auc_values)))\n",
    "            sys.stderr.flush()\n",
    "        \n",
    "        sys.stderr.write(\"\\n\")\n",
    "        sys.stderr.flush()\n",
    "        return numpy.mean(auc_values)\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "bpr_cold = BPR_Cold(8, max_bundle_size, len(user_bundle_map.keys()), len(items_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed 1902000 ( 100.00% ) in 0.0348 seconds\n",
      "Total training time 67.38 seconds; 3.542165e-05 per sample\n"
     ]
    }
   ],
   "source": [
    "bpr_cold.train(s_users=sgd_users, s_pos_items=sgd_pos_items, s_neg_items=sgd_neg_items, \n",
    "          s_pos_len=sgd_pos_len, s_neg_len=sgd_neg_len, s_pos_diversity=sgd_pos_diversity, s_neg_diversity=sgd_neg_diversity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Current AUC mean (2292000 samples): 0.90083\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.90082722513088997"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bpr_cold.test_bundle(test_users, test_pos_items, test_neg_items, test_n1, test_n2, test_pos_diversity, test_neg_diversity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Current AUC mean (1851000 samples): 0.72622\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.72621663965424099"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bpr_cold.test_bundle(cold_test_users, cold_test_pos_items, cold_test_neg_items, cold_test_n1, cold_test_n2, cold_test_pos_diversity, cold_test_neg_diversity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bundle Generation Through Greedy Model Begins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def generate_bundle(items_set, user, initial_size = 3, max_iteration = 1000, sample_size = 5):\n",
    "    current_bundle = np.random.choice(list(items_set), initial_size)\n",
    "    \n",
    "    T=1000.0\n",
    "    \n",
    "    iteration = 0\n",
    "    while iteration < max_iteration:\n",
    "        iteration+=1\n",
    "        curr_diversity = compute_diversity(current_bundle)\n",
    "        user_set=[]\n",
    "        pos_item_set=[]\n",
    "        actual_item_set=[]\n",
    "        neg_item_set=[]\n",
    "        pos_item_count=[]\n",
    "        neg_item_count=[]\n",
    "        pos_diversity=[]\n",
    "        neg_diversity=[]\n",
    "        \n",
    "        \n",
    "        candidate_items = set(np.random.choice(list(items_set), sample_size))\n",
    "        \n",
    "        for item in current_bundle:\n",
    "            if item in candidate_items:\n",
    "                candidate_items.remove(item)\n",
    "        \n",
    "    \n",
    "        #Generating new bundles by adding and removing new items  \n",
    "        for cand_item in candidate_items:\n",
    "            #Add an item case\n",
    "            if len(current_bundle)<10:\n",
    "                user_set.append(user)    \n",
    "                neg_item_set.append(add_bogus_items(current_bundle , max_bundle_size, len(items_set)))\n",
    "                neg_item_count.append(len(current_bundle))\n",
    "                neg_diversity.append(curr_diversity)         \n",
    "                new_bundle=list(current_bundle)\n",
    "                new_bundle.append(cand_item)\n",
    "                pos_item_count.append(len(new_bundle))\n",
    "                pos_diversity.append(compute_diversity(new_bundle))\n",
    "                actual_item_set.append(new_bundle)\n",
    "                pos_item_set.append(add_bogus_items(new_bundle , max_bundle_size, len(items_set)))\n",
    "\n",
    "            # Replace an item case\n",
    "            for curr_item in current_bundle:\n",
    "                user_set.append(user)\n",
    "                \n",
    "                neg_item_set.append(add_bogus_items(current_bundle , max_bundle_size, len(items_set)))\n",
    "                neg_item_count.append(len(current_bundle))\n",
    "                neg_diversity.append(curr_diversity)\n",
    "                \n",
    "                new_bundle=list(current_bundle)\n",
    "                new_bundle.append(cand_item)\n",
    "                new_bundle.remove(curr_item)\n",
    "                pos_item_set.append(add_bogus_items(new_bundle , max_bundle_size, len(items_set)))\n",
    "                actual_item_set.append(new_bundle)\n",
    "                pos_item_count.append(len(new_bundle))\n",
    "                pos_diversity.append(compute_diversity(new_bundle))\n",
    "         \n",
    "        \n",
    "        # Remove an item case\n",
    "        if len(current_bundle)>2:\n",
    "            for curr_item in current_bundle:\n",
    "                user_set.append(user)\n",
    "\n",
    "                neg_item_set.append(add_bogus_items(current_bundle , max_bundle_size, len(items_set)))\n",
    "                neg_item_count.append(len(current_bundle))\n",
    "                neg_diversity.append(curr_diversity)\n",
    "\n",
    "                new_bundle=list(current_bundle)\n",
    "                new_bundle.remove(curr_item)\n",
    "                actual_item_set.append(new_bundle)\n",
    "                pos_item_set.append(add_bogus_items(new_bundle , max_bundle_size, len(items_set)))\n",
    "                pos_item_count.append(len(new_bundle))\n",
    "                pos_diversity.append(compute_diversity(new_bundle))\n",
    "        \n",
    "                \n",
    "        pref_score = bpr_cold.test_model(user_set, pos_item_set, neg_item_set, pos_item_count, \n",
    "                                    neg_item_count, pos_diversity, neg_diversity)\n",
    "                                    \n",
    "        #print pref_score, pos_item_count, neg_item_count\n",
    "        index = np.argmax(pref_score)\n",
    "        #print \"Pref Score \", pref_score[index]\n",
    "        if(pref_score[index]>0):\n",
    "            current_bundle = actual_item_set[index]\n",
    "        else:\n",
    "            prob = np.exp(pref_score[index]/T)\n",
    "            if prob < .00001:\n",
    "                break\n",
    "            if np.random.rand() < prob:\n",
    "                current_bundle = actual_item_set[index]\n",
    "        T=T*0.9\n",
    "    print iteration\n",
    "    return current_bundle "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def add_bogus_items(bundle, max_bundle_size, index):\n",
    "    item=list(bundle);\n",
    "    for i in range(len(item),max_bundle_size):\n",
    "        item.append(index)\n",
    "    return item\n",
    "\n",
    "def remove_bogus_items(bundle, max_bundle_size, index):\n",
    "    item=list(bundle);\n",
    "    i=0\n",
    "    while i< len(bundle):\n",
    "        if bundle[i]==index:\n",
    "            break\n",
    "        i+=1\n",
    "    return bundle[:i]\n",
    "\n",
    "def get_bundle_rank(user, new_bundle, bundle_item_map, bundle_diversity_map):\n",
    "    user_set=[]\n",
    "    pos_item_set=[]\n",
    "    neg_item_set=[]\n",
    "    pos_item_count=[]\n",
    "    neg_item_count=[]\n",
    "    pos_diversity=[]\n",
    "    neg_diversity=[]\n",
    "    \n",
    "    bundle_diversity=compute_diversity(new_bundle)\n",
    "    for bundle_id,bundle in bundle_item_map.items():\n",
    "        user_set.append(user)\n",
    "        pos_item_set.append(add_bogus_items(bundle, max_bundle_size, len(items_set)))\n",
    "        neg_item_set.append(add_bogus_items(new_bundle, max_bundle_size, len(items_set)))\n",
    "        pos_item_count.append(len(bundle))\n",
    "        neg_item_count.append(len(new_bundle))\n",
    "        pos_diversity.append(bundle_diversity_map[bundle_id])\n",
    "        neg_diversity.append(bundle_diversity)\n",
    "        \n",
    "    pref_score = bpr_cold.test_model(user_set, pos_item_set, neg_item_set, pos_item_count, \n",
    "                                    neg_item_count, pos_diversity, neg_diversity)\n",
    "  \n",
    "    rank = np.sum([1.0 if p>0 else 0.0 for p in pref_score])\n",
    "    return rank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "73\n",
      "[270, 786]\n",
      "Rank of user 0 : 1, Size of bundle : 2, Bundles purchased : 6 Aggregate diversity: 2 Score: 2.000000, Average bundle size: 2.000000\n",
      "83\n",
      "[117, 2497]\n",
      "Rank of user 1 : 1, Size of bundle : 2, Bundles purchased : 15 Aggregate diversity: 4 Score: 2.000000, Average bundle size: 2.000000\n",
      "83\n",
      "[836, 1806]\n",
      "Rank of user 2 : 0, Size of bundle : 2, Bundles purchased : 1 Aggregate diversity: 6 Score: 1.666667, Average bundle size: 2.000000\n",
      "77\n",
      "[2510, 181]\n",
      "Rank of user 3 : 0, Size of bundle : 2, Bundles purchased : 7 Aggregate diversity: 8 Score: 1.500000, Average bundle size: 2.000000\n",
      "76\n",
      "[98, 81]\n",
      "Rank of user 4 : 0, Size of bundle : 2, Bundles purchased : 8 Aggregate diversity: 10 Score: 1.400000, Average bundle size: 2.000000\n",
      "87\n",
      "[1854, 2190]\n",
      "Rank of user 5 : 3, Size of bundle : 2, Bundles purchased : 5 Aggregate diversity: 12 Score: 1.833333, Average bundle size: 2.000000\n",
      "76\n",
      "[1563, 2432]\n",
      "Rank of user 6 : 0, Size of bundle : 2, Bundles purchased : 4 Aggregate diversity: 14 Score: 1.714286, Average bundle size: 2.000000\n",
      "77\n",
      "[465, 1560]\n",
      "Rank of user 7 : 0, Size of bundle : 2, Bundles purchased : 3 Aggregate diversity: 16 Score: 1.625000, Average bundle size: 2.000000\n",
      "77\n",
      "[2510, 214]\n",
      "Rank of user 8 : 0, Size of bundle : 2, Bundles purchased : 7 Aggregate diversity: 17 Score: 1.555556, Average bundle size: 2.000000\n",
      "81\n",
      "[16, 456]\n",
      "Rank of user 9 : 2, Size of bundle : 2, Bundles purchased : 8 Aggregate diversity: 19 Score: 1.700000, Average bundle size: 2.000000\n",
      "79\n",
      "[1264, 2432]\n",
      "Rank of user 10 : 0, Size of bundle : 2, Bundles purchased : 5 Aggregate diversity: 20 Score: 1.636364, Average bundle size: 2.000000\n",
      "79\n",
      "[164, 205]\n",
      "Rank of user 11 : 0, Size of bundle : 2, Bundles purchased : 2 Aggregate diversity: 22 Score: 1.583333, Average bundle size: 2.000000\n",
      "80\n",
      "[21, 2029]\n",
      "Rank of user 12 : 6, Size of bundle : 2, Bundles purchased : 10 Aggregate diversity: 24 Score: 2.000000, Average bundle size: 2.000000\n",
      "85\n",
      "[482, 450]\n",
      "Rank of user 13 : 1, Size of bundle : 2, Bundles purchased : 1 Aggregate diversity: 26 Score: 2.000000, Average bundle size: 2.000000\n",
      "93\n",
      "[2619, 645]\n",
      "Rank of user 14 : 3, Size of bundle : 2, Bundles purchased : 8 Aggregate diversity: 28 Score: 2.133333, Average bundle size: 2.000000\n",
      "73\n",
      "[129, 13]\n",
      "Rank of user 15 : 0, Size of bundle : 2, Bundles purchased : 8 Aggregate diversity: 30 Score: 2.062500, Average bundle size: 2.000000\n",
      "106\n",
      "[508, 64, 450]\n",
      "Rank of user 16 : 3, Size of bundle : 3, Bundles purchased : 4 Aggregate diversity: 32 Score: 2.176471, Average bundle size: 2.058824\n",
      "85\n",
      "[234, 1982]\n",
      "Rank of user 17 : 3, Size of bundle : 2, Bundles purchased : 1 Aggregate diversity: 34 Score: 2.277778, Average bundle size: 2.055556\n",
      "93\n",
      "[484, 2405]\n",
      "Rank of user 18 : 4, Size of bundle : 2, Bundles purchased : 3 Aggregate diversity: 36 Score: 2.421053, Average bundle size: 2.052632\n",
      "79\n",
      "[901, 2147]\n",
      "Rank of user 19 : 0, Size of bundle : 2, Bundles purchased : 3 Aggregate diversity: 38 Score: 2.350000, Average bundle size: 2.050000\n",
      "98\n",
      "[2768, 723, 1539]\n",
      "Rank of user 20 : 3, Size of bundle : 3, Bundles purchased : 3 Aggregate diversity: 41 Score: 2.428571, Average bundle size: 2.095238\n",
      "78\n",
      "[2247, 73]\n",
      "Rank of user 21 : 0, Size of bundle : 2, Bundles purchased : 3 Aggregate diversity: 43 Score: 2.363636, Average bundle size: 2.090909\n",
      "83\n",
      "[90, 2563]\n",
      "Rank of user 22 : 0, Size of bundle : 2, Bundles purchased : 4 Aggregate diversity: 45 Score: 2.304348, Average bundle size: 2.086957\n",
      "102\n",
      "[499, 379, 1011]\n",
      "Rank of user 23 : 4, Size of bundle : 3, Bundles purchased : 2 Aggregate diversity: 48 Score: 2.416667, Average bundle size: 2.125000\n",
      "76\n",
      "[2043, 1569]\n",
      "Rank of user 24 : 0, Size of bundle : 2, Bundles purchased : 1 Aggregate diversity: 50 Score: 2.360000, Average bundle size: 2.120000\n",
      "81\n",
      "[26, 157]\n",
      "Rank of user 25 : 2, Size of bundle : 2, Bundles purchased : 3 Aggregate diversity: 52 Score: 2.384615, Average bundle size: 2.115385\n",
      "76\n",
      "[1068, 1546]\n",
      "Rank of user 26 : 0, Size of bundle : 2, Bundles purchased : 2 Aggregate diversity: 54 Score: 2.333333, Average bundle size: 2.111111\n",
      "89\n",
      "[252, 508]\n",
      "Rank of user 27 : 1, Size of bundle : 2, Bundles purchased : 1 Aggregate diversity: 55 Score: 2.321429, Average bundle size: 2.107143\n",
      "90\n",
      "[1249, 499]\n",
      "Rank of user 28 : 3, Size of bundle : 2, Bundles purchased : 1 Aggregate diversity: 56 Score: 2.379310, Average bundle size: 2.103448\n",
      "79\n",
      "[2768, 1249]\n",
      "Rank of user 29 : 3, Size of bundle : 2, Bundles purchased : 4 Aggregate diversity: 56 Score: 2.433333, Average bundle size: 2.100000\n",
      "97\n",
      "[343, 996, 935]\n",
      "Rank of user 30 : 5, Size of bundle : 3, Bundles purchased : 1 Aggregate diversity: 59 Score: 2.548387, Average bundle size: 2.129032\n",
      "97\n",
      "[2190, 1013, 2123]\n",
      "Rank of user 31 : 3, Size of bundle : 3, Bundles purchased : 3 Aggregate diversity: 61 Score: 2.593750, Average bundle size: 2.156250\n",
      "76\n",
      "[770, 1542]\n",
      "Rank of user 32 : 0, Size of bundle : 2, Bundles purchased : 3 Aggregate diversity: 63 Score: 2.545455, Average bundle size: 2.151515\n",
      "77\n",
      "[2486, 843]\n",
      "Rank of user 33 : 0, Size of bundle : 2, Bundles purchased : 8 Aggregate diversity: 65 Score: 2.500000, Average bundle size: 2.147059\n",
      "79\n",
      "[706, 1406]\n",
      "Rank of user 34 : 4, Size of bundle : 2, Bundles purchased : 5 Aggregate diversity: 67 Score: 2.571429, Average bundle size: 2.142857\n",
      "84\n",
      "[57, 1791]\n",
      "Rank of user 35 : 4, Size of bundle : 2, Bundles purchased : 1 Aggregate diversity: 69 Score: 2.638889, Average bundle size: 2.138889\n",
      "76\n",
      "[2737, 1562]\n",
      "Rank of user 36 : 1, Size of bundle : 2, Bundles purchased : 1 Aggregate diversity: 71 Score: 2.621622, Average bundle size: 2.135135\n",
      "83\n",
      "[2190, 2545]\n",
      "Rank of user 37 : 3, Size of bundle : 2, Bundles purchased : 3 Aggregate diversity: 72 Score: 2.657895, Average bundle size: 2.131579\n",
      "74\n",
      "[4, 499]\n",
      "Rank of user 38 : 0, Size of bundle : 2, Bundles purchased : 11 Aggregate diversity: 73 Score: 2.615385, Average bundle size: 2.128205\n",
      "101\n",
      "[697, 194, 1001]\n",
      "Rank of user 39 : 5, Size of bundle : 3, Bundles purchased : 5 Aggregate diversity: 76 Score: 2.700000, Average bundle size: 2.150000\n",
      "84\n",
      "[22, 204]\n",
      "Rank of user 40 : 4, Size of bundle : 2, Bundles purchased : 1 Aggregate diversity: 78 Score: 2.756098, Average bundle size: 2.146341\n",
      "82\n",
      "[98, 15]\n",
      "Rank of user 41 : 1, Size of bundle : 2, Bundles purchased : 2 Aggregate diversity: 79 Score: 2.738095, Average bundle size: 2.142857\n",
      "91\n",
      "[80, 1823]\n",
      "Rank of user 42 : 3, Size of bundle : 2, Bundles purchased : 1 Aggregate diversity: 81 Score: 2.767442, Average bundle size: 2.139535\n",
      "78\n",
      "[163, 6]\n",
      "Rank of user 43 : 0, Size of bundle : 2, Bundles purchased : 5 Aggregate diversity: 83 Score: 2.727273, Average bundle size: 2.136364\n",
      "93\n",
      "[2360, 2261]\n",
      "Rank of user 44 : 1, Size of bundle : 2, Bundles purchased : 1 Aggregate diversity: 85 Score: 2.711111, Average bundle size: 2.133333\n",
      "81\n",
      "[98, 640]\n",
      "Rank of user 45 : 4, Size of bundle : 2, Bundles purchased : 5 Aggregate diversity: 86 Score: 2.760870, Average bundle size: 2.130435\n",
      "86\n",
      "[1823, 2443]\n",
      "Rank of user 46 : 4, Size of bundle : 2, Bundles purchased : 3 Aggregate diversity: 87 Score: 2.808511, Average bundle size: 2.127660\n",
      "105\n",
      "[110, 1892, 952]\n",
      "Rank of user 47 : 4, Size of bundle : 3, Bundles purchased : 1 Aggregate diversity: 90 Score: 2.854167, Average bundle size: 2.145833\n",
      "75\n",
      "[1872, 1843]\n",
      "Rank of user 48 : 3, Size of bundle : 2, Bundles purchased : 4 Aggregate diversity: 92 Score: 2.877551, Average bundle size: 2.142857\n",
      "79\n",
      "[749, 231]\n",
      "Rank of user 49 : 4, Size of bundle : 2, Bundles purchased : 1 Aggregate diversity: 94 Score: 2.920000, Average bundle size: 2.140000\n",
      "80\n",
      "[2768, 2405]\n",
      "Rank of user 50 : 3, Size of bundle : 2, Bundles purchased : 3 Aggregate diversity: 94 Score: 2.941176, Average bundle size: 2.137255\n",
      "98\n",
      "[1600, 281, 2409]\n",
      "Rank of user 51 : 3, Size of bundle : 3, Bundles purchased : 1 Aggregate diversity: 97 Score: 2.961538, Average bundle size: 2.153846\n",
      "78\n",
      "[1538, 810]\n",
      "Rank of user 52 : 0, Size of bundle : 2, Bundles purchased : 3 Aggregate diversity: 99 Score: 2.924528, Average bundle size: 2.150943\n",
      "86\n",
      "[822, 1956]\n",
      "Rank of user 53 : 3, Size of bundle : 2, Bundles purchased : 2 Aggregate diversity: 101 Score: 2.944444, Average bundle size: 2.148148\n",
      "81\n",
      "[2732, 2174]\n",
      "Rank of user 54 : 0, Size of bundle : 2, Bundles purchased : 2 Aggregate diversity: 103 Score: 2.909091, Average bundle size: 2.145455\n",
      "79\n",
      "[141, 478]\n",
      "Rank of user 55 : 0, Size of bundle : 2, Bundles purchased : 3 Aggregate diversity: 105 Score: 2.875000, Average bundle size: 2.142857\n",
      "88\n",
      "[409, 508]\n",
      "Rank of user 56 : 3, Size of bundle : 2, Bundles purchased : 2 Aggregate diversity: 106 Score: 2.894737, Average bundle size: 2.140351\n",
      "96\n",
      "[749, 797, 82]\n",
      "Rank of user 57 : 3, Size of bundle : 3, Bundles purchased : 3 Aggregate diversity: 108 Score: 2.913793, Average bundle size: 2.155172\n",
      "79\n",
      "[2496, 2525]\n",
      "Rank of user 58 : 0, Size of bundle : 2, Bundles purchased : 4 Aggregate diversity: 110 Score: 2.881356, Average bundle size: 2.152542\n",
      "75\n",
      "[2768, 808]\n",
      "Rank of user 59 : 0, Size of bundle : 2, Bundles purchased : 5 Aggregate diversity: 111 Score: 2.850000, Average bundle size: 2.150000\n",
      "83\n",
      "[2466, 7]\n",
      "Rank of user 60 : 0, Size of bundle : 2, Bundles purchased : 3 Aggregate diversity: 113 Score: 2.819672, Average bundle size: 2.147541\n",
      "79\n",
      "[2405, 645]\n",
      "Rank of user 61 : 0, Size of bundle : 2, Bundles purchased : 3 Aggregate diversity: 113 Score: 2.790323, Average bundle size: 2.145161\n",
      "95\n",
      "[1270, 499]\n",
      "Rank of user 62 : 2, Size of bundle : 2, Bundles purchased : 5 Aggregate diversity: 114 Score: 2.793651, Average bundle size: 2.142857\n",
      "81\n",
      "[645, 24]\n",
      "Rank of user 63 : 3, Size of bundle : 2, Bundles purchased : 5 Aggregate diversity: 115 Score: 2.812500, Average bundle size: 2.140625\n",
      "80\n",
      "[2499, 760]\n",
      "Rank of user 64 : 4, Size of bundle : 2, Bundles purchased : 1 Aggregate diversity: 117 Score: 2.846154, Average bundle size: 2.138462\n",
      "95\n",
      "[2806, 2043, 1691]\n",
      "Rank of user 65 : 0, Size of bundle : 3, Bundles purchased : 1 Aggregate diversity: 119 Score: 2.818182, Average bundle size: 2.151515\n",
      "82\n",
      "[2048, 1546]\n",
      "Rank of user 66 : 3, Size of bundle : 2, Bundles purchased : 3 Aggregate diversity: 120 Score: 2.835821, Average bundle size: 2.149254\n",
      "80\n",
      "[80, 1872]\n",
      "Rank of user 67 : 3, Size of bundle : 2, Bundles purchased : 1 Aggregate diversity: 120 Score: 2.852941, Average bundle size: 2.147059\n",
      "86\n",
      "[2029, 1263]\n",
      "Rank of user 68 : 3, Size of bundle : 2, Bundles purchased : 3 Aggregate diversity: 121 Score: 2.869565, Average bundle size: 2.144928\n",
      "88\n",
      "[2430, 1593]\n",
      "Rank of user 69 : 5, Size of bundle : 2, Bundles purchased : 1 Aggregate diversity: 123 Score: 2.914286, Average bundle size: 2.142857\n",
      "86\n",
      "[2778, 612]\n",
      "Rank of user 70 : 0, Size of bundle : 2, Bundles purchased : 3 Aggregate diversity: 125 Score: 2.887324, Average bundle size: 2.140845\n",
      "82\n",
      "[339, 2444]\n",
      "Rank of user 71 : 0, Size of bundle : 2, Bundles purchased : 4 Aggregate diversity: 127 Score: 2.861111, Average bundle size: 2.138889\n",
      "90\n",
      "[2236, 69]\n",
      "Rank of user 72 : 0, Size of bundle : 2, Bundles purchased : 1 Aggregate diversity: 129 Score: 2.835616, Average bundle size: 2.136986\n",
      "102\n",
      "[2546, 664, 1308]\n",
      "Rank of user 73 : 5, Size of bundle : 3, Bundles purchased : 2 Aggregate diversity: 132 Score: 2.878378, Average bundle size: 2.148649\n",
      "83\n",
      "[2771, 1474]\n",
      "Rank of user 74 : 3, Size of bundle : 2, Bundles purchased : 3 Aggregate diversity: 134 Score: 2.893333, Average bundle size: 2.146667\n",
      "94\n",
      "[157, 181]\n",
      "Rank of user 75 : 4, Size of bundle : 2, Bundles purchased : 3 Aggregate diversity: 134 Score: 2.921053, Average bundle size: 2.144737\n",
      "89\n",
      "[1249, 1909]\n",
      "Rank of user 76 : 0, Size of bundle : 2, Bundles purchased : 1 Aggregate diversity: 135 Score: 2.896104, Average bundle size: 2.142857\n",
      "89\n",
      "[1922, 2417]\n",
      "Rank of user 77 : 0, Size of bundle : 2, Bundles purchased : 1 Aggregate diversity: 137 Score: 2.871795, Average bundle size: 2.141026\n",
      "84\n",
      "[2049, 1208]\n",
      "Rank of user 78 : 3, Size of bundle : 2, Bundles purchased : 2 Aggregate diversity: 139 Score: 2.886076, Average bundle size: 2.139241\n",
      "88\n",
      "[2222, 1068]\n",
      "Rank of user 79 : 3, Size of bundle : 2, Bundles purchased : 3 Aggregate diversity: 140 Score: 2.900000, Average bundle size: 2.137500\n",
      "83\n",
      "[1838, 797]\n",
      "Rank of user 80 : 3, Size of bundle : 2, Bundles purchased : 1 Aggregate diversity: 141 Score: 2.913580, Average bundle size: 2.135802\n",
      "85\n",
      "[98, 142]\n",
      "Rank of user 81 : 1, Size of bundle : 2, Bundles purchased : 3 Aggregate diversity: 142 Score: 2.902439, Average bundle size: 2.134146\n",
      "83\n",
      "[15, 1552]\n",
      "Rank of user 82 : 0, Size of bundle : 2, Bundles purchased : 4 Aggregate diversity: 143 Score: 2.879518, Average bundle size: 2.132530\n",
      "83\n",
      "[2048, 1546]\n",
      "Rank of user 83 : 3, Size of bundle : 2, Bundles purchased : 3 Aggregate diversity: 143 Score: 2.892857, Average bundle size: 2.130952\n",
      "92\n",
      "[1277, 1854]\n",
      "Rank of user 84 : 0, Size of bundle : 2, Bundles purchased : 1 Aggregate diversity: 144 Score: 2.870588, Average bundle size: 2.129412\n",
      "86\n",
      "[26, 1132]\n",
      "Rank of user 85 : 1, Size of bundle : 2, Bundles purchased : 1 Aggregate diversity: 145 Score: 2.860465, Average bundle size: 2.127907\n",
      "83\n",
      "[98, 971]\n",
      "Rank of user 86 : 5, Size of bundle : 2, Bundles purchased : 6 Aggregate diversity: 146 Score: 2.896552, Average bundle size: 2.126437\n",
      "84\n",
      "[1561, 397]\n",
      "Rank of user 87 : 3, Size of bundle : 2, Bundles purchased : 5 Aggregate diversity: 148 Score: 2.909091, Average bundle size: 2.125000\n",
      "89\n",
      "[129, 602]\n",
      "Rank of user 88 : 0, Size of bundle : 2, Bundles purchased : 12 Aggregate diversity: 149 Score: 2.887640, Average bundle size: 2.123596\n",
      "87\n",
      "[22, 614]\n",
      "Rank of user 89 : 4, Size of bundle : 2, Bundles purchased : 1 Aggregate diversity: 150 Score: 2.911111, Average bundle size: 2.122222\n",
      "77\n",
      "[2263, 236]\n",
      "Rank of user 90 : 1, Size of bundle : 2, Bundles purchased : 5 Aggregate diversity: 152 Score: 2.901099, Average bundle size: 2.120879\n",
      "95\n",
      "[2333, 514, 1512]\n",
      "Rank of user 91 : 3, Size of bundle : 3, Bundles purchased : 1 Aggregate diversity: 155 Score: 2.913043, Average bundle size: 2.130435\n",
      "90\n",
      "[181, 1823]\n",
      "Rank of user 92 : 3, Size of bundle : 2, Bundles purchased : 1 Aggregate diversity: 155 Score: 2.924731, Average bundle size: 2.129032\n",
      "93\n",
      "[2619, 1224]\n",
      "Rank of user 93 : 4, Size of bundle : 2, Bundles purchased : 1 Aggregate diversity: 156 Score: 2.946809, Average bundle size: 2.127660\n",
      "95\n",
      "[1493, 1262, 2100]\n",
      "Rank of user 94 : 0, Size of bundle : 3, Bundles purchased : 2 Aggregate diversity: 159 Score: 2.926316, Average bundle size: 2.136842\n",
      "85\n",
      "[473, 2213]\n",
      "Rank of user 95 : 3, Size of bundle : 2, Bundles purchased : 6 Aggregate diversity: 161 Score: 2.937500, Average bundle size: 2.135417\n",
      "86\n",
      "[450, 1208]\n",
      "Rank of user 96 : 3, Size of bundle : 2, Bundles purchased : 3 Aggregate diversity: 161 Score: 2.948454, Average bundle size: 2.134021\n",
      "90\n",
      "[1823, 2768]\n",
      "Rank of user 97 : 3, Size of bundle : 2, Bundles purchased : 3 Aggregate diversity: 161 Score: 2.959184, Average bundle size: 2.132653\n",
      "77\n",
      "[1983, 129]\n",
      "Rank of user 98 : 0, Size of bundle : 2, Bundles purchased : 5 Aggregate diversity: 162 Score: 2.939394, Average bundle size: 2.131313\n",
      "76\n",
      "[69, 22]\n",
      "Rank of user 99 : 0, Size of bundle : 2, Bundles purchased : 1 Aggregate diversity: 162 Score: 2.920000, Average bundle size: 2.130000\n"
     ]
    }
   ],
   "source": [
    "pred_score=[]\n",
    "b_size=[]\n",
    "aggregate_diversity=set()\n",
    "\n",
    "for user in sorted(user_bundle_map.keys())[:100]:\n",
    "    new_bundle = generate_bundle(items_set, user, 4, 1000,3)\n",
    "    rank = get_bundle_rank(user, new_bundle, bundle_item_map, bundle_diversity_map)\n",
    "    purchased_bundles = len(user_bundle_map[user])\n",
    "    aggregate_diversity=aggregate_diversity.union(set(new_bundle))\n",
    "    print new_bundle\n",
    "    pred_score.append(rank)\n",
    "    b_size.append(len(new_bundle)*1.0)\n",
    "    print 'Rank of user %d : %d, Size of bundle : %d, Bundles purchased : %d Aggregate diversity: %d Score: %f, Average bundle size: %f' %(user, \n",
    "                                                                                 rank, \n",
    "                                                                                 len(new_bundle),                                 \n",
    "                                                                                 purchased_bundles,\n",
    "                                                                                 len(aggregate_diversity),\n",
    "                                                                                 1.0+np.mean(pred_score),\n",
    "                                                                                 np.mean(b_size))\n",
    "    \n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
