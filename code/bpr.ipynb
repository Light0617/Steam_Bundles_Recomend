{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import csv\n",
    "from lib import *\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['THEANO_FLAGS']='mode=FAST_RUN,device=gpu,lib.cnmem=0.7,floatX=float32'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bundle model begins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# folder = '../data/new_data'\n",
    "# bundle_item_map=pickle.load(open(folder + '/bundle_item_map.pkl','rb'))\n",
    "# bundle_discount_map = pickle.load(open(folder +'/bundle_discount_map.pkl','rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "folder = '../data/train_test'\n",
    "bpr_item = pickle.load(open(folder +'/bpr_item.pkl','rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "H_item=bpr_item.H.eval()\n",
    "B_item=bpr_item.B.eval()\n",
    "W_item=bpr_item.W.eval()\n",
    "TG_item=bpr_item.TG.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder = '../data/train_test'\n",
    "sgd_users = pickle.load(open(folder +  \"/sgd_users.pkl\", \"rb\" ))\n",
    "sgd_pos_items = pickle.load(open(folder +  \"/sgd_pos_items.pkl\", \"rb\" ))\n",
    "sgd_neg_items = pickle.load(open(folder +  \"/sgd_neg_items.pkl\", \"rb\" ))\n",
    "sgd_pos_bundles = pickle.load(open(folder +  \"/sgd_pos_bundles.pkl\", \"rb\" ))\n",
    "sgd_neg_bundles = pickle.load(open(folder +  \"/sgd_neg_bundles.pkl\", \"rb\" ))\n",
    "sgd_pos_len = pickle.load(open(folder +  \"/sgd_pos_len.pkl\", \"rb\" ))\n",
    "sgd_neg_len = pickle.load(open(folder +  \"/sgd_neg_len.pkl\", \"rb\" ))\n",
    "sgd_pos_diversity = pickle.load(open(folder +  \"/sgd_pos_diversity.pkl\", \"rb\" ))\n",
    "sgd_neg_diversity = pickle.load(open(folder +  \"/sgd_neg_diversity.pkl\", \"rb\" ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_users = pickle.load(open(folder +  \"/test_users.pkl\", \"rb\" ))\n",
    "test_pos_items = pickle.load(open(folder +  \"/test_pos_items.pkl\", \"rb\" ))\n",
    "test_neg_items = pickle.load(open(folder +  \"/test_neg_items.pkl\", \"rb\" ))\n",
    "test_pos_bundles = pickle.load(open(folder +  \"/test_pos_bundles.pkl\", \"rb\" ))\n",
    "test_neg_bundles = pickle.load(open(folder +  \"/test_neg_bundles.pkl\", \"rb\" ))\n",
    "test_n1 = pickle.load(open(folder +  \"/test_n1.pkl\", \"rb\" ))\n",
    "test_n2 = pickle.load(open(folder +  \"/test_n2.pkl\", \"rb\" ))\n",
    "test_pos_diversity = pickle.load(open(folder +  \"/test_pos_diversity.pkl\", \"rb\" ))\n",
    "test_neg_diversity =pickle.load(open(folder +  \"/test_neg_diversity.pkl\", \"rb\" ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "sgd_pos_items = pickle.load(open(folder +  \"/sgd_pos_items.pkl\", \"rb\" ))\n",
    "sgd_neg_items = pickle.load(open(folder +  \"/sgd_neg_items.pkl\", \"rb\" ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_pos_bundles = pickle.load(open(folder +  \"/test_pos_bundles.pkl\", \"rb\" ))\n",
    "test_neg_bundles = pickle.load(open(folder +  \"/test_neg_bundles.pkl\", \"rb\" ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "item_id_lookup =  pickle.load(open(\"../data/new_data/item_id_lookup.pkl\", \"rb\"))\n",
    "user_item_map =  pickle.load(open(\"../data/new_data/user_item_map.pkl\", \"rb\"))\n",
    "tag_array = pickle.load(open(\"../data/new_data/tag_array\", \"rb\"))\n",
    "bundle_discount_map = pickle.load(open(\"../data/new_data/bundle_discount_map.pkl\", \"rb\"))\n",
    "bundle_item_map = pickle.load(open(\"../data/new_data/bundle_item_map.pkl\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_bundle_size = np.max([len(bundle_item_map[key]) for key in bundle_item_map])\n",
    "num_items = len(item_id_lookup)\n",
    "num_users = len(user_item_map)\n",
    "num_tags = len(tag_array.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# folder = '../data/train_test'\n",
    "# pickle.dump(num_items, open(folder +  \"/num_items\", \"wb\" ))\n",
    "# pickle.dump(max_bundle_size, open(folder +  \"/max_bundle_size\", \"wb\" ))\n",
    "# pickle.dump(num_users, open(folder +  \"/num_users\", \"wb\" ))\n",
    "pickle.dump(num_tags, open(folder +  \"/num_tags\", \"wb\" ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_items = pickle.load(open(folder +  \"/num_items\", \"r\" ))\n",
    "max_bundle_size = pickle.load(open(folder +  \"/max_bundle_size\", \"r\" ))\n",
    "num_users = pickle.load(open(folder +  \"/num_users\", \"r\" ))\n",
    "num_tags = pickle.load(open(folder +  \"/num_tags\", \"r\" ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print len(sgd_neg_dicount), len(sgd_pos_diversity), len(sgd_pos_dicount)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2518, 10)\n",
      "(2518,)\n",
      "(2516, 327)\n"
     ]
    }
   ],
   "source": [
    "H_item = np.concatenate((H_item,np.zeros((1,np.shape(H_item)[1]))),axis=0)\n",
    "H_item=np.array(H_item).astype('float32')\n",
    "print np.shape(H_item)\n",
    "\n",
    "\n",
    "B_item = np.append(B_item,0)\n",
    "B_item=np.array(B_item).astype('float32')\n",
    "print np.shape(B_item)\n",
    "\n",
    "A_item = np.concatenate((TG_item,np.zeros((1,np.shape(TG_item)[1]))),axis=0)\n",
    "A_item=np.array(A_item).astype('float32')\n",
    "print np.shape(A_item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#bpr_cold = BPR_Cold(10, max_bundle_size, len(user_bundle_map.keys()), num_items, W_item, H_item, B_item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# #bpr_cold.train(s_users=sgd_users, s_pos_items=sgd_pos_items, s_neg_items=sgd_neg_items, \n",
    "#          s_pos_len=sgd_pos_len, s_neg_len=sgd_neg_len, s_pos_diversity=sgd_pos_diversity,\n",
    "#                s_neg_diversity=sgd_neg_diversity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#bpr_cold.test_bundle(test_users, test_pos_items, test_neg_items, test_n1, test_n2, test_pos_diversity, test_neg_diversity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print '0.68585732689210954'\n",
    "bundle_discount_array = [bundle_discount_map[i] for i in range(len(bundle_discount_map)) if i ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print 0.73830120101137786 + 'discount devrivate'\n",
    "print 0.72651723027375203 + 'no discount derivaite'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "bundle_discount_arr = [bundle_discount_map[i] for i in range(len(bundle_discount_map))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BPR_Buncle1(object):\n",
    "\n",
    "    def __init__(self, rank, bundle_size, n_users, n_items, n_tags, bundle_discount_arr, lambda_u = 0.0025, lambda_i = 0.0025, lambda_j = 0.00025, lambda_d = 0.0025, lambda_p = 0.00025, lambda_A = 0.01, lambda_bias = 0.0, learning_rate = 0.05):\n",
    "        \n",
    "        self._rank = rank\n",
    "        self._bundle_rank = bundle_size + 1\n",
    "        self._n_users = n_users\n",
    "        self._n_items = n_items\n",
    "        self._n_bundle = len(bundle_discount_map)\n",
    "        self.bundle_discount_arr = theano.shared(np.array(bundle_discount_arr))\n",
    "        self._lambda_u = lambda_u\n",
    "        self._lambda_i = lambda_i\n",
    "        self._lambda_j = lambda_j\n",
    "        self._lambda_d = lambda_d\n",
    "        self._lambda_p = lambda_p\n",
    "        self._lambda_bias = lambda_bias\n",
    "        self._lambda_A = lambda_A\n",
    "        self._learning_rate = learning_rate\n",
    "        \n",
    "        self._n_tags = n_tags\n",
    "        self._configure_theano()\n",
    "        self._generate_train_model_item_function()\n",
    "        self._generate_test_model_function()\n",
    "\n",
    "    def _configure_theano(self):\n",
    "        theano.config.mode = 'FAST_RUN'\n",
    "        theano.config.floatX = 'float32'\n",
    "    \n",
    "    def _generate_train_model_item_function(self):\n",
    "        u = T.lvector('u')\n",
    "        i = T.lmatrix('i')\n",
    "        j = T.lmatrix('j')\n",
    "        n1 = T.lvector('n1')#num of items in pos bundle\n",
    "        n2 = T.lvector('n2')\n",
    "        di = T.dvector('di')#cb\n",
    "        dj = T.dvector('dj')#cb\n",
    "        bi = T.lvector('bi')\n",
    "        bj = T.lvector('bj')\n",
    "        \n",
    "        self.W1 = bpr_item.W #Pu\n",
    "        self.H1 = theano.shared(H_item.astype('float32'), name='H')#Qi\n",
    "        self.B1 = theano.shared(B_item.astype('float32'), name='B')#Bi\n",
    "        self.TG1 = theano.shared(A_item.astype('float32'), name='TG')\n",
    "        self.theta1 = bpr_item.theta\n",
    "        \n",
    "        self.M1 = theano.shared(numpy.random.random((self._rank, self._rank)).astype('float64'), name='M1')#w\n",
    "        self.M2 = theano.shared(numpy.random.random((self._rank, self._rank)).astype('float64'), name='M2')#u\n",
    "        self.BDu = theano.shared(numpy.random.random((self._n_users)).astype('float64'), name='bdu')#u\n",
    "        self.K = theano.shared(numpy.random.rand(), name='K')#k\n",
    "        self.D = theano.shared(numpy.random.rand(), name='D')#C\n",
    "        self.N = theano.shared(numpy.random.random(self._bundle_rank).astype('float32'), name='N')#Nb\n",
    "        self.A1 = theano.shared(numpy.random.random((self._n_tags, self._n_tags)), name='A1')\n",
    "        self.A2 = theano.shared(numpy.random.random((self._n_tags, self._n_tags)), name='A2')\n",
    "                                \n",
    "        \n",
    "        x_ui = T.dot(T.dot(self.W1[u],self.M2), T.dot(self.M1, self.H1[i].sum(axis=1).T/n1)).diagonal() + self.K*(self.B1[i].T/n1).T.sum(axis=1) + self.N[n1] + self.D*di + T.dot(T.dot(self.theta1[u],self.A1), T.dot(self.A2, self.TG1[i].sum(axis=1).T/n1)).diagonal()\\\n",
    "                + self.BDu[u] * self.bundle_discount_arr[bi]\n",
    "        x_uj = T.dot(T.dot(self.W1[u],self.M2), T.dot(self.M1, self.H1[j].sum(axis=1).T/n2)).diagonal() + self.K*(self.B1[j].T/n2).T.sum(axis=1) + self.N[n2] + self.D*dj + T.dot(T.dot(self.theta1[u],self.A1), T.dot(self.A2, self.TG1[j].sum(axis=1).T/n1)).diagonal() \\\n",
    "                + self.BDu[u] * self.bundle_discount_arr[bj]\n",
    "        \n",
    "        x_uij = T.nnet.sigmoid(x_ui-x_uj)\n",
    "        obj = T.sum(T.log(x_uij) - self._lambda_u * (self.M1 ** 2).sum() - \\\n",
    "                    self._lambda_u * (self.M2 ** 2).sum()  - self._lambda_d * (self.K**2) - self._lambda_d * (self.D**2)\\\n",
    "                    -self._lambda_p * (self.N[n2]**2) - self._lambda_p * (self.N[n1]**2)) - self._lambda_A * (self.A1 ** 2).sum() - self._lambda_A * (self.A2 ** 2).sum()\n",
    "        cost = - obj\n",
    "\n",
    "        g_cost_M1 = T.grad(cost=cost, wrt=self.M1)\n",
    "        g_cost_M2 = T.grad(cost=cost, wrt=self.M2)\n",
    "        g_cost_K = T.grad(cost=cost, wrt=self.K)\n",
    "        g_cost_N = T.grad(cost=cost, wrt=self.N)\n",
    "        g_cost_D = T.grad(cost=cost, wrt=self.D)\n",
    "        g_cost_A1 = T.grad(cost=cost, wrt=self.A1)\n",
    "        g_cost_A2 = T.grad(cost=cost, wrt=self.A2)\n",
    "        g_cost_BDu = T.grad(cost=cost, wrt=self.BDu)\n",
    "\n",
    "        \n",
    "        updates = [(self.M1, self.M1 - self._learning_rate * .001* g_cost_M1), (self.M2, self.M2 - self._learning_rate *.001* g_cost_M2), \n",
    "                   (self.K, self.K - self._learning_rate * .001*g_cost_K), (self.N, self.N - self._learning_rate *g_cost_N),\n",
    "                  (self.D, self.D - self._learning_rate * g_cost_D), \n",
    "                   (self.A1, self.A1-self._learning_rate * .001 * g_cost_A1), (self.A2, self.A2-self._learning_rate * .001 * g_cost_A2),\n",
    "                  (self.BDu, self.BDu-self._learning_rate * .001 * g_cost_BDu)]\n",
    "\n",
    "        self.train_model_item = theano.function(inputs=[u, i, j, n1, n2, di, dj, bi, bj], outputs=cost, updates=updates)\n",
    "\n",
    "    \n",
    "    def train(self, s_users=None, s_pos_items=None, s_neg_items=None, s_pos_len=None, s_neg_len=None,\n",
    "             s_pos_diversity=None, s_neg_diversity=None, bd_pos_bundle = None, bd_neg_bundle = None, batch_size=1000):\n",
    "        \n",
    "        if len(s_users) < batch_size:\n",
    "            sys.stderr.write(\"WARNING: Batch size is greater than number of training samples, switching to a batch size of %s\\n\" % str(len(train_data)))\n",
    "            batch_size = len(s_users)\n",
    "        \n",
    "        sgd_users, sgd_pos_items, sgd_neg_items = s_users, s_pos_items, s_neg_items\n",
    "        n_sgd_samples = len(s_users)\n",
    "\n",
    "        z = 0\n",
    "        t2 = t1 = t0 = time.time()\n",
    "        while (z+1)*batch_size < n_sgd_samples:\n",
    "            \n",
    "            self.train_model_item(\n",
    "                sgd_users[z*batch_size: (z+1)*batch_size],\n",
    "                sgd_pos_items[z*batch_size: (z+1)*batch_size],\n",
    "                sgd_neg_items[z*batch_size: (z+1)*batch_size],\n",
    "                s_pos_len[z*batch_size: (z+1)*batch_size],\n",
    "                s_neg_len[z*batch_size: (z+1)*batch_size],\n",
    "                s_pos_diversity[z*batch_size: (z+1)*batch_size],\n",
    "                s_neg_diversity[z*batch_size: (z+1)*batch_size],\n",
    "                bd_pos_bundle[z*batch_size: (z+1)*batch_size],\n",
    "                bd_neg_bundle[z*batch_size: (z+1)*batch_size]\n",
    "            )\n",
    "            z += 1\n",
    "            t2 = time.time()\n",
    "            sys.stderr.write(\"\\rProcessed %s ( %.2f%% ) in %.4f seconds\" %(str(z*batch_size), 100.0 * float(z*batch_size)/n_sgd_samples, t2 - t1))\n",
    "            sys.stderr.flush()\n",
    "            t1 = t2\n",
    "        if n_sgd_samples > 0:\n",
    "            sys.stderr.write(\"\\nTotal training time %.2f seconds; %e per sample\\n\" % (t2 - t0, (t2 - t0)/n_sgd_samples))\n",
    "            sys.stderr.flush()\n",
    "    \n",
    "    def _generate_test_model_function(self):\n",
    "        u = T.lvector('u')\n",
    "        i = T.lmatrix('i')\n",
    "        j = T.lmatrix('j')\n",
    "        n1 = T.lvector('n1')\n",
    "        n2 = T.lvector('n2')\n",
    "        di = T.dvector('di')\n",
    "        dj = T.dvector('dj')\n",
    "        bi = T.lvector('bi')\n",
    "        bj = T.lvector('bj')\n",
    "        \n",
    "        x_ui = T.dot(T.dot(self.W1[u],self.M2), T.dot(self.M1, self.H1[i].sum(axis=1).T/n1)).diagonal() + self.K*(self.B1[i].T/n1).T.sum(axis=1) + self.N[n1] + self.D*di + T.dot(T.dot(self.theta1[u],self.A1), T.dot(self.A2, self.TG1[i].sum(axis=1).T/n1)).diagonal() \\\n",
    "            + self.BDu[u] * self.bundle_discount_arr[bi]\n",
    "        x_uj = T.dot(T.dot(self.W1[u],self.M2), T.dot(self.M1, self.H1[j].sum(axis=1).T/n2)).diagonal() + self.K*(self.B1[j].T/n2).T.sum(axis=1) + self.N[n2] + self.D*dj + T.dot(T.dot(self.theta1[u],self.A1), T.dot(self.A2, self.TG1[j].sum(axis=1).T/n1)).diagonal() \\\n",
    "            + self.BDu[u] * self.bundle_discount_arr[bj]\n",
    "        y_ui = x_ui - self.BDu[u] * self.bundle_discount_arr[bi]\n",
    "        y_uj = x_uj - self.BDu[u] * self.bundle_discount_arr[bj]\n",
    "        \n",
    "        x_uij = x_ui-x_uj\n",
    "#         self.test_model = theano.function(inputs=[u, i, j, n1, n2, di, dj, bi, bj], outputs=x_uij)\n",
    "        out = [x_uij, x_ui, x_uj, y_ui, y_uj]\n",
    "        self.test_model = theano.function(inputs=[u, i, j, n1, n2, di, dj, bi, bj], outputs= out)\n",
    "\n",
    "    def test_bundle(self, sgd_users, sgd_pos_items, sgd_neg_items, s_pos_len, s_neg_len,\n",
    "                    s_pos_diversity, s_neg_diversity, bd_pos_bundle, bd_neg_bundle, batch_size=1000):\n",
    "        \n",
    "        auc_values = []\n",
    "        z = 0\n",
    "        t2 = t1 = t0 = time.time()\n",
    "        n_sgd_samples = len(sgd_users)\n",
    "        x_uis, x_ujs, y_uis, y_ujs = [], [], [], []\n",
    "        while (z+1)*batch_size < n_sgd_samples:\n",
    "            pref_list, xu1, xu2, yu1, yu2 =self.test_model(\n",
    "                sgd_users[z*batch_size: (z+1)*batch_size],\n",
    "                sgd_pos_items[z*batch_size: (z+1)*batch_size],\n",
    "                sgd_neg_items[z*batch_size: (z+1)*batch_size],\n",
    "                s_pos_len[z*batch_size: (z+1)*batch_size],\n",
    "                s_neg_len[z*batch_size: (z+1)*batch_size],\n",
    "                s_pos_diversity[z*batch_size: (z+1)*batch_size],\n",
    "                s_neg_diversity[z*batch_size: (z+1)*batch_size],\n",
    "                bd_pos_bundle[z*batch_size: (z+1)*batch_size],\n",
    "                bd_neg_bundle[z*batch_size: (z+1)*batch_size]\n",
    "            )\n",
    "            x_uis += [x for x in xu1]\n",
    "            x_ujs += [x for x in xu2]\n",
    "            y_uis += [x for x in yu1]\n",
    "            y_ujs += [x for x in yu2]\n",
    "            z += 1\n",
    "            t2 = time.time()\n",
    "            sys.stderr.write(\"\\rProcessed %s ( %.2f%% ) in %.4f seconds\" %(str(z*batch_size), 100.0 * float(z*batch_size)/n_sgd_samples, t2 - t1))\n",
    "            t1 = t2\n",
    "            \n",
    "            sys.stderr.write(\"pref%d\" % len(pref_list))\n",
    "            sys.stderr.flush()\n",
    "            auc = np.sum([1.0 if a > 0.0 else 0.0 for a in pref_list])\n",
    "            auc /= batch_size\n",
    "            \n",
    "            auc_values.append(auc)\n",
    "            sys.stderr.write(\"\\rCurrent AUC mean (%s samples): %0.5f\" % (str(z*batch_size), numpy.mean(auc_values)))\n",
    "            sys.stderr.flush()\n",
    "        sys.stderr.write(\"\\n\")\n",
    "        sys.stderr.flush()\n",
    "        return [x_uis, x_ujs, y_uis, y_ujs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "bpr_bundle = BPR_Buncle1(10, max_bundle_size, num_users, num_items, \\\n",
    "                        num_tags, bundle_discount_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "82 29633 2515 327\n"
     ]
    }
   ],
   "source": [
    "num_tags = 327\n",
    "print max_bundle_size, num_users, num_items,num_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed 9000 ( 90.00% ) in 0.5506 seconds\n",
      "Total training time 5.64 seconds; 5.636148e-04 per sample\n"
     ]
    }
   ],
   "source": [
    "# bpr_bundle.train(s_users=sgd_users[:10000], s_pos_items=sgd_pos_items[:10000], s_neg_items=sgd_neg_items[:10000], \n",
    "#             s_pos_len=sgd_pos_len[:10000], s_neg_len=sgd_neg_len[:10000], s_pos_diversity=sgd_pos_diversity[:10000],\n",
    "#             s_neg_diversity=sgd_neg_diversity[:10000], \\\n",
    "#                 bd_pos_bundle = sgd_pos_bundles[:10000], bd_neg_bundle = sgd_neg_bundles[:10000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed 2101000 ( 99.99% ) in 0.4112 seconds\n",
      "Total training time 1111.54 seconds; 5.290189e-04 per sample\n"
     ]
    }
   ],
   "source": [
    "bpr_bundle.train(s_users=sgd_users, s_pos_items=sgd_pos_items, s_neg_items=sgd_neg_items, \n",
    "            s_pos_len=sgd_pos_len, s_neg_len=sgd_neg_len, s_pos_diversity=sgd_pos_diversity,\n",
    "            s_neg_diversity=sgd_neg_diversity, \\\n",
    "                bd_pos_bundle = sgd_pos_bundles, bd_neg_bundle = sgd_neg_bundles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Current AUC mean (420000 samples): 0.93265ndspref1000\n"
     ]
    }
   ],
   "source": [
    "res= bpr_bundle.test_bundle(test_users, test_pos_items, test_neg_items, test_n1, test_n2,\\\n",
    "                       test_pos_diversity, test_neg_diversity,\\\n",
    "                      test_pos_bundles, test_neg_bundles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Current AUC mean (9000 samples): 0.44789ndspref1000\n"
     ]
    }
   ],
   "source": [
    "# res = bpr_bundle.test_bundle(test_users[:10000], test_pos_items[:10000], test_neg_items[:10000], test_n1[:10000], test_n2[:10000],\\\n",
    "#                        test_pos_diversity[:10000], test_neg_diversity[:10000],\\\n",
    "#                       test_pos_bundles[:10000], test_neg_bundles[:10000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "391712 420000\n",
      "(4, 420000) (420000,) (420000,)\n"
     ]
    }
   ],
   "source": [
    "cnt, total = 0, 0\n",
    "ux1, ux2, uy1, uy2 = res[0], res[1], res[2], res[3]\n",
    "for x, y in zip(ux1, ux2):\n",
    "    cnt += 1\n",
    "    total += 1 if x > y else 0\n",
    "print total/ float(cnt)\n",
    "print np.shape(res), np.shape(ux1),np.shape(ux2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_bundle_score = {}\n",
    "for u, x, y, b1, b2 in zip(test_users, ux1, ux2, test_pos_bundles, test_neg_bundles):\n",
    "    if u not in user_bundle_score:\n",
    "        user_bundle_score[u] = {}\n",
    "    user_bundle_score[u][b1] = x \n",
    "    user_bundle_score[u][b2] = y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10006\n"
     ]
    }
   ],
   "source": [
    "print len(user_bundle_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 1000)\n"
     ]
    }
   ],
   "source": [
    "print np.shape(res[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bpr_bundle.test_bundle(test_users, test_pos_items, test_neg_items, test_n1, test_n2,\\\n",
    "#                        test_pos_diversity, test_neg_diversity,\\\n",
    "#                       test_pos_bundles, test_neg_bundles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fold = '../data/new_data'\n",
    "bundle_price_map = pickle.load(open(fold + '/bundle_price_map', 'r'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "bundle_price_arr = [bundle_price_map[i] for i in range(len(bundle_price_map)) if i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D\n"
     ]
    }
   ],
   "source": [
    "print bpr_bundle.D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print max_bundle_size, len(user_bundle_map.keys()), num_items, len(tag_array.T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## final price + discount model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class BPR_Buncle2(object):\n",
    "\n",
    "    def __init__(self, rank, bundle_size, n_users, n_items, n_tags, bundle_discount_arr, bundle_price_arr, lambda_u = 0.0025, lambda_i = 0.0025, lambda_j = 0.00025, lambda_d = 0.0025, lambda_p = 0.00025, lambda_A = 0.01, lambda_bias = 0.0, learning_rate = 0.05):\n",
    "        \n",
    "        self._rank = rank\n",
    "        self._bundle_rank = bundle_size + 1\n",
    "        self._n_users = n_users\n",
    "        self._n_items = n_items\n",
    "        self._n_bundle = len(bundle_discount_map)\n",
    "        self.bundle_discount_arr = theano.shared(np.array(bundle_discount_arr))\n",
    "        self.bundle_price_arr = theano.shared(np.array(bundle_price_arr))\n",
    "        self._lambda_u = lambda_u\n",
    "        self._lambda_i = lambda_i\n",
    "        self._lambda_j = lambda_j\n",
    "        self._lambda_d = lambda_d\n",
    "        self._lambda_p = lambda_p\n",
    "        self._lambda_bias = lambda_bias\n",
    "        self._lambda_A = lambda_A\n",
    "        self._learning_rate = learning_rate\n",
    "        \n",
    "        self._n_tags = n_tags\n",
    "        self._configure_theano()\n",
    "        self._generate_train_model_item_function()\n",
    "        self._generate_test_model_function()\n",
    "\n",
    "    def _configure_theano(self):\n",
    "        theano.config.mode = 'FAST_RUN'\n",
    "        theano.config.floatX = 'float32'\n",
    "    \n",
    "    def _generate_train_model_item_function(self):\n",
    "        u = T.lvector('u')\n",
    "        i = T.lmatrix('i')\n",
    "        j = T.lmatrix('j')\n",
    "        n1 = T.lvector('n1')#num of items in pos bundle\n",
    "        n2 = T.lvector('n2')\n",
    "        di = T.dvector('di')#cb\n",
    "        dj = T.dvector('dj')#cb\n",
    "        bi = T.lvector('bi')\n",
    "        bj = T.lvector('bj')\n",
    "        \n",
    "        self.W1 = bpr_item.W #Pu\n",
    "        self.H1 = theano.shared(H_item.astype('float32'), name='H')#Qi\n",
    "        self.B1 = theano.shared(B_item.astype('float32'), name='B')#Bi\n",
    "        self.TG1 = theano.shared(A_item.astype('float32'), name='TG')\n",
    "        self.theta1 = bpr_item.theta\n",
    "        \n",
    "        self.M1 = theano.shared(numpy.random.random((self._rank, self._rank)).astype('float64'), name='M1')#w\n",
    "        self.M2 = theano.shared(numpy.random.random((self._rank, self._rank)).astype('float64'), name='M2')#u\n",
    "        self.BDu = theano.shared(numpy.random.random((self._n_users)).astype('float64'), name='bdu')#u\n",
    "        self.BPu = theano.shared(numpy.random.random((self._n_users)).astype('float64'), name='bpu')\n",
    "        self.K = theano.shared(numpy.random.rand(), name='K')#k\n",
    "        self.D = theano.shared(numpy.random.rand(), name='D')#C\n",
    "        self.N = theano.shared(numpy.random.random(self._bundle_rank).astype('float32'), name='N')#Nb\n",
    "        self.A1 = theano.shared(numpy.random.random((self._n_tags, self._n_tags)), name='A1')\n",
    "        self.A2 = theano.shared(numpy.random.random((self._n_tags, self._n_tags)), name='A2')\n",
    "                                \n",
    "        \n",
    "        x_ui = T.dot(T.dot(self.W1[u],self.M2), T.dot(self.M1, self.H1[i].sum(axis=1).T/n1)).diagonal() + self.K*(self.B1[i].T/n1).T.sum(axis=1) + self.N[n1] + self.D*di + T.dot(T.dot(self.theta1[u],self.A1), T.dot(self.A2, self.TG1[i].sum(axis=1).T/n1)).diagonal()\\\n",
    "                + self.BDu[u] * self.bundle_discount_arr[bi] - self.BPu[u] * self.bundle_price_arr[bi]\n",
    "        x_uj = T.dot(T.dot(self.W1[u],self.M2), T.dot(self.M1, self.H1[j].sum(axis=1).T/n2)).diagonal() + self.K*(self.B1[j].T/n2).T.sum(axis=1) + self.N[n2] + self.D*dj + T.dot(T.dot(self.theta1[u],self.A1), T.dot(self.A2, self.TG1[j].sum(axis=1).T/n1)).diagonal() \\\n",
    "                + self.BDu[u] * self.bundle_discount_arr[bj] - self.BPu[u] * self.bundle_price_arr[bj]\n",
    "        \n",
    "        x_uij = T.nnet.sigmoid(x_ui-x_uj)\n",
    "        obj = T.sum(T.log(x_uij) - self._lambda_u * (self.M1 ** 2).sum() - \\\n",
    "                    self._lambda_u * (self.M2 ** 2).sum()  - self._lambda_d * (self.K**2) - self._lambda_d * (self.D**2)\\\n",
    "                    -self._lambda_p * (self.N[n2]**2) - self._lambda_p * (self.N[n1]**2)) - self._lambda_A * (self.A1 ** 2).sum() - self._lambda_A * (self.A2 ** 2).sum()\\\n",
    "                    \n",
    "        cost = - obj\n",
    "\n",
    "        g_cost_M1 = T.grad(cost=cost, wrt=self.M1)\n",
    "        g_cost_M2 = T.grad(cost=cost, wrt=self.M2)\n",
    "        g_cost_K = T.grad(cost=cost, wrt=self.K)\n",
    "        g_cost_N = T.grad(cost=cost, wrt=self.N)\n",
    "        g_cost_D = T.grad(cost=cost, wrt=self.D)\n",
    "        g_cost_A1 = T.grad(cost=cost, wrt=self.A1)\n",
    "        g_cost_A2 = T.grad(cost=cost, wrt=self.A2)\n",
    "        g_cost_BDu = T.grad(cost=cost, wrt=self.BDu)\n",
    "        g_cost_BPu = T.grad(cost=cost, wrt=self.BPu)\n",
    "\n",
    "        \n",
    "        updates = [(self.M1, self.M1 - self._learning_rate * .001* g_cost_M1), (self.M2, self.M2 - self._learning_rate *.001* g_cost_M2), \n",
    "                   (self.K, self.K - self._learning_rate * .001*g_cost_K), (self.N, self.N - self._learning_rate *g_cost_N),\n",
    "                  (self.D, self.D - self._learning_rate * g_cost_D), \n",
    "                   (self.A1, self.A1-self._learning_rate * .001 * g_cost_A1), (self.A2, self.A2-self._learning_rate * .001 * g_cost_A2),\n",
    "                  (self.BDu, self.BDu-self._learning_rate * .001 * g_cost_BDu),\n",
    "                  (self.BPu, self.BPu-self._learning_rate * .001 * g_cost_BPu)]\n",
    "\n",
    "        self.train_model_item = theano.function(inputs=[u, i, j, n1, n2, di, dj, bi, bj], outputs=cost, updates=updates)\n",
    "\n",
    "    \n",
    "    def train(self, s_users=None, s_pos_items=None, s_neg_items=None, s_pos_len=None, s_neg_len=None,\n",
    "             s_pos_diversity=None, s_neg_diversity=None, bd_pos_bundle = None, bd_neg_bundle = None, batch_size=1000):\n",
    "        \n",
    "        if len(s_users) < batch_size:\n",
    "            sys.stderr.write(\"WARNING: Batch size is greater than number of training samples, switching to a batch size of %s\\n\" % str(len(train_data)))\n",
    "            batch_size = len(s_users)\n",
    "        \n",
    "        sgd_users, sgd_pos_items, sgd_neg_items = s_users, s_pos_items, s_neg_items\n",
    "        n_sgd_samples = len(s_users)\n",
    "\n",
    "        z = 0\n",
    "        t2 = t1 = t0 = time.time()\n",
    "        while (z+1)*batch_size < n_sgd_samples:\n",
    "            \n",
    "            self.train_model_item(\n",
    "                sgd_users[z*batch_size: (z+1)*batch_size],\n",
    "                sgd_pos_items[z*batch_size: (z+1)*batch_size],\n",
    "                sgd_neg_items[z*batch_size: (z+1)*batch_size],\n",
    "                s_pos_len[z*batch_size: (z+1)*batch_size],\n",
    "                s_neg_len[z*batch_size: (z+1)*batch_size],\n",
    "                s_pos_diversity[z*batch_size: (z+1)*batch_size],\n",
    "                s_neg_diversity[z*batch_size: (z+1)*batch_size],\n",
    "                bd_pos_bundle[z*batch_size: (z+1)*batch_size],\n",
    "                bd_neg_bundle[z*batch_size: (z+1)*batch_size]\n",
    "            )\n",
    "            z += 1\n",
    "            t2 = time.time()\n",
    "            sys.stderr.write(\"\\rProcessed %s ( %.2f%% ) in %.4f seconds\" %(str(z*batch_size), 100.0 * float(z*batch_size)/n_sgd_samples, t2 - t1))\n",
    "            sys.stderr.flush()\n",
    "            t1 = t2\n",
    "        if n_sgd_samples > 0:\n",
    "            sys.stderr.write(\"\\nTotal training time %.2f seconds; %e per sample\\n\" % (t2 - t0, (t2 - t0)/n_sgd_samples))\n",
    "            sys.stderr.flush()\n",
    "    \n",
    "    def _generate_test_model_function(self):\n",
    "        u = T.lvector('u')\n",
    "        i = T.lmatrix('i')\n",
    "        j = T.lmatrix('j')\n",
    "        n1 = T.lvector('n1')\n",
    "        n2 = T.lvector('n2')\n",
    "        di = T.dvector('di')\n",
    "        dj = T.dvector('dj')\n",
    "        bi = T.lvector('bi')\n",
    "        bj = T.lvector('bj')\n",
    "        \n",
    "        x_ui = T.dot(T.dot(self.W1[u],self.M2), T.dot(self.M1, self.H1[i].sum(axis=1).T/n1)).diagonal() + self.K*(self.B1[i].T/n1).T.sum(axis=1) + self.N[n1] + self.D*di + T.dot(T.dot(self.theta1[u],self.A1), T.dot(self.A2, self.TG1[i].sum(axis=1).T/n1)).diagonal() \\\n",
    "            + self.BDu[u] * self.bundle_discount_arr[bi] - self.BPu[u] * self.bundle_price_arr[bi]\n",
    "        x_uj = T.dot(T.dot(self.W1[u],self.M2), T.dot(self.M1, self.H1[j].sum(axis=1).T/n2)).diagonal() + self.K*(self.B1[j].T/n2).T.sum(axis=1) + self.N[n2] + self.D*dj + T.dot(T.dot(self.theta1[u],self.A1), T.dot(self.A2, self.TG1[j].sum(axis=1).T/n1)).diagonal() \\\n",
    "            + self.BDu[u] * self.bundle_discount_arr[bj] - self.BPu[u] * self.bundle_price_arr[bj]\n",
    "        y_ui = x_ui - self.BDu[u] * self.bundle_discount_arr[bi] + self.BPu[u] * self.bundle_price_arr[bi]\n",
    "        y_uj = x_uj - self.BDu[u] * self.bundle_discount_arr[bj] + self.BPu[u] * self.bundle_price_arr[bj]\n",
    "        \n",
    "        x_uij = x_ui-x_uj\n",
    "        #self.test_model = theano.function(inputs=[u, i, j, n1, n2, di, dj, bi, bj], outputs=x_uij)\n",
    "        out = [x_uij, x_ui, x_uj, y_ui, y_uj]\n",
    "        self.test_model = theano.function(inputs=[u, i, j, n1, n2, di, dj, bi, bj], outputs= out)\n",
    "        \n",
    "    def test_bundle(self, sgd_users, sgd_pos_items, sgd_neg_items, s_pos_len, s_neg_len,\n",
    "                    s_pos_diversity, s_neg_diversity, bd_pos_bundle, bd_neg_bundle, batch_size=1000):\n",
    "        \n",
    "        auc_values = []\n",
    "        z = 0\n",
    "        t2 = t1 = t0 = time.time()\n",
    "        n_sgd_samples = len(sgd_users)\n",
    "        x_uis, x_ujs, y_uis, y_ujs = [], [], [], []\n",
    "        while (z+1)*batch_size < n_sgd_samples:\n",
    "            pref_list, xu1, xu2, yu1, yu2 =self.test_model(\n",
    "                sgd_users[z*batch_size: (z+1)*batch_size],\n",
    "                sgd_pos_items[z*batch_size: (z+1)*batch_size],\n",
    "                sgd_neg_items[z*batch_size: (z+1)*batch_size],\n",
    "                s_pos_len[z*batch_size: (z+1)*batch_size],\n",
    "                s_neg_len[z*batch_size: (z+1)*batch_size],\n",
    "                s_pos_diversity[z*batch_size: (z+1)*batch_size],\n",
    "                s_neg_diversity[z*batch_size: (z+1)*batch_size],\n",
    "                bd_pos_bundle[z*batch_size: (z+1)*batch_size],\n",
    "                bd_neg_bundle[z*batch_size: (z+1)*batch_size]\n",
    "            )\n",
    "            x_uis += [x for x in xu1]\n",
    "            x_ujs += [x for x in xu2]\n",
    "            y_uis += [x for x in yu1]\n",
    "            y_ujs += [x for x in yu2]\n",
    "            z += 1\n",
    "            t2 = time.time()\n",
    "            sys.stderr.write(\"\\rProcessed %s ( %.2f%% ) in %.4f seconds\" %(str(z*batch_size), 100.0 * float(z*batch_size)/n_sgd_samples, t2 - t1))\n",
    "            t1 = t2\n",
    "            \n",
    "            sys.stderr.write(\"pref%d\" % len(pref_list))\n",
    "            sys.stderr.flush()\n",
    "            auc = np.sum([1.0 if a > 0.0 else 0.0 for a in pref_list])\n",
    "            auc /= batch_size\n",
    "            \n",
    "            auc_values.append(auc)\n",
    "            sys.stderr.write(\"\\rCurrent AUC mean (%s samples): %0.5f\" % (str(z*batch_size), numpy.mean(auc_values)))\n",
    "            sys.stderr.flush()\n",
    "        sys.stderr.write(\"\\n\")\n",
    "        sys.stderr.flush()\n",
    "        return [x_uis, x_ujs, y_uis, y_ujs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "bpr_bundle2 = BPR_Buncle2(10, max_bundle_size, num_users, num_items, \\\n",
    "                        num_tags, bundle_discount_arr, bundle_price_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed 2101000 ( 99.99% ) in 0.3776 seconds\n",
      "Total training time 778.86 seconds; 3.706853e-04 per sample\n"
     ]
    }
   ],
   "source": [
    "bpr_bundle2.train(s_users=sgd_users, s_pos_items=sgd_pos_items, s_neg_items=sgd_neg_items, \n",
    "            s_pos_len=sgd_pos_len, s_neg_len=sgd_neg_len, s_pos_diversity=sgd_pos_diversity,\n",
    "            s_neg_diversity=sgd_neg_diversity, \\\n",
    "                bd_pos_bundle = sgd_pos_bundles, bd_neg_bundle = sgd_neg_bundles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Current AUC mean (420000 samples): 0.90889ndspref1000\n"
     ]
    }
   ],
   "source": [
    "res2 = bpr_bundle2.test_bundle(test_users, test_pos_items, test_neg_items, test_n1, test_n2,\\\n",
    "                       test_pos_diversity, test_neg_diversity,\\\n",
    "                      test_pos_bundles, test_neg_bundles)\n",
    "#0.91110n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## final price + discount price + final price * discount price model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class BPR_Buncle3(object):\n",
    "\n",
    "    def __init__(self, rank, bundle_size, n_users, n_items, n_tags, bundle_discount_arr, bundle_price_arr, lambda_u = 0.0025, lambda_i = 0.0025, lambda_j = 0.00025, lambda_d = 0.0025, lambda_p = 0.00025, lambda_A = 0.01, lambda_bias = 0.0, learning_rate = 0.05):\n",
    "        \n",
    "        self._rank = rank\n",
    "        self._bundle_rank = bundle_size + 1\n",
    "        self._n_users = n_users\n",
    "        self._n_items = n_items\n",
    "        self._n_bundle = len(bundle_discount_map)\n",
    "        self.bundle_discount_arr = theano.shared(np.array(bundle_discount_arr))\n",
    "        self.bundle_price_arr = theano.shared(np.array(bundle_price_arr))\n",
    "        self._lambda_u = lambda_u\n",
    "        self._lambda_i = lambda_i\n",
    "        self._lambda_j = lambda_j\n",
    "        self._lambda_d = lambda_d\n",
    "        self._lambda_p = lambda_p\n",
    "        self._lambda_bias = lambda_bias\n",
    "        self._lambda_A = lambda_A\n",
    "        self._learning_rate = learning_rate\n",
    "        \n",
    "        self._n_tags = n_tags\n",
    "        self._configure_theano()\n",
    "        self._generate_train_model_item_function()\n",
    "        self._generate_test_model_function()\n",
    "\n",
    "    def _configure_theano(self):\n",
    "        theano.config.mode = 'FAST_RUN'\n",
    "        theano.config.floatX = 'float32'\n",
    "    \n",
    "    def _generate_train_model_item_function(self):\n",
    "        u = T.lvector('u')\n",
    "        i = T.lmatrix('i')\n",
    "        j = T.lmatrix('j')\n",
    "        n1 = T.lvector('n1')#num of items in pos bundle\n",
    "        n2 = T.lvector('n2')\n",
    "        di = T.dvector('di')#cb\n",
    "        dj = T.dvector('dj')#cb\n",
    "        bi = T.lvector('bi')\n",
    "        bj = T.lvector('bj')\n",
    "        \n",
    "        self.W1 = bpr_item.W #Pu\n",
    "        self.H1 = theano.shared(H_item.astype('float32'), name='H')#Qi\n",
    "        self.B1 = theano.shared(B_item.astype('float32'), name='B')#Bi\n",
    "        self.TG1 = theano.shared(A_item.astype('float32'), name='TG')\n",
    "        self.theta1 = bpr_item.theta\n",
    "        \n",
    "        self.M1 = theano.shared(numpy.random.random((self._rank, self._rank)).astype('float64'), name='M1')#w\n",
    "        self.M2 = theano.shared(numpy.random.random((self._rank, self._rank)).astype('float64'), name='M2')#u\n",
    "        self.BDu = theano.shared(numpy.random.random((self._n_users)).astype('float64'), name='bdu')#discount\n",
    "        self.BPu = theano.shared(numpy.random.random((self._n_users)).astype('float64'), name='bpu')#final price\n",
    "        self.BDPu = theano.shared(numpy.random.random((self._n_users)).astype('float64'), name='bdpu')#discount price\n",
    "        self.K = theano.shared(numpy.random.rand(), name='K')#k\n",
    "        self.D = theano.shared(numpy.random.rand(), name='D')#C\n",
    "        self.N = theano.shared(numpy.random.random(self._bundle_rank).astype('float32'), name='N')#Nb\n",
    "        self.A1 = theano.shared(numpy.random.random((self._n_tags, self._n_tags)), name='A1')\n",
    "        self.A2 = theano.shared(numpy.random.random((self._n_tags, self._n_tags)), name='A2')\n",
    "                                \n",
    "        discount_price_i =  self.bundle_price_arr[bi] / (1 - self.bundle_discount_arr[bi]) * self.bundle_discount_arr[bi]\n",
    "        discount_price_j =  self.bundle_price_arr[bj] / (1 - self.bundle_discount_arr[bj]) * self.bundle_discount_arr[bj]\n",
    "        \n",
    "        x_ui = T.dot(T.dot(self.W1[u],self.M2), T.dot(self.M1, self.H1[i].sum(axis=1).T/n1)).diagonal() + self.K*(self.B1[i].T/n1).T.sum(axis=1) + self.N[n1] + self.D*di + T.dot(T.dot(self.theta1[u],self.A1), T.dot(self.A2, self.TG1[i].sum(axis=1).T/n1)).diagonal()\\\n",
    "                + self.BDu[u] * self.bundle_discount_arr[bi] - self.BPu[u] * self.bundle_price_arr[bi] \\\n",
    "                + self.BDPu[u] * discount_price_i\n",
    "        x_uj = T.dot(T.dot(self.W1[u],self.M2), T.dot(self.M1, self.H1[j].sum(axis=1).T/n2)).diagonal() + self.K*(self.B1[j].T/n2).T.sum(axis=1) + self.N[n2] + self.D*dj + T.dot(T.dot(self.theta1[u],self.A1), T.dot(self.A2, self.TG1[j].sum(axis=1).T/n1)).diagonal() \\\n",
    "                + self.BDu[u] * self.bundle_discount_arr[bj] - self.BPu[u] * self.bundle_price_arr[bj] \\\n",
    "                + self.BDPu[u] * discount_price_j\n",
    "        \n",
    "        x_uij = T.nnet.sigmoid(x_ui-x_uj)\n",
    "        obj = T.sum(T.log(x_uij) - self._lambda_u * (self.M1 ** 2).sum() - \\\n",
    "                    self._lambda_u * (self.M2 ** 2).sum()  - self._lambda_d * (self.K**2) - self._lambda_d * (self.D**2)\\\n",
    "                    -self._lambda_p * (self.N[n2]**2) - self._lambda_p * (self.N[n1]**2)) - self._lambda_A * (self.A1 ** 2).sum() - self._lambda_A * (self.A2 ** 2).sum()\n",
    "        cost = - obj\n",
    "\n",
    "        g_cost_M1 = T.grad(cost=cost, wrt=self.M1)\n",
    "        g_cost_M2 = T.grad(cost=cost, wrt=self.M2)\n",
    "        g_cost_K = T.grad(cost=cost, wrt=self.K)\n",
    "        g_cost_N = T.grad(cost=cost, wrt=self.N)\n",
    "        g_cost_D = T.grad(cost=cost, wrt=self.D)\n",
    "        g_cost_A1 = T.grad(cost=cost, wrt=self.A1)\n",
    "        g_cost_A2 = T.grad(cost=cost, wrt=self.A2)\n",
    "        g_cost_BDu = T.grad(cost=cost, wrt=self.BDu)\n",
    "        g_cost_BPu = T.grad(cost=cost, wrt=self.BPu)\n",
    "        g_cost_BDPu = T.grad(cost=cost, wrt=self.BDPu)\n",
    "        \n",
    "        updates = [(self.M1, self.M1 - self._learning_rate * .001* g_cost_M1), (self.M2, self.M2 - self._learning_rate *.001* g_cost_M2), \n",
    "                   (self.K, self.K - self._learning_rate * .001*g_cost_K), (self.N, self.N - self._learning_rate *g_cost_N),\n",
    "                  (self.D, self.D - self._learning_rate * g_cost_D), \n",
    "                   (self.A1, self.A1-self._learning_rate * .001 * g_cost_A1), (self.A2, self.A2-self._learning_rate * .001 * g_cost_A2),\n",
    "                  (self.BDu, self.BDu-self._learning_rate * .001 * g_cost_BDu),\n",
    "                  (self.BPu, self.BPu-self._learning_rate * .001 * g_cost_BPu),\n",
    "                  (self.BDPu, self.BDPu-self._learning_rate * .001 * g_cost_BDPu)]\n",
    "\n",
    "        self.train_model_item = theano.function(inputs=[u, i, j, n1, n2, di, dj, bi, bj], outputs=cost, updates=updates)\n",
    "\n",
    "    \n",
    "    def train(self, s_users=None, s_pos_items=None, s_neg_items=None, s_pos_len=None, s_neg_len=None,\n",
    "             s_pos_diversity=None, s_neg_diversity=None, bd_pos_bundle = None, bd_neg_bundle = None, batch_size=1000):\n",
    "        \n",
    "        if len(s_users) < batch_size:\n",
    "            sys.stderr.write(\"WARNING: Batch size is greater than number of training samples, switching to a batch size of %s\\n\" % str(len(train_data)))\n",
    "            batch_size = len(s_users)\n",
    "        \n",
    "        sgd_users, sgd_pos_items, sgd_neg_items = s_users, s_pos_items, s_neg_items\n",
    "        n_sgd_samples = len(s_users)\n",
    "\n",
    "        z = 0\n",
    "        t2 = t1 = t0 = time.time()\n",
    "        while (z+1)*batch_size < n_sgd_samples:\n",
    "            \n",
    "            self.train_model_item(\n",
    "                sgd_users[z*batch_size: (z+1)*batch_size],\n",
    "                sgd_pos_items[z*batch_size: (z+1)*batch_size],\n",
    "                sgd_neg_items[z*batch_size: (z+1)*batch_size],\n",
    "                s_pos_len[z*batch_size: (z+1)*batch_size],\n",
    "                s_neg_len[z*batch_size: (z+1)*batch_size],\n",
    "                s_pos_diversity[z*batch_size: (z+1)*batch_size],\n",
    "                s_neg_diversity[z*batch_size: (z+1)*batch_size],\n",
    "                bd_pos_bundle[z*batch_size: (z+1)*batch_size],\n",
    "                bd_neg_bundle[z*batch_size: (z+1)*batch_size]\n",
    "            )\n",
    "            z += 1\n",
    "            t2 = time.time()\n",
    "            sys.stderr.write(\"\\rProcessed %s ( %.2f%% ) in %.4f seconds\" %(str(z*batch_size), 100.0 * float(z*batch_size)/n_sgd_samples, t2 - t1))\n",
    "            sys.stderr.flush()\n",
    "            t1 = t2\n",
    "        if n_sgd_samples > 0:\n",
    "            sys.stderr.write(\"\\nTotal training time %.2f seconds; %e per sample\\n\" % (t2 - t0, (t2 - t0)/n_sgd_samples))\n",
    "            sys.stderr.flush()\n",
    "    \n",
    "    def _generate_test_model_function(self):\n",
    "        u = T.lvector('u')\n",
    "        i = T.lmatrix('i')\n",
    "        j = T.lmatrix('j')\n",
    "        n1 = T.lvector('n1')\n",
    "        n2 = T.lvector('n2')\n",
    "        di = T.dvector('di')\n",
    "        dj = T.dvector('dj')\n",
    "        bi = T.lvector('bi')\n",
    "        bj = T.lvector('bj')\n",
    "        \n",
    "        x_ui = T.dot(T.dot(self.W1[u],self.M2), T.dot(self.M1, self.H1[i].sum(axis=1).T/n1)).diagonal() + self.K*(self.B1[i].T/n1).T.sum(axis=1) + self.N[n1] + self.D*di + T.dot(T.dot(self.theta1[u],self.A1), T.dot(self.A2, self.TG1[i].sum(axis=1).T/n1)).diagonal() \\\n",
    "            + self.BDu[u] * self.bundle_discount_arr[bi] - self.BPu[u] * self.bundle_price_arr[bi]\n",
    "        x_uj = T.dot(T.dot(self.W1[u],self.M2), T.dot(self.M1, self.H1[j].sum(axis=1).T/n2)).diagonal() + self.K*(self.B1[j].T/n2).T.sum(axis=1) + self.N[n2] + self.D*dj + T.dot(T.dot(self.theta1[u],self.A1), T.dot(self.A2, self.TG1[j].sum(axis=1).T/n1)).diagonal() \\\n",
    "            + self.BDu[u] * self.bundle_discount_arr[bj] - self.BPu[u] * self.bundle_price_arr[bj]\n",
    "        y_ui = x_ui - self.BDu[u] * self.bundle_discount_arr[bi] + self.BPu[u] * self.bundle_price_arr[bi]\n",
    "        y_uj = x_uj - self.BDu[u] * self.bundle_discount_arr[bj] + self.BPu[u] * self.bundle_price_arr[bj]\n",
    "        \n",
    "        x_uij = x_ui-x_uj\n",
    "        #self.test_model = theano.function(inputs=[u, i, j, n1, n2, di, dj, bi, bj], outputs=x_uij)\n",
    "        out = [x_uij, x_ui, x_uj, y_ui, y_uj]\n",
    "        self.test_model = theano.function(inputs=[u, i, j, n1, n2, di, dj, bi, bj], outputs= out)\n",
    "        \n",
    "    def test_bundle(self, sgd_users, sgd_pos_items, sgd_neg_items, s_pos_len, s_neg_len,\n",
    "                    s_pos_diversity, s_neg_diversity, bd_pos_bundle, bd_neg_bundle, batch_size=1000):\n",
    "        \n",
    "        auc_values = []\n",
    "        z = 0\n",
    "        t2 = t1 = t0 = time.time()\n",
    "        n_sgd_samples = len(sgd_users)\n",
    "        x_uis, x_ujs, y_uis, y_ujs = [], [], [], []\n",
    "        while (z+1)*batch_size < n_sgd_samples:\n",
    "            pref_list, xu1, xu2, yu1, yu2 =self.test_model(\n",
    "                sgd_users[z*batch_size: (z+1)*batch_size],\n",
    "                sgd_pos_items[z*batch_size: (z+1)*batch_size],\n",
    "                sgd_neg_items[z*batch_size: (z+1)*batch_size],\n",
    "                s_pos_len[z*batch_size: (z+1)*batch_size],\n",
    "                s_neg_len[z*batch_size: (z+1)*batch_size],\n",
    "                s_pos_diversity[z*batch_size: (z+1)*batch_size],\n",
    "                s_neg_diversity[z*batch_size: (z+1)*batch_size],\n",
    "                bd_pos_bundle[z*batch_size: (z+1)*batch_size],\n",
    "                bd_neg_bundle[z*batch_size: (z+1)*batch_size]\n",
    "            )\n",
    "            x_uis += [x for x in xu1]\n",
    "            x_ujs += [x for x in xu2]\n",
    "            y_uis += [x for x in yu1]\n",
    "            y_ujs += [x for x in yu2]\n",
    "            z += 1\n",
    "            t2 = time.time()\n",
    "            sys.stderr.write(\"\\rProcessed %s ( %.2f%% ) in %.4f seconds\" %(str(z*batch_size), 100.0 * float(z*batch_size)/n_sgd_samples, t2 - t1))\n",
    "            t1 = t2\n",
    "            \n",
    "            sys.stderr.write(\"pref%d\" % len(pref_list))\n",
    "            sys.stderr.flush()\n",
    "            auc = np.sum([1.0 if a > 0.0 else 0.0 for a in pref_list])\n",
    "            auc /= batch_size\n",
    "            \n",
    "            auc_values.append(auc)\n",
    "            sys.stderr.write(\"\\rCurrent AUC mean (%s samples): %0.5f\" % (str(z*batch_size), numpy.mean(auc_values)))\n",
    "            sys.stderr.flush()\n",
    "        sys.stderr.write(\"\\n\")\n",
    "        sys.stderr.flush()\n",
    "        return [x_uis, x_ujs, y_uis, y_ujs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bpr_bundle3 = BPR_Buncle3(10, max_bundle_size, num_users, num_items, \\\n",
    "                        num_tags, bundle_discount_arr, bundle_price_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed 2101000 ( 99.99% ) in 0.3650 seconds\n",
      "Total training time 834.21 seconds; 3.970270e-04 per sample\n"
     ]
    }
   ],
   "source": [
    "bpr_bundle3.train(s_users=sgd_users, s_pos_items=sgd_pos_items, s_neg_items=sgd_neg_items, \n",
    "            s_pos_len=sgd_pos_len, s_neg_len=sgd_neg_len, s_pos_diversity=sgd_pos_diversity,\n",
    "            s_neg_diversity=sgd_neg_diversity, \\\n",
    "                bd_pos_bundle = sgd_pos_bundles, bd_neg_bundle = sgd_neg_bundles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Current AUC mean (420000 samples): 0.88165ndspref1000\n"
     ]
    }
   ],
   "source": [
    "res3 = bpr_bundle3.test_bundle(test_users, test_pos_items, test_neg_items, test_n1, test_n2,\\\n",
    "                       test_pos_diversity, test_neg_diversity,\\\n",
    "                      test_pos_bundles, test_neg_bundles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## multiplier douscount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class BPR_Buncle4(object):\n",
    "    def __init__(self, rank, bundle_size, n_users, n_items, n_tags, bundle_discount_arr, bundle_price_arr, lambda_u = 0.0025, lambda_i = 0.0025, lambda_j = 0.00025, lambda_d = 0.0025, lambda_p = 0.00025, lambda_A = 0.01, lambda_bias = 0.0, learning_rate = 0.05):\n",
    "        \n",
    "        self._rank = rank\n",
    "        self._bundle_rank = bundle_size + 1\n",
    "        self._n_users = n_users\n",
    "        self._n_items = n_items\n",
    "        self._n_bundle = len(bundle_discount_map)\n",
    "        self.bundle_discount_arr = theano.shared(np.array(bundle_discount_arr))\n",
    "        self.bundle_price_arr = theano.shared(np.array(bundle_price_arr))\n",
    "        self._lambda_u = lambda_u\n",
    "        self._lambda_i = lambda_i\n",
    "        self._lambda_j = lambda_j\n",
    "        self._lambda_d = lambda_d\n",
    "        self._lambda_p = lambda_p\n",
    "        self._lambda_bias = lambda_bias\n",
    "        self._lambda_A = lambda_A\n",
    "        self._learning_rate = learning_rate\n",
    "        \n",
    "        self._n_tags = n_tags\n",
    "        self._configure_theano()\n",
    "        self._generate_train_model_item_function()\n",
    "        self._generate_test_model_function()\n",
    "\n",
    "    def _configure_theano(self):\n",
    "        theano.config.mode = 'FAST_RUN'\n",
    "        theano.config.floatX = 'float32'\n",
    "    \n",
    "    def _generate_train_model_item_function(self):\n",
    "        u = T.lvector('u')\n",
    "        i = T.lmatrix('i')\n",
    "        j = T.lmatrix('j')\n",
    "        n1 = T.lvector('n1')#num of items in pos bundle\n",
    "        n2 = T.lvector('n2')\n",
    "        di = T.dvector('di')#cb\n",
    "        dj = T.dvector('dj')#cb\n",
    "        bi = T.lvector('bi')\n",
    "        bj = T.lvector('bj')\n",
    "        \n",
    "        self.W1 = bpr_item.W #Pu\n",
    "        self.H1 = theano.shared(H_item.astype('float32'), name='H')#Qi\n",
    "        self.B1 = theano.shared(B_item.astype('float32'), name='B')#Bi\n",
    "        self.TG1 = theano.shared(A_item.astype('float32'), name='TG')\n",
    "        self.theta1 = bpr_item.theta\n",
    "        \n",
    "        self.M1 = theano.shared(numpy.random.random((self._rank, self._rank)).astype('float64'), name='M1')#w\n",
    "        self.M2 = theano.shared(numpy.random.random((self._rank, self._rank)).astype('float64'), name='M2')#u\n",
    "        self.BDu = theano.shared(numpy.random.random((self._n_users)).astype('float64'), name='bdu')#discount\n",
    "        self.K = theano.shared(numpy.random.rand(), name='K')#k\n",
    "        self.D = theano.shared(numpy.random.rand(), name='D')#C\n",
    "        self.N = theano.shared(numpy.random.random(self._bundle_rank).astype('float32'), name='N')#Nb\n",
    "        self.A1 = theano.shared(numpy.random.random((self._n_tags, self._n_tags)), name='A1')\n",
    "        self.A2 = theano.shared(numpy.random.random((self._n_tags, self._n_tags)), name='A2')\n",
    "                                \n",
    "        discount_price_i =  self.bundle_price_arr[bi] / (1 - self.bundle_discount_arr[bi]) * self.bundle_discount_arr[bi]\n",
    "        discount_price_j =  self.bundle_price_arr[bj] / (1 - self.bundle_discount_arr[bj]) * self.bundle_discount_arr[bj]\n",
    "        \n",
    "        x_ui = T.dot(T.dot(self.W1[u],self.M2), T.dot(self.M1, self.H1[i].sum(axis=1).T/n1)).diagonal() + self.K*(self.B1[i].T/n1).T.sum(axis=1) + self.N[n1] + self.D*di + T.dot(T.dot(self.theta1[u],self.A1), T.dot(self.A2, self.TG1[i].sum(axis=1).T/n1)).diagonal()\\\n",
    "                + self.BDu[u] * T.nnet.sigmoid(theano.tensor.tan(self.bundle_discount_arr[bi] * theano.tensor.constant(np.pi).eval()))\n",
    "        x_uj = T.dot(T.dot(self.W1[u],self.M2), T.dot(self.M1, self.H1[j].sum(axis=1).T/n2)).diagonal() + self.K*(self.B1[j].T/n2).T.sum(axis=1) + self.N[n2] + self.D*dj + T.dot(T.dot(self.theta1[u],self.A1), T.dot(self.A2, self.TG1[j].sum(axis=1).T/n1)).diagonal()\\\n",
    "                + self.BDu[u] * T.nnet.sigmoid(theano.tensor.tan(self.bundle_discount_arr[bj] * theano.tensor.constant(np.pi).eval()))\n",
    "        \n",
    "        \n",
    "        x_uij = T.nnet.sigmoid(x_ui-x_uj)\n",
    "        obj = T.sum(T.log(x_uij) - self._lambda_u * (self.M1 ** 2).sum() - \\\n",
    "                    self._lambda_u * (self.M2 ** 2).sum()  - self._lambda_d * (self.K**2) - self._lambda_d * (self.D**2)\\\n",
    "                    -self._lambda_p * (self.N[n2]**2) - self._lambda_p * (self.N[n1]**2)) - self._lambda_A * (self.A1 ** 2).sum() - self._lambda_A * (self.A2 ** 2).sum()\n",
    "        cost = - obj\n",
    "\n",
    "        g_cost_M1 = T.grad(cost=cost, wrt=self.M1)\n",
    "        g_cost_M2 = T.grad(cost=cost, wrt=self.M2)\n",
    "        g_cost_K = T.grad(cost=cost, wrt=self.K)\n",
    "        g_cost_N = T.grad(cost=cost, wrt=self.N)\n",
    "        g_cost_D = T.grad(cost=cost, wrt=self.D)\n",
    "        g_cost_A1 = T.grad(cost=cost, wrt=self.A1)\n",
    "        g_cost_A2 = T.grad(cost=cost, wrt=self.A2)\n",
    "        g_cost_BDu = T.grad(cost=cost, wrt=self.BDu)\n",
    "        \n",
    "        updates = [(self.M1, self.M1 - self._learning_rate * .001* g_cost_M1), (self.M2, self.M2 - self._learning_rate *.001* g_cost_M2), \n",
    "                   (self.K, self.K - self._learning_rate * .001*g_cost_K), (self.N, self.N - self._learning_rate *g_cost_N),\n",
    "                  (self.D, self.D - self._learning_rate * g_cost_D), \n",
    "                   (self.A1, self.A1-self._learning_rate * .001 * g_cost_A1), (self.A2, self.A2-self._learning_rate * .001 * g_cost_A2),\n",
    "                  (self.BDu, self.BDu-self._learning_rate * .001 * g_cost_BDu)]\n",
    "\n",
    "        self.train_model_item = theano.function(inputs=[u, i, j, n1, n2, di, dj, bi, bj], outputs=cost, updates=updates)\n",
    "\n",
    "    \n",
    "    def train(self, s_users=None, s_pos_items=None, s_neg_items=None, s_pos_len=None, s_neg_len=None,\n",
    "             s_pos_diversity=None, s_neg_diversity=None, bd_pos_bundle = None, bd_neg_bundle = None, batch_size=1000):\n",
    "        \n",
    "        if len(s_users) < batch_size:\n",
    "            sys.stderr.write(\"WARNING: Batch size is greater than number of training samples, switching to a batch size of %s\\n\" % str(len(train_data)))\n",
    "            batch_size = len(s_users)\n",
    "        \n",
    "        sgd_users, sgd_pos_items, sgd_neg_items = s_users, s_pos_items, s_neg_items\n",
    "        n_sgd_samples = len(s_users)\n",
    "\n",
    "        z = 0\n",
    "        t2 = t1 = t0 = time.time()\n",
    "        while (z+1)*batch_size < n_sgd_samples:\n",
    "            \n",
    "            self.train_model_item(\n",
    "                sgd_users[z*batch_size: (z+1)*batch_size],\n",
    "                sgd_pos_items[z*batch_size: (z+1)*batch_size],\n",
    "                sgd_neg_items[z*batch_size: (z+1)*batch_size],\n",
    "                s_pos_len[z*batch_size: (z+1)*batch_size],\n",
    "                s_neg_len[z*batch_size: (z+1)*batch_size],\n",
    "                s_pos_diversity[z*batch_size: (z+1)*batch_size],\n",
    "                s_neg_diversity[z*batch_size: (z+1)*batch_size],\n",
    "                bd_pos_bundle[z*batch_size: (z+1)*batch_size],\n",
    "                bd_neg_bundle[z*batch_size: (z+1)*batch_size]\n",
    "            )\n",
    "            z += 1\n",
    "            t2 = time.time()\n",
    "            sys.stderr.write(\"\\rProcessed %s ( %.2f%% ) in %.4f seconds\" %(str(z*batch_size), 100.0 * float(z*batch_size)/n_sgd_samples, t2 - t1))\n",
    "            sys.stderr.flush()\n",
    "            t1 = t2\n",
    "        if n_sgd_samples > 0:\n",
    "            sys.stderr.write(\"\\nTotal training time %.2f seconds; %e per sample\\n\" % (t2 - t0, (t2 - t0)/n_sgd_samples))\n",
    "            sys.stderr.flush()\n",
    "    \n",
    "    def _generate_test_model_function(self):\n",
    "        u = T.lvector('u')\n",
    "        i = T.lmatrix('i')\n",
    "        j = T.lmatrix('j')\n",
    "        n1 = T.lvector('n1')\n",
    "        n2 = T.lvector('n2')\n",
    "        di = T.dvector('di')\n",
    "        dj = T.dvector('dj')\n",
    "        bi = T.lvector('bi')\n",
    "        bj = T.lvector('bj')\n",
    "        \n",
    "#         x_ui = T.dot(T.dot(self.W1[u],self.M2), T.dot(self.M1, self.H1[i].sum(axis=1).T/n1)).diagonal() + self.K*(self.B1[i].T/n1).T.sum(axis=1) + self.N[n1] + self.D*di + T.dot(T.dot(self.theta1[u],self.A1), T.dot(self.A2, self.TG1[i].sum(axis=1).T/n1)).diagonal()\\\n",
    "#                 + self.BDu[u] * T.nnet.sigmoid(theano.tensor.tan(self.bundle_discount_arr[bi] * theano.tensor.constant(np.pi).eval()))\n",
    "#         x_uj = T.dot(T.dot(self.W1[u],self.M2), T.dot(self.M1, self.H1[j].sum(axis=1).T/n2)).diagonal() + self.K*(self.B1[j].T/n2).T.sum(axis=1) + self.N[n2] + self.D*dj + T.dot(T.dot(self.theta1[u],self.A1), T.dot(self.A2, self.TG1[j].sum(axis=1).T/n1)).diagonal()\\\n",
    "#                 + self.BDu[u] * T.nnet.sigmoid(theano.tensor.tan(self.bundle_discount_arr[bj] * theano.tensor.constant(np.pi).eval()))\n",
    "        x_ui = T.dot(T.dot(self.W1[u],self.M2), T.dot(self.M1, self.H1[i].sum(axis=1).T/n1)).diagonal() + self.K*(self.B1[i].T/n1).T.sum(axis=1) + self.N[n1] + self.D*di + T.dot(T.dot(self.theta1[u],self.A1), T.dot(self.A2, self.TG1[i].sum(axis=1).T/n1)).diagonal()\\\n",
    "                + self.BDu * T.nnet.sigmoid(theano.tensor.tan(self.bundle_discount_arr[bi] * theano.tensor.constant(np.pi).eval()))\n",
    "        x_uj = T.dot(T.dot(self.W1[u],self.M2), T.dot(self.M1, self.H1[j].sum(axis=1).T/n2)).diagonal() + self.K*(self.B1[j].T/n2).T.sum(axis=1) + self.N[n2] + self.D*dj + T.dot(T.dot(self.theta1[u],self.A1), T.dot(self.A2, self.TG1[j].sum(axis=1).T/n1)).diagonal()\\\n",
    "                + self.BDu * T.nnet.sigmoid(theano.tensor.tan(self.bundle_discount_arr[bj] * theano.tensor.constant(np.pi).eval()))\n",
    "\n",
    "#         y_ui = x_ui - self.BDu[u] * T.nnet.sigmoid(theano.tensor.tan(self.bundle_discount_arr[bi] * theano.tensor.constant(np.pi).eval()))\n",
    "#         y_uj = x_uj - self.BDu[u] * T.nnet.sigmoid(theano.tensor.tan(self.bundle_discount_arr[bj] * theano.tensor.constant(np.pi).eval()))\n",
    "        y_ui = x_ui - self.BDu * T.nnet.sigmoid(theano.tensor.tan(self.bundle_discount_arr[bi] * theano.tensor.constant(np.pi).eval()))\n",
    "        y_uj = x_uj - self.BDu * T.nnet.sigmoid(theano.tensor.tan(self.bundle_discount_arr[bj] * theano.tensor.constant(np.pi).eval()))\n",
    "\n",
    "#         x_uij = x_ui-x_uj\n",
    "        x_uij = y_ui-y_uj\n",
    "        #self.test_model = theano.function(inputs=[u, i, j, n1, n2, di, dj, bi, bj], outputs=x_uij)\n",
    "        out = [x_uij, x_ui, x_uj, y_ui, y_uj]\n",
    "        self.test_model = theano.function(inputs=[u, i, j, n1, n2, di, dj, bi, bj], outputs= out)\n",
    "        \n",
    "    def test_bundle(self, sgd_users, sgd_pos_items, sgd_neg_items, s_pos_len, s_neg_len,\n",
    "                    s_pos_diversity, s_neg_diversity, bd_pos_bundle, bd_neg_bundle, batch_size=1000):\n",
    "        \n",
    "        auc_values = []\n",
    "        z = 0\n",
    "        t2 = t1 = t0 = time.time()\n",
    "        n_sgd_samples = len(sgd_users)\n",
    "        x_uis, x_ujs, y_uis, y_ujs = [], [], [], []\n",
    "        while (z+1)*batch_size < n_sgd_samples:\n",
    "            pref_list, xu1, xu2, yu1, yu2 =self.test_model(\n",
    "                sgd_users[z*batch_size: (z+1)*batch_size],\n",
    "                sgd_pos_items[z*batch_size: (z+1)*batch_size],\n",
    "                sgd_neg_items[z*batch_size: (z+1)*batch_size],\n",
    "                s_pos_len[z*batch_size: (z+1)*batch_size],\n",
    "                s_neg_len[z*batch_size: (z+1)*batch_size],\n",
    "                s_pos_diversity[z*batch_size: (z+1)*batch_size],\n",
    "                s_neg_diversity[z*batch_size: (z+1)*batch_size],\n",
    "                bd_pos_bundle[z*batch_size: (z+1)*batch_size],\n",
    "                bd_neg_bundle[z*batch_size: (z+1)*batch_size]\n",
    "            )\n",
    "            x_uis += [x for x in xu1]\n",
    "            x_ujs += [x for x in xu2]\n",
    "            y_uis += [x for x in yu1]\n",
    "            y_ujs += [x for x in yu2]\n",
    "            z += 1\n",
    "            t2 = time.time()\n",
    "            sys.stderr.write(\"\\rProcessed %s ( %.2f%% ) in %.4f seconds\" %(str(z*batch_size), 100.0 * float(z*batch_size)/n_sgd_samples, t2 - t1))\n",
    "            t1 = t2\n",
    "            \n",
    "            sys.stderr.write(\"pref%d\" % len(pref_list))\n",
    "            sys.stderr.flush()\n",
    "            auc = np.sum([1.0 if a > 0.0 else 0.0 for a in pref_list])\n",
    "            auc /= batch_size\n",
    "            \n",
    "            auc_values.append(auc)\n",
    "            sys.stderr.write(\"\\rCurrent AUC mean (%s samples): %0.5f\" % (str(z*batch_size), numpy.mean(auc_values)))\n",
    "            sys.stderr.flush()\n",
    "        sys.stderr.write(\"\\n\")\n",
    "        sys.stderr.flush()\n",
    "        return numpy.mean(auc_values)\n",
    "        #return [x_uis, x_ujs, y_uis, y_ujs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [],
   "source": [
    "bpr_bundle4 = BPR_Buncle4(10, max_bundle_size, num_users, num_items, \\\n",
    "                        num_tags, bundle_discount_arr, bundle_price_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed 2101000 ( 99.99% ) in 0.3186 seconds\n",
      "Total training time 847.38 seconds; 4.032961e-04 per sample\n"
     ]
    }
   ],
   "source": [
    "bpr_bundle4.train(s_users=sgd_users, s_pos_items=sgd_pos_items, s_neg_items=sgd_neg_items, \n",
    "            s_pos_len=sgd_pos_len, s_neg_len=sgd_neg_len, s_pos_diversity=sgd_pos_diversity,\n",
    "            s_neg_diversity=sgd_neg_diversity, \\\n",
    "                bd_pos_bundle = sgd_pos_bundles, bd_neg_bundle = sgd_neg_bundles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Current AUC mean (420000 samples): 0.93170ndspref1000\n"
     ]
    }
   ],
   "source": [
    "res4 = bpr_bundle4.test_bundle(test_users, test_pos_items, test_neg_items, test_n1, test_n2,\\\n",
    "                       test_pos_diversity, test_neg_diversity,\\\n",
    "                      test_pos_bundles, test_neg_bundles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BPR_Buncle5(object):\n",
    "    def __init__(self, rank, bundle_size, n_users, n_items, n_tags, bundle_discount_arr, bundle_price_arr, lambda_u = 0.0025, lambda_i = 0.0025, lambda_j = 0.00025, lambda_d = 0.0025, lambda_p = 0.00025, lambda_A = 0.01, lambda_bias = 0.0, learning_rate = 0.05):\n",
    "        \n",
    "        self._rank = rank\n",
    "        self._bundle_rank = bundle_size + 1\n",
    "        self._n_users = n_users\n",
    "        self._n_items = n_items\n",
    "        self._n_bundle = len(bundle_discount_map)\n",
    "        self.bundle_discount_arr = theano.shared(np.array(bundle_discount_arr))\n",
    "        self.bundle_price_arr = theano.shared(np.array(bundle_price_arr))\n",
    "        self._lambda_u = lambda_u\n",
    "        self._lambda_i = lambda_i\n",
    "        self._lambda_j = lambda_j\n",
    "        self._lambda_d = lambda_d\n",
    "        self._lambda_p = lambda_p\n",
    "        self._lambda_bias = lambda_bias\n",
    "        self._lambda_A = lambda_A\n",
    "        self._learning_rate = learning_rate\n",
    "        \n",
    "        self._n_tags = n_tags\n",
    "        self._configure_theano()\n",
    "        self._generate_train_model_item_function()\n",
    "        self._generate_test_model_function()\n",
    "\n",
    "    def _configure_theano(self):\n",
    "        theano.config.mode = 'FAST_RUN'\n",
    "        theano.config.floatX = 'float32'\n",
    "    \n",
    "    def _generate_train_model_item_function(self):\n",
    "        u = T.lvector('u')\n",
    "        i = T.lmatrix('i')\n",
    "        j = T.lmatrix('j')\n",
    "        n1 = T.lvector('n1')#num of items in pos bundle\n",
    "        n2 = T.lvector('n2')\n",
    "        di = T.dvector('di')#cb\n",
    "        dj = T.dvector('dj')#cb\n",
    "        bi = T.lvector('bi')\n",
    "        bj = T.lvector('bj')\n",
    "        \n",
    "        self.W1 = bpr_item.W #Pu\n",
    "        self.H1 = theano.shared(H_item.astype('float32'), name='H')#Qi\n",
    "        self.B1 = theano.shared(B_item.astype('float32'), name='B')#Bi\n",
    "        self.TG1 = theano.shared(A_item.astype('float32'), name='TG')\n",
    "        self.theta1 = bpr_item.theta\n",
    "        \n",
    "        self.M1 = theano.shared(numpy.random.random((self._rank, self._rank)).astype('float64'), name='M1')#w\n",
    "        self.M2 = theano.shared(numpy.random.random((self._rank, self._rank)).astype('float64'), name='M2')#u\n",
    "        self.BDu = theano.shared(numpy.random.rand(), name='bdu')#discount\n",
    "        self.K = theano.shared(numpy.random.rand(), name='K')#k\n",
    "        self.D = theano.shared(numpy.random.rand(), name='D')#C\n",
    "        self.N = theano.shared(numpy.random.random(self._bundle_rank).astype('float32'), name='N')#Nb\n",
    "        self.A1 = theano.shared(numpy.random.random((self._n_tags, self._n_tags)), name='A1')\n",
    "        self.A2 = theano.shared(numpy.random.random((self._n_tags, self._n_tags)), name='A2')\n",
    "                                \n",
    "        discount_price_i =  self.bundle_price_arr[bi] / (1 - self.bundle_discount_arr[bi]) * self.bundle_discount_arr[bi]\n",
    "        discount_price_j =  self.bundle_price_arr[bj] / (1 - self.bundle_discount_arr[bj]) * self.bundle_discount_arr[bj]\n",
    "        \n",
    "        x_ui = T.dot(T.dot(self.W1[u],self.M2), T.dot(self.M1, self.H1[i].sum(axis=1).T/n1)).diagonal() + self.K*(self.B1[i].T/n1).T.sum(axis=1) + self.N[n1] + self.D*di + T.dot(T.dot(self.theta1[u],self.A1), T.dot(self.A2, self.TG1[i].sum(axis=1).T/n1)).diagonal()\\\n",
    "                + self.BDu * T.nnet.sigmoid(theano.tensor.tan(self.bundle_discount_arr[bi] * theano.tensor.constant(np.pi).eval()))\n",
    "        x_uj = T.dot(T.dot(self.W1[u],self.M2), T.dot(self.M1, self.H1[j].sum(axis=1).T/n2)).diagonal() + self.K*(self.B1[j].T/n2).T.sum(axis=1) + self.N[n2] + self.D*dj + T.dot(T.dot(self.theta1[u],self.A1), T.dot(self.A2, self.TG1[j].sum(axis=1).T/n1)).diagonal()\\\n",
    "                + self.BDu * T.nnet.sigmoid(theano.tensor.tan(self.bundle_discount_arr[bj] * theano.tensor.constant(np.pi).eval()))\n",
    "        \n",
    "        \n",
    "        x_uij = T.nnet.sigmoid(x_ui-x_uj)\n",
    "        obj = T.sum(T.log(x_uij) - self._lambda_u * (self.M1 ** 2).sum() - \\\n",
    "                    self._lambda_u * (self.M2 ** 2).sum()  - self._lambda_d * (self.K**2) - self._lambda_d * (self.D**2)\\\n",
    "                    -self._lambda_p * (self.N[n2]**2) - self._lambda_p * (self.N[n1]**2)) - self._lambda_A * (self.A1 ** 2).sum() - self._lambda_A * (self.A2 ** 2).sum()\\\n",
    "                    - 0.001 * (self.BDu ** 2)\n",
    "        cost = - obj\n",
    "\n",
    "        g_cost_M1 = T.grad(cost=cost, wrt=self.M1)\n",
    "        g_cost_M2 = T.grad(cost=cost, wrt=self.M2)\n",
    "        g_cost_K = T.grad(cost=cost, wrt=self.K)\n",
    "        g_cost_N = T.grad(cost=cost, wrt=self.N)\n",
    "        g_cost_D = T.grad(cost=cost, wrt=self.D)\n",
    "        g_cost_A1 = T.grad(cost=cost, wrt=self.A1)\n",
    "        g_cost_A2 = T.grad(cost=cost, wrt=self.A2)\n",
    "        g_cost_BDu = T.grad(cost=cost, wrt=self.BDu)\n",
    "        \n",
    "        updates = [(self.M1, self.M1 - self._learning_rate * .001* g_cost_M1), (self.M2, self.M2 - self._learning_rate *.001* g_cost_M2), \n",
    "                   (self.K, self.K - self._learning_rate * .001*g_cost_K), (self.N, self.N - self._learning_rate *g_cost_N),\n",
    "                  (self.D, self.D - self._learning_rate * g_cost_D), \n",
    "                   (self.A1, self.A1-self._learning_rate * .001 * g_cost_A1), (self.A2, self.A2-self._learning_rate * .001 * g_cost_A2),\n",
    "                  (self.BDu, self.BDu-self._learning_rate * .001 * g_cost_BDu)]\n",
    "\n",
    "        self.train_model_item = theano.function(inputs=[u, i, j, n1, n2, di, dj, bi, bj], outputs=cost, updates=updates)\n",
    "\n",
    "    \n",
    "    def train(self, s_users=None, s_pos_items=None, s_neg_items=None, s_pos_len=None, s_neg_len=None,\n",
    "             s_pos_diversity=None, s_neg_diversity=None, bd_pos_bundle = None, bd_neg_bundle = None, batch_size=1000):\n",
    "        \n",
    "        if len(s_users) < batch_size:\n",
    "            sys.stderr.write(\"WARNING: Batch size is greater than number of training samples, switching to a batch size of %s\\n\" % str(len(train_data)))\n",
    "            batch_size = len(s_users)\n",
    "        \n",
    "        sgd_users, sgd_pos_items, sgd_neg_items = s_users, s_pos_items, s_neg_items\n",
    "        n_sgd_samples = len(s_users)\n",
    "\n",
    "        z = 0\n",
    "        t2 = t1 = t0 = time.time()\n",
    "        while (z+1)*batch_size < n_sgd_samples:\n",
    "            \n",
    "            self.train_model_item(\n",
    "                sgd_users[z*batch_size: (z+1)*batch_size],\n",
    "                sgd_pos_items[z*batch_size: (z+1)*batch_size],\n",
    "                sgd_neg_items[z*batch_size: (z+1)*batch_size],\n",
    "                s_pos_len[z*batch_size: (z+1)*batch_size],\n",
    "                s_neg_len[z*batch_size: (z+1)*batch_size],\n",
    "                s_pos_diversity[z*batch_size: (z+1)*batch_size],\n",
    "                s_neg_diversity[z*batch_size: (z+1)*batch_size],\n",
    "                bd_pos_bundle[z*batch_size: (z+1)*batch_size],\n",
    "                bd_neg_bundle[z*batch_size: (z+1)*batch_size]\n",
    "            )\n",
    "            z += 1\n",
    "            t2 = time.time()\n",
    "            sys.stderr.write(\"\\rProcessed %s ( %.2f%% ) in %.4f seconds\" %(str(z*batch_size), 100.0 * float(z*batch_size)/n_sgd_samples, t2 - t1))\n",
    "            sys.stderr.flush()\n",
    "            t1 = t2\n",
    "        if n_sgd_samples > 0:\n",
    "            sys.stderr.write(\"\\nTotal training time %.2f seconds; %e per sample\\n\" % (t2 - t0, (t2 - t0)/n_sgd_samples))\n",
    "            sys.stderr.flush()\n",
    "    \n",
    "    def _generate_test_model_function(self):\n",
    "        u = T.lvector('u')\n",
    "        i = T.lmatrix('i')\n",
    "        j = T.lmatrix('j')\n",
    "        n1 = T.lvector('n1')\n",
    "        n2 = T.lvector('n2')\n",
    "        di = T.dvector('di')\n",
    "        dj = T.dvector('dj')\n",
    "        bi = T.lvector('bi')\n",
    "        bj = T.lvector('bj')\n",
    "        \n",
    "        x_ui = T.dot(T.dot(self.W1[u],self.M2), T.dot(self.M1, self.H1[i].sum(axis=1).T/n1)).diagonal() + self.K*(self.B1[i].T/n1).T.sum(axis=1) + self.N[n1] + self.D*di + T.dot(T.dot(self.theta1[u],self.A1), T.dot(self.A2, self.TG1[i].sum(axis=1).T/n1)).diagonal()\\\n",
    "                + self.BDu * T.nnet.sigmoid(theano.tensor.tan(self.bundle_discount_arr[bi] * theano.tensor.constant(np.pi).eval()))\n",
    "        x_uj = T.dot(T.dot(self.W1[u],self.M2), T.dot(self.M1, self.H1[j].sum(axis=1).T/n2)).diagonal() + self.K*(self.B1[j].T/n2).T.sum(axis=1) + self.N[n2] + self.D*dj + T.dot(T.dot(self.theta1[u],self.A1), T.dot(self.A2, self.TG1[j].sum(axis=1).T/n1)).diagonal()\\\n",
    "                + self.BDu * T.nnet.sigmoid(theano.tensor.tan(self.bundle_discount_arr[bj] * theano.tensor.constant(np.pi).eval()))\n",
    "\n",
    "        y_ui = x_ui - self.BDu * T.nnet.sigmoid(theano.tensor.tan(self.bundle_discount_arr[bi] * theano.tensor.constant(np.pi).eval()))\n",
    "        y_uj = x_uj - self.BDu * T.nnet.sigmoid(theano.tensor.tan(self.bundle_discount_arr[bj] * theano.tensor.constant(np.pi).eval()))\n",
    "\n",
    "        x_uij = x_ui-x_uj\n",
    "#         x_uij = y_ui-y_uj\n",
    "        #self.test_model = theano.function(inputs=[u, i, j, n1, n2, di, dj, bi, bj], outputs=x_uij)\n",
    "        out = [x_uij, x_ui, x_uj, y_ui, y_uj]\n",
    "        self.test_model = theano.function(inputs=[u, i, j, n1, n2, di, dj, bi, bj], outputs= out)\n",
    "        \n",
    "    def test_bundle(self, sgd_users, sgd_pos_items, sgd_neg_items, s_pos_len, s_neg_len,\n",
    "                    s_pos_diversity, s_neg_diversity, bd_pos_bundle, bd_neg_bundle, batch_size=1000):\n",
    "        \n",
    "        auc_values = []\n",
    "        z = 0\n",
    "        t2 = t1 = t0 = time.time()\n",
    "        n_sgd_samples = len(sgd_users)\n",
    "        x_uis, x_ujs, y_uis, y_ujs = [], [], [], []\n",
    "        while (z+1)*batch_size < n_sgd_samples:\n",
    "            pref_list, xu1, xu2, yu1, yu2 =self.test_model(\n",
    "                sgd_users[z*batch_size: (z+1)*batch_size],\n",
    "                sgd_pos_items[z*batch_size: (z+1)*batch_size],\n",
    "                sgd_neg_items[z*batch_size: (z+1)*batch_size],\n",
    "                s_pos_len[z*batch_size: (z+1)*batch_size],\n",
    "                s_neg_len[z*batch_size: (z+1)*batch_size],\n",
    "                s_pos_diversity[z*batch_size: (z+1)*batch_size],\n",
    "                s_neg_diversity[z*batch_size: (z+1)*batch_size],\n",
    "                bd_pos_bundle[z*batch_size: (z+1)*batch_size],\n",
    "                bd_neg_bundle[z*batch_size: (z+1)*batch_size]\n",
    "            )\n",
    "            x_uis += [x for x in xu1]\n",
    "            x_ujs += [x for x in xu2]\n",
    "            y_uis += [x for x in yu1]\n",
    "            y_ujs += [x for x in yu2]\n",
    "            z += 1\n",
    "            t2 = time.time()\n",
    "            sys.stderr.write(\"\\rProcessed %s ( %.2f%% ) in %.4f seconds\" %(str(z*batch_size), 100.0 * float(z*batch_size)/n_sgd_samples, t2 - t1))\n",
    "            t1 = t2\n",
    "            \n",
    "            sys.stderr.write(\"pref%d\" % len(pref_list))\n",
    "            sys.stderr.flush()\n",
    "            auc = np.sum([1.0 if a > 0.0 else 0.0 for a in pref_list])\n",
    "            auc /= batch_size\n",
    "            \n",
    "            auc_values.append(auc)\n",
    "            sys.stderr.write(\"\\rCurrent AUC mean (%s samples): %0.5f\" % (str(z*batch_size), numpy.mean(auc_values)))\n",
    "            sys.stderr.flush()\n",
    "        sys.stderr.write(\"\\n\")\n",
    "        sys.stderr.flush()\n",
    "        return numpy.mean(auc_values)\n",
    "        #return [x_uis, x_ujs, y_uis, y_ujs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [],
   "source": [
    "bpr_bundle5 = BPR_Buncle5(10, max_bundle_size, num_users, num_items, \\\n",
    "                        num_tags, bundle_discount_arr, bundle_price_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed 2101000 ( 99.99% ) in 0.4221 seconds\n",
      "Total training time 858.89 seconds; 4.087711e-04 per sample\n"
     ]
    }
   ],
   "source": [
    "bpr_bundle5.train(s_users=sgd_users, s_pos_items=sgd_pos_items, s_neg_items=sgd_neg_items, \n",
    "            s_pos_len=sgd_pos_len, s_neg_len=sgd_neg_len, s_pos_diversity=sgd_pos_diversity,\n",
    "            s_neg_diversity=sgd_neg_diversity, \\\n",
    "                bd_pos_bundle = sgd_pos_bundles, bd_neg_bundle = sgd_neg_bundles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Current AUC mean (420000 samples): 0.93530ndspref1000\n"
     ]
    }
   ],
   "source": [
    "res5 = bpr_bundle5.test_bundle(test_users, test_pos_items, test_neg_items, test_n1, test_n2,\\\n",
    "                       test_pos_diversity, test_neg_diversity,\\\n",
    "                      test_pos_bundles, test_neg_bundles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# generate_bundle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder = '../data/new_data'\n",
    "user_item_map = pickle.load(open(folder + '/user_item_map.pkl','rb'))\n",
    "user_bundle_map = pickle.load(open(folder + '/user_bundle_map.pkl','rb'))\n",
    "items_set = set(pickle.load(open(folder + '/item_id_lookup.pkl','rb')).keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate_bundle(items_set, user, user_item_map, initial_size = 3, max_iteration = 1000, sample_size = 5):\n",
    "    current_bundle = np.random.choice(list(items_set - user_item_map[user]), initial_size )\n",
    "    \n",
    "    T=1000.0\n",
    "    \n",
    "    iteration = 0\n",
    "    while iteration < max_iteration:\n",
    "        iteration+=1\n",
    "        Gamma = bpr_item.H.eval()\n",
    "        print Gamma\n",
    "        #curr_diversity = compute_diversity(current_bundle, Gamma)\n",
    "#         user_set=[]\n",
    "#         pos_item_set=[]\n",
    "#         actual_item_set=[]\n",
    "#         neg_item_set=[]\n",
    "#         pos_item_count=[]\n",
    "#         neg_item_count=[]\n",
    "#         pos_diversity=[]\n",
    "#         neg_diversity=[]\n",
    "                \n",
    "#         candidate_items = set(np.random.choice(list(items_set - user_item_map[user]), sample_size))\n",
    "#         for item in current_bundle:\n",
    "#             if item in candidate_items:\n",
    "#                 candidate_items.remove(item)\n",
    "        \n",
    "    \n",
    "#         #Generating new bundles by adding and removing new items  \n",
    "#         for cand_item in candidate_items:\n",
    "#             #Add an item case\n",
    "#             if len(current_bundle)<10:\n",
    "#                 user_set.append(user)    \n",
    "#                 neg_item_set.append(add_bogus_items(current_bundle , max_bundle_size, len(items_set)))\n",
    "#                 neg_item_count.append(len(current_bundle))\n",
    "#                 neg_diversity.append(curr_diversity)         \n",
    "#                 new_bundle=list(current_bundle)\n",
    "#                 new_bundle.append(cand_item)\n",
    "#                 pos_item_count.append(len(new_bundle))\n",
    "#                 pos_diversity.append(compute_diversity(new_bundle, Gamma))\n",
    "#                 actual_item_set.append(new_bundle)\n",
    "#                 pos_item_set.append(add_bogus_items(new_bundle , max_bundle_size, len(items_set)))\n",
    "\n",
    "#             # Replace an item case\n",
    "#             for curr_item in current_bundle:\n",
    "#                 user_set.append(user)\n",
    "                \n",
    "#                 neg_item_set.append(add_bogus_items(current_bundle , max_bundle_size, len(items_set)))\n",
    "#                 neg_item_count.append(len(current_bundle))\n",
    "#                 neg_diversity.append(curr_diversity)\n",
    "                \n",
    "#                 new_bundle=list(current_bundle)\n",
    "#                 new_bundle.append(cand_item)\n",
    "#                 new_bundle.remove(curr_item)\n",
    "#                 pos_item_set.append(add_bogus_items(new_bundle , max_bundle_size, len(items_set)))\n",
    "#                 actual_item_set.append(new_bundle)\n",
    "#                 pos_item_count.append(len(new_bundle))\n",
    "#                 pos_diversity.append(compute_diversity(new_bundle, Gamma))\n",
    "         \n",
    "        \n",
    "#         # Remove an item case\n",
    "#         if len(current_bundle)>2:\n",
    "#             for curr_item in current_bundle:\n",
    "#                 user_set.append(user)\n",
    "\n",
    "#                 neg_item_set.append(add_bogus_items(current_bundle , max_bundle_size, len(items_set)))\n",
    "#                 neg_item_count.append(len(current_bundle))\n",
    "#                 neg_diversity.append(curr_diversity)\n",
    "\n",
    "#                 new_bundle=list(current_bundle)\n",
    "#                 new_bundle.remove(curr_item)\n",
    "#                 actual_item_set.append(new_bundle)\n",
    "#                 pos_item_set.append(add_bogus_items(new_bundle , max_bundle_size, len(items_set)))\n",
    "#                 pos_item_count.append(len(new_bundle))\n",
    "#                 pos_diversity.append(compute_diversity(new_bundle, Gamma))\n",
    "        \n",
    "                \n",
    "#         pref_score = bpr_cold.test_model(user_set, pos_item_set, neg_item_set, pos_item_count, \n",
    "#                                     neg_item_count, pos_diversity, neg_diversity)\n",
    "                                    \n",
    "        #print pref_score, pos_item_count, neg_item_count\n",
    "#         index = np.argmax(pref_score)\n",
    "#         #print \"Pref Score \", pref_score[index]\n",
    "#         if(pref_score[index]>0):\n",
    "#             current_bundle = actual_item_set[index]\n",
    "#         else:\n",
    "#             prob = np.exp(pref_score[index]/T)\n",
    "#             if prob < .00001:\n",
    "#                 break\n",
    "#             if np.random.rand() < prob:\n",
    "#                 current_bundle = actual_item_set[index]\n",
    "#         T=T*0.9\n",
    "    #print iteration\n",
    "    return current_bundle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def add_bogus_items(bundle, max_bundle_size, index):\n",
    "    item=list(bundle);\n",
    "    for i in range(len(item),max_bundle_size):\n",
    "        item.append(index)\n",
    "    return item\n",
    "\n",
    "def remove_bogus_items(bundle, max_bundle_size, index):\n",
    "    item=list(bundle);\n",
    "    i=0\n",
    "    while i< len(bundle):\n",
    "        if bundle[i]==index:\n",
    "            break\n",
    "        i+=1\n",
    "    return bundle[:i]\n",
    "\n",
    "def get_bundle_rank(user, new_bundle, bundle_item_map, bundle_diversity_map):\n",
    "    user_set=[]\n",
    "    pos_item_set=[]\n",
    "    neg_item_set=[]\n",
    "    pos_item_count=[]\n",
    "    neg_item_count=[]\n",
    "    pos_diversity=[]\n",
    "    neg_diversity=[]\n",
    "    \n",
    "    bundle_diversity=compute_diversity(new_bundle, Gamma)\n",
    "    for bundle_id,bundle in bundle_item_map.items():\n",
    "        user_set.append(user)\n",
    "        pos_item_set.append(add_bogus_items(bundle, max_bundle_size, len(items_set)))\n",
    "        neg_item_set.append(add_bogus_items(new_bundle, max_bundle_size, len(items_set)))\n",
    "        pos_item_count.append(len(bundle))\n",
    "        neg_item_count.append(len(new_bundle))\n",
    "        pos_diversity.append(bundle_diversity_map[bundle_id])\n",
    "        neg_diversity.append(bundle_diversity)\n",
    "        \n",
    "    pref_score = bpr_cold.test_model(user_set, pos_item_set, neg_item_set, pos_item_count, \n",
    "                                    neg_item_count, pos_diversity, neg_diversity)\n",
    "  \n",
    "    rank = np.sum([1.0 if p>0 else 0.0 for p in pref_score])\n",
    "    return rank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "sizes=[10]\n",
    "diversities=[]\n",
    "scores=[]\n",
    "bundle_sizes=[]\n",
    "for size in sizes:\n",
    "    aggregate_diversity=set()\n",
    "    pred_score=[]\n",
    "    b_size=[]\n",
    "#     generated_bundles=[]\n",
    "#     for user in sorted(user_bundle_map.keys())[:100]:\n",
    "#         new_bundle = generate_bundle(items_set, user, user_item_map, 4, 1000,size)\n",
    "#         break\n",
    "#         rank = get_bundle_rank(user, new_bundle, bundle_item_map, bundle_diversity_map)\n",
    "#         purchased_bundles = len(user_bundle_map[user])\n",
    "#         aggregate_diversity=aggregate_diversity.union(set(new_bundle))\n",
    "#         generated_bundles.append(new_bundle)\n",
    "#         pred_score.append(rank)\n",
    "#         b_size.append(len(new_bundle)*1.0)\n",
    "#         print 'Rank of user %d : %d, Size of bundle : %d, Bundles purchased : %d Aggregate diversity: %d Score: %f, Average bundle size: %f' %(user, \n",
    "#                                                                                      rank, \n",
    "#                                                                                      len(new_bundle),                                 \n",
    "#                                                                                      purchased_bundles,\n",
    "#                                                                                      len(aggregate_diversity),\n",
    "#                                                                                      1.0+np.mean(pred_score),\n",
    "#                                                                                      np.mean(b_size))\n",
    "#     diversities.append(len(aggregate_diversity))\n",
    "#     scores.append(1.0+np.mean(pred_score))\n",
    "#     bundle_sizes.append(np.mean(b_size)*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:py2]",
   "language": "python",
   "name": "conda-env-py2-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
