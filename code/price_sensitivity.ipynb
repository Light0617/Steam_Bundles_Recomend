{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import csv\n",
    "from lib import *\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "folder = '../data/processed_data'\n",
    "user_item_map=pickle.load(open(folder + '/user_item_map','rb'))\n",
    "item_id_lookup = pickle.load(open(folder + '/item_id_lookup','rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bundle_item_map=pickle.load(open(folder + '/bundle_item_map','rb'))\n",
    "user_bundle_map=pickle.load(open(folder +'/user_bundle_map','rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bundle_discount_map = pickle.load(open(folder +'/bundle_discount_map','rb'))\n",
    "bundle_final_price_map = pickle.load(open(folder +'/bundle_final_price_map','rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "itemID_price = pickle.load(open(folder +'/itemID_discount_map','rb'))\n",
    "user_price_means = pickle.load(open(folder +'/user_price_means','rb'))\n",
    "user_price_stds = pickle.load(open(folder +'/user_price_stds','rb'))\n",
    "# itemID_discount_map = pickle.load(open(folder +'/user_price_stds','rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_data = pickle.load(open(folder +'/all_data','rb'))\n",
    "all_item_data = pickle.load(open(folder +'/all_item_data','rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "random.shuffle(all_data)\n",
    "data_size=len(all_data)\n",
    "\n",
    "# Training data for bundle for bpr model\n",
    "training_data=all_data[:int(0.8*data_size)]\n",
    "test_data=all_data[int(0.8*data_size):]\n",
    "\n",
    "# Training data for items for bpr_item model\n",
    "training_data_2=all_item_data[:int(0.8*len(all_item_data))]\n",
    "test_data_2=all_item_data[int(0.8*len(all_item_data)):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29634 2819\n"
     ]
    }
   ],
   "source": [
    "num_users = len(user_item_map)\n",
    "num_items = len(item_id_lookup)\n",
    "print num_users, num_items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "itemID_price = [itemID_price[i] for i in range(len(itemID_price))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2819\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.909919028340081, 0, 0, 0, 0, 0, 0.7702702702702703, 0, 0, 0, 0, 0, 0, 0, 0, 0.7190635451505016]\n"
     ]
    }
   ],
   "source": [
    "print len(itemID_price)\n",
    "print itemID_price[:30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "10000000\n"
     ]
    }
   ],
   "source": [
    "# Generting training data for items through graph sampling.\n",
    "sgd_train_users_items, sgd_train_pos_items, sgd_train_neg_items\\\n",
    "        = graph_sampling(len(training_data_2)*30, training_data_2, user_item_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "sgd_test_users_items, sgd_test_pos_items, sgd_test_neg_items\\\n",
    "        = graph_sampling(len(test_data_2)*30, test_data_2, user_item_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "test_users_cold, test_pos_items_cold, test_neg_items_cold=\\\n",
    "        get_test_data_items(test_data_2, training_data_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['THEANO_FLAGS']='mode=FAST_RUN,device=gpu,lib.cnmem=0.7,floatX=float32'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# theano-bpr\n",
    "#\n",
    "# Copyright (c) 2014 British Broadcasting Corporation\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "# http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "\n",
    "import theano, numpy\n",
    "import theano.tensor as T\n",
    "import time\n",
    "import sys\n",
    "from collections import defaultdict\n",
    "\n",
    "class BPR_ItemP(object):\n",
    "\n",
    "    def __init__(self, rank, n_users, n_items, itemID_price,\\\n",
    "                 lambda_u = 0.0025, lambda_i = 0.0025, lambda_j = 0.00025, \\\n",
    "                 lambda_d = 0.0025, lambda_p = 0.00025, lambda_bias = 0.0, learning_rate = 0.05):\n",
    "        \n",
    "        self._rank = rank\n",
    "        self._rank2 = 3\n",
    "        self._n_users = n_users\n",
    "        self._n_items = n_items\n",
    "        self._lambda_u = lambda_u\n",
    "        self._lambda_i = lambda_i\n",
    "        self._lambda_j = lambda_j\n",
    "        self._lambda_d = lambda_d\n",
    "        self._lambda_p = lambda_p\n",
    "        self._lambda_bias = lambda_bias\n",
    "        self.itemID_price = theano.shared(np.array(itemID_price).astype('float32'), name='itemID_price')\n",
    "        self._learning_rate = learning_rate\n",
    "        self._configure_theano()\n",
    "        self._generate_train_model_function()\n",
    "        self._generate_test_model_function()\n",
    "\n",
    "    def _configure_theano(self):\n",
    "        \"\"\"\n",
    "          Configures Theano to run in fast mode\n",
    "          and using 32-bit floats. \n",
    "        \"\"\"\n",
    "        theano.config.mode = 'FAST_RUN'\n",
    "        theano.config.floatX = 'float32'\n",
    "\n",
    "    def _generate_train_model_function(self):\n",
    "        u = T.lvector('u')\n",
    "        i = T.lvector('i')\n",
    "        j = T.lvector('j')\n",
    "\n",
    "        h = numpy.random.random((self._n_items, self._rank))\n",
    "        b = numpy.random.random(self._n_items)\n",
    "        pu = numpy.random.random(self._n_users)\n",
    "#         px = numpy.random.random(self._n_items)\n",
    "        \n",
    "        self.W = theano.shared(numpy.random.random((self._n_users, self._rank)).astype('float32'), name='W')\n",
    "        self.H = theano.shared(h.astype('float32'), name='H')\n",
    "        self.B = theano.shared(b.astype('float32'), name='B')\n",
    "#         self.P = theano.shared(numpy.random.rand(), name='P')\n",
    "        self.Pu = theano.shared(pu.astype('float32'), name='Pu')\n",
    "#         self.Px = theano.shared(px.astype('float32'), name='Px')\n",
    "        \n",
    "        x_ui = T.dot(self.W[u], self.H[i].T).diagonal() + self.B[i]\n",
    "        x_uj = T.dot(self.W[u], self.H[j].T).diagonal() + self.B[j]\n",
    "        pi = self.itemID_price[i]\n",
    "        pj = self.itemID_price[j]\n",
    "        x_uij = T.nnet.sigmoid(x_ui - x_uj + (self.Pu[u]) *(pi - pj))\n",
    "\n",
    "        obj = T.sum(T.log(x_uij) - self._lambda_u * (self.W[u] ** 2).sum(axis=1) - \n",
    "                    self._lambda_i * (self.H[i] ** 2).sum(axis=1) - self._lambda_j * \n",
    "                    (self.H[j] ** 2).sum(axis=1) - self._lambda_bias * \n",
    "                    (self.B[i] ** 2 + self.B[j] ** 2))\n",
    "#                     - self._lambda_u * (self.Pu[u] ** 2))\n",
    "       \n",
    "    \n",
    "        cost = - obj\n",
    "\n",
    "        g_cost_W = T.grad(cost=cost, wrt=self.W)\n",
    "        g_cost_H = T.grad(cost=cost, wrt=self.H)\n",
    "        g_cost_B = T.grad(cost=cost, wrt=self.B)\n",
    "        g_cost_Pu = T.grad(cost=cost, wrt=self.Pu)\n",
    "        \n",
    "        updates = [(self.W, self.W - self._learning_rate * g_cost_W), (self.H, self.H - self._learning_rate * g_cost_H), \n",
    "                   (self.B, self.B - self._learning_rate * g_cost_B),\n",
    "                   (self.Pu, self.Pu - self._learning_rate * g_cost_Pu)]\n",
    "\n",
    "        self.train_model = theano.function(inputs=[u, i, j], outputs=cost, updates=updates)\n",
    "\n",
    "    \n",
    "    def train(self, sgd_users=None, sgd_pos_items=None, sgd_neg_items=None, batch_size=1000):\n",
    "        \"\"\"\n",
    "          Trains the BPR Matrix Factorisation model using Stochastic\n",
    "          Gradient Descent and minibatches over `train_data`.\n",
    "\n",
    "          `train_data` is an array of (user_index, item_index) tuples.\n",
    "\n",
    "          We first create a set of random samples from `train_data` for \n",
    "          training, of size `epochs` * size of `train_data`.\n",
    "\n",
    "          We then iterate through the resulting training samples by\n",
    "          batches of length `batch_size`, and run one iteration of gradient\n",
    "          descent for the batch.\n",
    "        \"\"\"\n",
    "        if len(sgd_pos_items) < batch_size:\n",
    "            sys.stderr.write(\"WARNING: Batch size is greater than number of training samples, switching to a batch size of %s\\n\" % str(len(train_data)))\n",
    "            batch_size = len(sgd_pos_items)\n",
    "            \n",
    "        n_sgd_samples = len(sgd_users)\n",
    "        \n",
    "        z = 0\n",
    "        t2 = t1 = t0 = time.time()\n",
    "        while (z+1)*batch_size < n_sgd_samples:\n",
    "            self.train_model(\n",
    "                sgd_users[z*batch_size: (z+1)*batch_size],\n",
    "                sgd_pos_items[z*batch_size: (z+1)*batch_size],\n",
    "                sgd_neg_items[z*batch_size: (z+1)*batch_size]\n",
    "            )\n",
    "            z += 1\n",
    "            t2 = time.time()\n",
    "            sys.stderr.write(\"\\rProcessed %s ( %.2f%% ) in %.4f seconds\" %(str(z*batch_size), 100.0 * float(z*batch_size)/n_sgd_samples, t2 - t1))\n",
    "            sys.stderr.flush()\n",
    "            t1 = t2\n",
    "        if n_sgd_samples > 0:\n",
    "            sys.stderr.write(\"\\nTotal training time %.2f seconds; %e per sample\\n\" % (t2 - t0, (t2 - t0)/n_sgd_samples))\n",
    "            sys.stderr.flush()\n",
    "            \n",
    "    def _generate_test_model_function(self):\n",
    "        \"\"\"\n",
    "          Computes item predictions for `user_index`.\n",
    "          Returns an array of prediction values for each item\n",
    "          in the dataset.\n",
    "        \"\"\"\n",
    "        u = T.lvector('u')\n",
    "        i = T.lvector('i')\n",
    "        j = T.lvector('j')\n",
    "\n",
    "        x_ui = T.dot(self.W[u], self.H[i].T).diagonal() + self.B[i]\n",
    "        x_uj = T.dot(self.W[u], self.H[j].T).diagonal() + self.B[j]\n",
    "        pi = self.itemID_price[i]\n",
    "        pj = self.itemID_price[j]\n",
    "        x_uij = x_ui - x_uj + (self.Pu[u]) *(pi - pj)\n",
    "\n",
    "        \n",
    "        self.test_model = theano.function(inputs=[u, i, j], outputs=x_uij)\n",
    "   \n",
    "    def test_bundle(self, sgd_users, sgd_pos_items, sgd_neg_items, batch_size=1000):\n",
    "        \"\"\"\n",
    "          Computes the Area Under Curve (AUC) on `test_data`.\n",
    "\n",
    "          `test_data` is an array of (user_index, item_index) tuples.\n",
    "\n",
    "          During this computation we ignore users and items\n",
    "          that didn't appear in the training data, to allow\n",
    "          for non-overlapping training and testing sets.\n",
    "        \"\"\"\n",
    "        \n",
    "        auc_values = []\n",
    "        z = 0\n",
    "        t2 = t1 = t0 = time.time()\n",
    "        n_sgd_samples = len(sgd_users)\n",
    "        while (z+1)*batch_size < n_sgd_samples:\n",
    "            pref_list=self.test_model(\n",
    "                sgd_users[z*batch_size: (z+1)*batch_size],\n",
    "                sgd_pos_items[z*batch_size: (z+1)*batch_size],\n",
    "                sgd_neg_items[z*batch_size: (z+1)*batch_size]\n",
    "            )\n",
    "            z += 1\n",
    "            t2 = time.time()\n",
    "            sys.stderr.write(\"\\rProcessed %s ( %.2f%% ) in %.4f seconds\" %(str(z*batch_size), 100.0 * float(z*batch_size)/n_sgd_samples, t2 - t1))\n",
    "            t1 = t2\n",
    "            \n",
    "            auc = np.sum([1.0 if a>0.0 else 0.0 for a in pref_list])\n",
    "            auc /= batch_size\n",
    "            \n",
    "            auc_values.append(auc)\n",
    "            sys.stderr.write(\"\\rCurrent AUC mean (%s samples): %0.5f\" % (str(z*batch_size), numpy.mean(auc_values)))\n",
    "            sys.stderr.flush()\n",
    "        \n",
    "        sys.stderr.write(\"\\n\")\n",
    "        sys.stderr.flush()\n",
    "        return numpy.mean(auc_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "bpr_item = BPR_ItemP(10, num_users, num_items, itemID_price)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed 21671000 ( 100.00% ) in 0.0203 seconds\n",
      "Total training time 370.49 seconds; 1.709609e-05 per sample\n"
     ]
    }
   ],
   "source": [
    "bpr_item.train(sgd_users=sgd_train_users_items, sgd_pos_items=sgd_train_pos_items, sgd_neg_items=sgd_train_neg_items)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Current AUC mean (66000 samples): 0.80833nds\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.80833333333333335"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bpr_item.test_bundle(test_users_cold, test_pos_items_cold, test_neg_items_cold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Current AUC mean (1269000 samples): 0.87988ds"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-77-693c47e5db94>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mbpr_item\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest_bundle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msgd_train_users_items\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msgd_train_pos_items\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msgd_train_neg_items\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-73-8b0dc659cd32>\u001b[0m in \u001b[0;36mtest_bundle\u001b[0;34m(self, sgd_users, sgd_pos_items, sgd_neg_items, batch_size)\u001b[0m\n\u001b[1;32m    180\u001b[0m             \u001b[0mauc_values\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mauc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m             \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstderr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\rCurrent AUC mean (%s samples): %0.5f\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mauc_values\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 182\u001b[0;31m             \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstderr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflush\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    183\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m         \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstderr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\n\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/light/anaconda/envs/py2/lib/python2.7/site-packages/ipykernel/iostream.pyc\u001b[0m in \u001b[0;36mflush\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    320\u001b[0m             \u001b[0mevt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mthreading\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEvent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    321\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpub_thread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mschedule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 322\u001b[0;31m             \u001b[0mevt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    323\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    324\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_flush\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/light/anaconda/envs/py2/lib/python2.7/threading.pyc\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    612\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__cond\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    613\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__flag\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 614\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__cond\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    615\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__flag\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    616\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/light/anaconda/envs/py2/lib/python2.7/threading.pyc\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    338\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m    \u001b[0;31m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    339\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 340\u001b[0;31m                 \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    341\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0m__debug__\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    342\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_note\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"%s.wait(): got it\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "bpr_item.test_bundle(sgd_train_users_items, sgd_train_pos_items, sgd_train_neg_items)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "with price Current AUC mean (20000 samples)\n",
      "cold:  0.85866666666666658\n",
      "train: 0.88515809145863134\n",
      "test: 0.50776850655344286\n"
     ]
    }
   ],
   "source": [
    "print 'with price Current AUC mean (20000 samples)'\n",
    "print 'cold:  0.84213636363636357'\n",
    "print 'train: 0.82246onds'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "withOUT price Current AUC mean (20000 samples)\n",
      "cold:  0.897121212121211999\n",
      "train: 0.88172414747819672\n",
      "test: 0.50183681004245895\n"
     ]
    }
   ],
   "source": [
    "print 'withOUT price Current AUC mean (20000 samples)'\n",
    "print 'cold:  0.90787878787878784'\n",
    "print 'train: 0.88172414747819672'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### original paper BPR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bpr_item0 = BPR_Item(10, num_users, num_items)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed 21671000 ( 100.00% ) in 0.0270 seconds\n",
      "Total training time 405.17 seconds; 1.869632e-05 per sample\n"
     ]
    }
   ],
   "source": [
    "bpr_item0.train(s_users=sgd_train_users_items, s_pos_items=sgd_train_pos_items, s_neg_items=sgd_train_neg_items)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Current AUC mean (66000 samples): 0.87635nds\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.87634848484848482"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bpr_item0.test_bundle(test_users_cold, test_pos_items_cold, test_neg_items_cold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed 1534000 ( 7.08% ) in 0.0129 seconds"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-28-3b564c836823>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mbpr_item0\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest_bundle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msgd_train_users_items\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msgd_train_pos_items\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msgd_train_neg_items\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/Users/light/Documents/2017Fall/Steam_Bundles_Recomend/code/lib.pyc\u001b[0m in \u001b[0;36mtest_bundle\u001b[0;34m(self, sgd_users, sgd_pos_items, sgd_neg_items, batch_size)\u001b[0m\n\u001b[1;32m    227\u001b[0m             \u001b[0mt1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mt2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    228\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 229\u001b[0;31m             \u001b[0mauc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1.0\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m>\u001b[0m\u001b[0;36m0.0\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;36m0.0\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ma\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpref_list\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    230\u001b[0m             \u001b[0mauc\u001b[0m \u001b[0;34m/=\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    231\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "bpr_item0.test_bundle(sgd_train_users_items, sgd_train_pos_items, sgd_train_neg_items)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bundle model begins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "# Generting training data for bundles through graph sampling.\n",
    "sgd_users, sgd_pos_bundles, sgd_neg_bundles = graph_sampling(len(training_data)*30, training_data, user_bundle_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# max_bundle_size=0\n",
    "# for bundle,items in bundle_item_map.items():\n",
    "#     if(len(items)>max_bundle_size):\n",
    "#         max_bundle_size=len(items)\n",
    "# print max_bundle_size\n",
    "max_bundle_size = np.max([len(bundle_item_map[key]) for key in bundle_item_map])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_items(bundle_id, max_bundle_size, index):\n",
    "    item=list(bundle_item_map[bundle_id]);\n",
    "    for i in range(len(item),max_bundle_size):\n",
    "        item.append(index)\n",
    "    return item\n",
    "\n",
    "sgd_pos_items=[get_items(b_id, max_bundle_size, num_items) for b_id in sgd_pos_bundles]\n",
    "sgd_neg_items=[get_items(b_id, max_bundle_size, num_items) for b_id in sgd_neg_bundles]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import jaccard_similarity_score\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "def compute_diversity_tags(app_data):\n",
    "    l=len(app_data)\n",
    "    app_data=[item_id_lookup[d] for d in app_data]\n",
    "    count=0.0\n",
    "    similarity=0.0\n",
    "    for i in range(l):\n",
    "        if app_data[i] in item_data_map:\n",
    "            for j in range(i+1,l):\n",
    "                if app_data[j] in item_data_map:\n",
    "                    count+=1\n",
    "                    similarity+=jaccard_similarity_score(get_feat(item_data_map[app_data[i]]['tags']),\n",
    "                                                         get_feat(item_data_map[app_data[j]]['tags']))\n",
    "    if count>0:\n",
    "        return 1-(similarity/count)\n",
    "    else:\n",
    "        return 0.0\n",
    "\n",
    "def compute_diversity_latent(items, H):\n",
    "    count=0.0\n",
    "    similarity=0.0\n",
    "    for i in range(len(items)):\n",
    "        for j in range(i + 1, len(items)):\n",
    "            x, y = items[i], items[j]\n",
    "            count+=1\n",
    "            similarity+=cosine_similarity(H[x].reshape(1,-1),H[y].reshape(1,-1))[0,0]\n",
    "    if count>0:\n",
    "        return 1-(similarity/count)\n",
    "    else:\n",
    "        return 0.0\n",
    "    \n",
    "def compute_diversity(app_data, H):\n",
    "    if H is not None:\n",
    "        return compute_diversity_latent(app_data, H)\n",
    "    else :\n",
    "        return compute_diversity_tags(app_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "H_item=bpr_item0.H.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "bundle_diversity_map=dict()\n",
    "for bundle,items in bundle_item_map.items():\n",
    "    bundle_diversity_map[bundle]=compute_diversity_latent(list(items), H_item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "sgd_pos_len=[len(bundle_item_map[b_id]) for b_id in sgd_pos_bundles]\n",
    "sgd_neg_len=[len(bundle_item_map[b_id]) for b_id in sgd_neg_bundles]\n",
    "sgd_pos_diversity=[bundle_diversity_map[b_id] for b_id in sgd_pos_bundles]\n",
    "sgd_neg_diversity=[bundle_diversity_map[b_id] for b_id in sgd_neg_bundles]\n",
    "sgd_pos_dicount=[bundle_discount_map[b_id] for b_id in sgd_pos_bundles]\n",
    "sgd_neg_dicount=[bundle_discount_map[b_id] for b_id in sgd_neg_bundles]\n",
    "sgd_pos_price =[bundle_final_price_map[b_id] for b_id in sgd_pos_bundles]\n",
    "sgd_neg_price =[bundle_final_price_map[b_id] for b_id in sgd_neg_bundles]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2101560 2101560 2101560\n"
     ]
    }
   ],
   "source": [
    "print len(sgd_neg_dicount), len(sgd_pos_diversity), len(sgd_pos_dicount)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_test_data_bundles1(test_data, train_data, n_items):\n",
    "    users=[]\n",
    "    pos_items=[]\n",
    "    neg_items=[]\n",
    "    n1=[]\n",
    "    n2=[]\n",
    "    pos_diversity, neg_diversity=[], []\n",
    "    pos_price, neg_price = [], []\n",
    "    test_bd_pos_discount, test_bd_neg_discount = [], []\n",
    "    train_dict, train_users, train_items  = data_to_dict(train_data)\n",
    "    test_dict, test_users, test_items = data_to_dict(test_data)\n",
    "    auc_values = []\n",
    "    z = 0\n",
    "    for i,user in enumerate(test_dict.keys()):\n",
    "        if(i%100000==0):\n",
    "            print i\n",
    "\n",
    "        if user in train_users: \n",
    "            for pos_item in test_dict[user]:\n",
    "                if pos_item in train_items:\n",
    "                    for neg_item in train_items:\n",
    "                        if neg_item not in test_dict[user] and neg_item not in train_dict[user]:\n",
    "                            pos_diversity.append(bundle_diversity_map[pos_item])\n",
    "                            neg_diversity.append(bundle_diversity_map[neg_item])\n",
    "                            users.append(user)\n",
    "                            pos_items.append(pos_item)\n",
    "                            neg_items.append(neg_item)\n",
    "                            n1.append(len(bundle_item_map[pos_item]))\n",
    "                            n2.append(len(bundle_item_map[neg_item]))\n",
    "                            pos_price.append(bundle_final_price_map[pos_item])\n",
    "                            neg_price.append(bundle_final_price_map[neg_item])\n",
    "                            test_bd_pos_discount.append(bundle_discount_map[pos_item])\n",
    "                            test_bd_neg_discount.append(bundle_discount_map[neg_item])\n",
    "                            \n",
    "\n",
    "    pos_bunldes = pos_items\n",
    "    neg_bundles = neg_items\n",
    "    pos_items=[get_items(b_id, max_bundle_size, n_items) for b_id in pos_bunldes]\n",
    "    neg_items=[get_items(b_id, max_bundle_size, n_items) for b_id in neg_bundles]\n",
    "    return users, pos_items, neg_items, n1, n2, pos_diversity, neg_diversity, \\\n",
    "            pos_price, neg_price, test_bd_pos_discount, test_bd_neg_discount, \\\n",
    "            pos_bunldes, neg_bundles\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "test_users, test_pos_items, test_neg_items, test_n1, test_n2, test_pos_diversity, test_neg_diversity,\\\n",
    "test_pos_price, test_neg_price, test_bd_pos_discount, test_bd_neg_discount, test_pos_bunldes, test_neg_bundles=\\\n",
    "                    get_test_data_bundles1(test_data, training_data, num_items)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22\n"
     ]
    }
   ],
   "source": [
    "print test_n1[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2819, 10)\n",
      "(2820, 10)\n",
      "(2819,)\n",
      "(2820,)\n"
     ]
    }
   ],
   "source": [
    "print np.shape(bpr_item.H.eval())\n",
    "H_item = np.concatenate((H_item,np.zeros((1,np.shape(H_item)[1]))),axis=0)\n",
    "H_item=np.array(H_item).astype('float32')\n",
    "print np.shape(H_item)\n",
    "\n",
    "print np.shape(bpr_item.B.eval())\n",
    "B_item=bpr_item.B.eval()\n",
    "B_item = np.append(B_item,0)\n",
    "B_item=np.array(B_item).astype('float32')\n",
    "print np.shape(B_item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class BPR_Cold(object):\n",
    "\n",
    "    def __init__(self, rank, bundle_size, n_users, n_items, lambda_u = 0.0025, lambda_i = 0.0025, lambda_j = 0.00025, lambda_d = 0.0025, lambda_p = 0.00025, lambda_bias = 0.0, learning_rate = 0.05):\n",
    "        \n",
    "        self._rank = rank\n",
    "        self._bundle_rank = bundle_size + 1\n",
    "        self._n_users = n_users\n",
    "        self._n_items = n_items\n",
    "        self._lambda_u = lambda_u\n",
    "        self._lambda_i = lambda_i\n",
    "        self._lambda_j = lambda_j\n",
    "        self._lambda_d = lambda_d\n",
    "        self._lambda_p = lambda_p\n",
    "        self._lambda_bias = lambda_bias\n",
    "        self._learning_rate = learning_rate\n",
    "        self._configure_theano()\n",
    "        self._generate_train_model_item_function()\n",
    "        self._generate_test_model_function()\n",
    "\n",
    "    def _configure_theano(self):\n",
    "        theano.config.mode = 'FAST_RUN'\n",
    "        theano.config.floatX = 'float32'\n",
    "    \n",
    "    def _generate_train_model_item_function(self):\n",
    "        u = T.lvector('u')\n",
    "        i = T.lmatrix('i')\n",
    "        j = T.lmatrix('j')\n",
    "        n1 = T.lvector('n1')\n",
    "        n2 = T.lvector('n2')\n",
    "        di = T.dvector('di')\n",
    "        dj = T.dvector('dj')\n",
    "        \n",
    "        self.W1 = bpr_item.W\n",
    "        self.H1 = theano.shared(H_item.astype('float32'), name='H')\n",
    "        self.B1 = theano.shared(B_item.astype('float32'), name='B')\n",
    "        \n",
    "        \n",
    "        self.M1 = theano.shared(numpy.random.random((self._rank, self._rank)).astype('float64'), name='M1')\n",
    "        self.M2 = theano.shared(numpy.random.random((self._rank, self._rank)).astype('float64'), name='M2')\n",
    "        self.K = theano.shared(numpy.random.rand(), name='K')\n",
    "        self.D = theano.shared(numpy.random.rand(), name='D')\n",
    "        self.N = theano.shared(numpy.random.random(self._bundle_rank).astype('float32'), name='N')\n",
    "        \n",
    "        x_ui = T.dot(T.dot(self.W1[u],self.M2), T.dot(self.M1, self.H1[i].sum(axis=1).T/n1)).diagonal() + self.K*(self.B1[i].T/n1).T.sum(axis=1) + self.N[n1] + self.D*di\n",
    "        x_uj = T.dot(T.dot(self.W1[u],self.M2), T.dot(self.M1, self.H1[j].sum(axis=1).T/n2)).diagonal() + self.K*(self.B1[j].T/n2).T.sum(axis=1) + self.N[n2] + self.D*dj\n",
    "        \n",
    "        x_uij = T.nnet.sigmoid(x_ui-x_uj)\n",
    "        obj = T.sum(T.log(x_uij) - self._lambda_u * (self.M1 ** 2).sum() - \\\n",
    "                    self._lambda_u * (self.M2 ** 2).sum()  - self._lambda_d * (self.K**2) - self._lambda_d * (self.D**2)\\\n",
    "                    -self._lambda_p * (self.N[n2]**2) - self._lambda_p * (self.N[n1]**2))\n",
    "        cost = - obj\n",
    "\n",
    "        g_cost_M1 = T.grad(cost=cost, wrt=self.M1)\n",
    "        g_cost_M2 = T.grad(cost=cost, wrt=self.M2)\n",
    "        g_cost_K = T.grad(cost=cost, wrt=self.K)\n",
    "        g_cost_N = T.grad(cost=cost, wrt=self.N)\n",
    "        g_cost_D = T.grad(cost=cost, wrt=self.D)\n",
    "        \n",
    "        updates = [(self.M1, self.M1 - self._learning_rate * .001* g_cost_M1), (self.M2, self.M2 - self._learning_rate *.001* g_cost_M2), \n",
    "                   (self.K, self.K - self._learning_rate * .001*g_cost_K), (self.N, self.N - self._learning_rate *g_cost_N),\n",
    "                  (self.D, self.D - self._learning_rate * g_cost_D)]\n",
    "\n",
    "        self.train_model_item = theano.function(inputs=[u, i, j, n1, n2, di, dj], outputs=cost, updates=updates)\n",
    "\n",
    "    \n",
    "    def train(self, s_users=None, s_pos_items=None, s_neg_items=None, s_pos_len=None, s_neg_len=None,\n",
    "             s_pos_diversity=None, s_neg_diversity=None,batch_size=1000):\n",
    "        \n",
    "        if len(s_users) < batch_size:\n",
    "            sys.stderr.write(\"WARNING: Batch size is greater than number of training samples, switching to a batch size of %s\\n\" % str(len(train_data)))\n",
    "            batch_size = len(s_users)\n",
    "        \n",
    "        sgd_users, sgd_pos_items, sgd_neg_items = s_users, s_pos_items, s_neg_items\n",
    "        n_sgd_samples = len(s_users)\n",
    "\n",
    "        z = 0\n",
    "        t2 = t1 = t0 = time.time()\n",
    "        while (z+1)*batch_size < n_sgd_samples:\n",
    "            \n",
    "            self.train_model_item(\n",
    "                sgd_users[z*batch_size: (z+1)*batch_size],\n",
    "                sgd_pos_items[z*batch_size: (z+1)*batch_size],\n",
    "                sgd_neg_items[z*batch_size: (z+1)*batch_size],\n",
    "                s_pos_len[z*batch_size: (z+1)*batch_size],\n",
    "                s_neg_len[z*batch_size: (z+1)*batch_size],\n",
    "                s_pos_diversity[z*batch_size: (z+1)*batch_size],\n",
    "                s_neg_diversity[z*batch_size: (z+1)*batch_size],\n",
    "            )\n",
    "            z += 1\n",
    "            t2 = time.time()\n",
    "            sys.stderr.write(\"\\rProcessed %s ( %.2f%% ) in %.4f seconds\" %(str(z*batch_size), 100.0 * float(z*batch_size)/n_sgd_samples, t2 - t1))\n",
    "            sys.stderr.flush()\n",
    "            t1 = t2\n",
    "        if n_sgd_samples > 0:\n",
    "            sys.stderr.write(\"\\nTotal training time %.2f seconds; %e per sample\\n\" % (t2 - t0, (t2 - t0)/n_sgd_samples))\n",
    "            sys.stderr.flush()\n",
    "    \n",
    "    def _generate_test_model_function(self):\n",
    "        u = T.lvector('u')\n",
    "        i = T.lmatrix('i')\n",
    "        j = T.lmatrix('j')\n",
    "        n1 = T.lvector('n1')\n",
    "        n2 = T.lvector('n2')\n",
    "        di = T.dvector('di')\n",
    "        dj = T.dvector('dj')\n",
    "        \n",
    "        x_ui = T.dot(T.dot(self.W1[u],self.M2), T.dot(self.M1, self.H1[i].sum(axis=1).T/n1)).diagonal() + self.K*(self.B1[i].T/n1).T.sum(axis=1) + self.N[n1] + self.D*di\n",
    "        x_uj = T.dot(T.dot(self.W1[u],self.M2), T.dot(self.M1, self.H1[j].sum(axis=1).T/n2)).diagonal() + self.K*(self.B1[j].T/n2).T.sum(axis=1) + self.N[n2] + self.D*dj \n",
    "        \n",
    "        x_uij = x_ui-x_uj\n",
    "        self.test_model = theano.function(inputs=[u, i, j, n1, n2, di, dj], outputs=x_uij)\n",
    "        \n",
    "    def test_bundle(self, sgd_users, sgd_pos_items, sgd_neg_items, s_pos_len, s_neg_len, s_pos_diversity, s_neg_diversity, batch_size=1000):\n",
    "        \n",
    "        auc_values = []\n",
    "        z = 0\n",
    "        t2 = t1 = t0 = time.time()\n",
    "        n_sgd_samples = len(sgd_users)\n",
    "        while (z+1)*batch_size < n_sgd_samples:\n",
    "            pref_list=self.test_model(\n",
    "                sgd_users[z*batch_size: (z+1)*batch_size],\n",
    "                sgd_pos_items[z*batch_size: (z+1)*batch_size],\n",
    "                sgd_neg_items[z*batch_size: (z+1)*batch_size],\n",
    "                s_pos_len[z*batch_size: (z+1)*batch_size],\n",
    "                s_neg_len[z*batch_size: (z+1)*batch_size],\n",
    "                s_pos_diversity[z*batch_size: (z+1)*batch_size],\n",
    "                s_neg_diversity[z*batch_size: (z+1)*batch_size]\n",
    "            )\n",
    "            z += 1\n",
    "            t2 = time.time()\n",
    "            sys.stderr.write(\"\\rProcessed %s ( %.2f%% ) in %.4f seconds\" %(str(z*batch_size), 100.0 * float(z*batch_size)/n_sgd_samples, t2 - t1))\n",
    "            t1 = t2\n",
    "            \n",
    "            auc = np.sum([1.0 if a>0.0 else 0.0 for a in pref_list])\n",
    "            auc /= batch_size\n",
    "            \n",
    "            auc_values.append(auc)\n",
    "            sys.stderr.write(\"\\rCurrent AUC mean (%s samples): %0.5f\" % (str(z*batch_size), numpy.mean(auc_values)))\n",
    "            sys.stderr.flush()\n",
    "        \n",
    "        sys.stderr.write(\"\\n\")\n",
    "        sys.stderr.flush()\n",
    "        return numpy.mean(auc_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bpr_cold = BPR_Cold(10, max_bundle_size, len(user_bundle_map.keys()), num_items)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed 2101000 ( 99.97% ) in 0.0664 seconds\n",
      "Total training time 151.57 seconds; 7.212076e-05 per sample\n"
     ]
    }
   ],
   "source": [
    "bpr_cold.train(s_users=sgd_users, s_pos_items=sgd_pos_items, s_neg_items=sgd_neg_items, \n",
    "         s_pos_len=sgd_pos_len, s_neg_len=sgd_neg_len, s_pos_diversity=sgd_pos_diversity,\n",
    "               s_neg_diversity=sgd_neg_diversity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Current AUC mean (3119000 samples): 0.65776nds\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.65775921769798007"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bpr_cold.test_bundle(test_users, test_pos_items, test_neg_items, test_n1, test_n2, test_pos_diversity, test_neg_diversity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.68585732689210954\n"
     ]
    }
   ],
   "source": [
    "print '0.68585732689210954'\n",
    "bundle_discount_array = [bundle_discount_map[i] for i in range(len(bundle_discount_map)) if i ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class BPR_Buncle(object):\n",
    "\n",
    "    def __init__(self, rank, bundle_size, n_users, n_items, \\\n",
    "                 itemID_price, user_price_means, user_price_stds,\\\n",
    "                 lambda_u = 0.0025, lambda_i = 0.0025, lambda_j = 0.00025, lambda_d = 0.0025, lambda_p = 0.00025, lambda_bias = 0.0, learning_rate = 0.05):\n",
    "        \n",
    "        self._rank = rank\n",
    "        self._bundle_rank = bundle_size + 1\n",
    "        self._n_users = n_users\n",
    "        self._n_items = n_items\n",
    "        self._lambda_u = lambda_u\n",
    "        self._lambda_i = lambda_i\n",
    "        self._lambda_j = lambda_j\n",
    "        self._lambda_d = lambda_d\n",
    "        self._lambda_p = lambda_p\n",
    "        self._lambda_bias = lambda_bias\n",
    "        self.itemID_price = theano.shared(np.array(itemID_price).astype('float32'), name='itemID_price')\n",
    "        self.user_price_mean = theano.shared(np.array(user_price_means).astype('float32'), name='user_price_mean')\n",
    "        self.user_price_std= theano.shared(np.array(user_price_stds).astype('float32'), name='user_price_std')\n",
    "        self._learning_rate = learning_rate\n",
    "        self._configure_theano()\n",
    "        self._generate_train_model_item_function()\n",
    "        self._generate_test_model_function()\n",
    "\n",
    "    def _configure_theano(self):\n",
    "        theano.config.mode = 'FAST_RUN'\n",
    "        theano.config.floatX = 'float32'\n",
    "    \n",
    "    def _generate_train_model_item_function(self):\n",
    "        u = T.lvector('u')\n",
    "        i = T.lmatrix('i')\n",
    "        j = T.lmatrix('j')\n",
    "        n1 = T.lvector('n1')\n",
    "        n2 = T.lvector('n2')\n",
    "        di = T.dvector('di')\n",
    "        dj = T.dvector('dj')\n",
    "        #itmes price\n",
    "#         pi = T.dvector('pi')\n",
    "#         pj = T.dvector('pj')\n",
    "        #bundle discount rate\n",
    "        bdi = T.dvector('bdi')\n",
    "        bdj = T.dvector('bdj')\n",
    "        #bundle id\n",
    "#         bi = T.lvector('bi')\n",
    "#         bj = T.lvector('bj')\n",
    "        \n",
    "        self.W1 = bpr_item.W\n",
    "        self.H1 = theano.shared(H_item.astype('float32'), name='H')\n",
    "        self.B1 = theano.shared(B_item.astype('float32'), name='B')\n",
    "        self.P = theano.shared(numpy.random.random(self._n_users).astype('float32'), name='P')\n",
    "        self.pbu = theano.shared(numpy.random.random(self._n_users).astype('float32'), name='Pbu')\n",
    "        self.pbx = theano.shared(numpy.random.random(self._bundle_rank).astype('float32'), name='Pbx')\n",
    "        \n",
    "        self.M1 = theano.shared(numpy.random.random((self._rank, self._rank)).astype('float64'), name='M1')\n",
    "        self.M2 = theano.shared(numpy.random.random((self._rank, self._rank)).astype('float64'), name='M2')\n",
    "        self.K = theano.shared(numpy.random.rand(), name='K')\n",
    "        self.D = theano.shared(numpy.random.rand(), name='D')\n",
    "        self.N = theano.shared(numpy.random.random(self._bundle_rank).astype('float32'), name='N')\n",
    "\n",
    "        \n",
    "        x_ui = T.dot(T.dot(self.W1[u],self.M2), T.dot(self.M1, self.H1[i].sum(axis=1).T/n1)).diagonal() +\\\n",
    "                self.K*(self.B1[i].T/n1).T.sum(axis=1) + self.N[n1] + self.D*di+\\\n",
    "                (self.pbu[u])* bdi\n",
    "        x_uj = T.dot(T.dot(self.W1[u],self.M2), T.dot(self.M1, self.H1[j].sum(axis=1).T/n2)).diagonal() +\\\n",
    "                self.K*(self.B1[j].T/n2).T.sum(axis=1) + self.N[n2] + self.D*dj+\\\n",
    "                (self.pbu[u])* bdj\n",
    "        \n",
    "        x_uij = T.nnet.sigmoid(x_ui -x_uj)\n",
    "        \n",
    "        obj = T.sum(T.log(x_uij) - self._lambda_u * (self.M1 ** 2).sum() - \\\n",
    "                    self._lambda_u * (self.M2 ** 2).sum()  - self._lambda_d * (self.K**2) - self._lambda_d * (self.D**2)\\\n",
    "                    -self._lambda_p * (self.N[n2]**2) - self._lambda_p * (self.N[n1]**2)\\\n",
    "                    -self._lambda_p * (self.pbu[u]**2))\n",
    "        cost = - obj\n",
    "\n",
    "        g_cost_M1 = T.grad(cost=cost, wrt=self.M1)\n",
    "        g_cost_M2 = T.grad(cost=cost, wrt=self.M2)\n",
    "        g_cost_K = T.grad(cost=cost, wrt=self.K)\n",
    "        g_cost_N = T.grad(cost=cost, wrt=self.N)\n",
    "        g_cost_D = T.grad(cost=cost, wrt=self.D)\n",
    "        g_cost_pbu = T.grad(cost=cost, wrt=self.pbu)\n",
    "        \n",
    "        updates = [(self.M1, self.M1 - self._learning_rate * .001* g_cost_M1), (self.M2, self.M2 - self._learning_rate *.001* g_cost_M2), \n",
    "                   (self.K, self.K - self._learning_rate * .001*g_cost_K), (self.N, self.N - self._learning_rate *g_cost_N),\n",
    "                  (self.D, self.D - self._learning_rate * g_cost_D),\\\n",
    "                  (self.Pbu, self.Pbu - self._learning_rate * g_cost_Pbu)]\n",
    "\n",
    "        self.train_model_item = theano.function(inputs=[u, i, j, n1, n2, di, dj, pi, pj, bdi, bdj, bi, bj], outputs=cost, updates=updates)\n",
    "\n",
    "    def _generate_test_model_function(self):\n",
    "        u = T.lvector('u')\n",
    "        i = T.lmatrix('i')\n",
    "        j = T.lmatrix('j')\n",
    "        n1 = T.lvector('n1')\n",
    "        n2 = T.lvector('n2')\n",
    "        di = T.dvector('di')\n",
    "        dj = T.dvector('dj')\n",
    "#         pi = T.dvector('pi')\n",
    "#         pj = T.dvector('pj')\n",
    "        bdi = T.dvector('bdi')\n",
    "        bdj = T.dvector('bdj')\n",
    "        #bundle id\n",
    "#         bi = T.lvector('bi')\n",
    "#         bj = T.lvector('bj')\n",
    "        \n",
    "        x_ui = T.dot(T.dot(self.W1[u],self.M2), T.dot(self.M1, self.H1[i].sum(axis=1).T/n1)).diagonal() +\\\n",
    "                self.K*(self.B1[i].T/n1).T.sum(axis=1) + self.N[n1] + self.D*di +\\\n",
    "                (self.pbu[u])* bdi\n",
    "        x_uj = T.dot(T.dot(self.W1[u],self.M2), T.dot(self.M1, self.H1[j].sum(axis=1).T/n2)).diagonal() +\\\n",
    "                self.K*(self.B1[j].T/n2).T.sum(axis=1) + self.N[n2] + self.D*dj  +\\\n",
    "                (self.pbu[u])* bdj\n",
    "        x_uij = x_ui -x_uj\n",
    "        \n",
    "        self.test_model = theano.function(inputs=[u, i, j, n1, n2, di, dj, bdi, bdj], outputs=x_uij)\n",
    "    \n",
    "    def train(self, s_users=None, s_pos_items=None, s_neg_items=None, s_pos_len=None, s_neg_len=None,\\\n",
    "              s_pos_diversity=None, s_neg_diversity=None, \\\n",
    "              bd_pos_discount = None, bd_neg_discount = None, \\\n",
    "              batch_size=1000):\n",
    "        \n",
    "        if len(s_users) < batch_size:\n",
    "            sys.stderr.write(\"WARNING: Batch size is greater than number of training samples, switching to a batch size of %s\\n\" % str(len(train_data)))\n",
    "            batch_size = len(s_users)\n",
    "        \n",
    "        sgd_users, sgd_pos_items, sgd_neg_items = s_users, s_pos_items, s_neg_items\n",
    "        n_sgd_samples = len(s_users)\n",
    "\n",
    "        z = 0\n",
    "        t2 = t1 = t0 = time.time()\n",
    "        while (z+1)*batch_size < n_sgd_samples:\n",
    "            self.train_model_item(\n",
    "                sgd_users[z*batch_size: (z+1)*batch_size],\n",
    "                sgd_pos_items[z*batch_size: (z+1)*batch_size],\n",
    "                sgd_neg_items[z*batch_size: (z+1)*batch_size],\n",
    "                s_pos_len[z*batch_size: (z+1)*batch_size],\n",
    "                s_neg_len[z*batch_size: (z+1)*batch_size],\n",
    "                s_pos_diversity[z*batch_size: (z+1)*batch_size],\n",
    "                s_neg_diversity[z*batch_size: (z+1)*batch_size],\n",
    "#                 items_pos_price[z*batch_size: (z+1)*batch_size],\n",
    "#                 items_neg_price[z*batch_size: (z+1)*batch_size],\n",
    "                bd_pos_discount[z*batch_size: (z+1)*batch_size],\n",
    "                bd_neg_discount[z*batch_size: (z+1)*batch_size]\n",
    "#                 bd_pos_discount[z*batch_size: (z+1)*batch_size],\n",
    "#                 bd_neg_discount[z*batch_size: (z+1)*batch_size]\n",
    "            )\n",
    "            z += 1\n",
    "            t2 = time.time()\n",
    "            sys.stderr.write(\"\\rProcessed %s ( %.2f%% ) in %.4f seconds\" %(str(z*batch_size), 100.0 * float(z*batch_size)/n_sgd_samples, t2 - t1))\n",
    "            sys.stderr.flush()\n",
    "            t1 = t2\n",
    "        if n_sgd_samples > 0:\n",
    "            sys.stderr.write(\"\\nTotal training time %.2f seconds; %e per sample\\n\" % (t2 - t0, (t2 - t0)/n_sgd_samples))\n",
    "            sys.stderr.flush()\n",
    "        \n",
    "    def test_bundle(self, sgd_users, sgd_pos_items, sgd_neg_items, s_pos_len, s_neg_len, s_pos_diversity, s_neg_diversity,\\\n",
    "                    bd_pos_discount, bd_neg_discount, batch_size=1000):\n",
    "        \n",
    "        auc_values = []\n",
    "        z = 0\n",
    "        t2 = t1 = t0 = time.time()\n",
    "        n_sgd_samples = len(sgd_users)\n",
    "        while (z+1)*batch_size < n_sgd_samples:\n",
    "            pref_list=self.test_model(\n",
    "                sgd_users[z*batch_size: (z+1)*batch_size],\n",
    "                sgd_pos_items[z*batch_size: (z+1)*batch_size],\n",
    "                sgd_neg_items[z*batch_size: (z+1)*batch_size],\n",
    "                s_pos_len[z*batch_size: (z+1)*batch_size],\n",
    "                s_neg_len[z*batch_size: (z+1)*batch_size],\n",
    "                s_pos_diversity[z*batch_size: (z+1)*batch_size],\n",
    "                s_neg_diversity[z*batch_size: (z+1)*batch_size],\n",
    "#                 items_pos_price[z*batch_size: (z+1)*batch_size],\n",
    "#                 items_neg_price[z*batch_size: (z+1)*batch_size],\n",
    "                bd_pos_discount[z*batch_size: (z+1)*batch_size],\n",
    "                bd_neg_discount[z*batch_size: (z+1)*batch_size]\n",
    "            )\n",
    "            z += 1\n",
    "            t2 = time.time()\n",
    "            sys.stderr.write(\"\\rProcessed %s ( %.2f%% ) in %.4f seconds\" %(str(z*batch_size), 100.0 * float(z*batch_size)/n_sgd_samples, t2 - t1))\n",
    "            t1 = t2\n",
    "            \n",
    "            auc = np.sum([1.0 if a>0.0 else 0.0 for a in pref_list])\n",
    "            auc /= batch_size\n",
    "            \n",
    "            auc_values.append(auc)\n",
    "            sys.stderr.write(\"\\rCurrent AUC mean (%s samples): %0.5f\" % (str(z*batch_size), numpy.mean(auc_values)))\n",
    "            sys.stderr.flush()\n",
    "        \n",
    "        sys.stderr.write(\"\\n\")\n",
    "        sys.stderr.flush()\n",
    "        return numpy.mean(auc_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "bpr_bundle = BPR_Buncle(10, max_bundle_size, len(user_bundle_map.keys()), num_items, \\\n",
    "                        itemID_price, user_price_means, user_price_stds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bpr_bundle.train(s_users=sgd_users, s_pos_items=sgd_pos_items, s_neg_items=sgd_neg_items, \n",
    "         s_pos_len=sgd_pos_len, s_neg_len=sgd_neg_len, s_pos_diversity=sgd_pos_diversity,\n",
    "               s_neg_diversity=sgd_neg_diversity, \\\n",
    "                 bd_pos_discount = sgd_pos_dicount, bd_neg_discount = sgd_neg_dicount)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bpr_bundle.test_bundle(test_users, test_pos_items, test_neg_items, test_n1, test_n2,\\\n",
    "                       test_pos_diversity, test_neg_diversity, \\\n",
    "                       test_bd_pos_discount, test_bd_neg_discount)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print 0.73830120101137786 + 'discount devrivate'\n",
    "print 0.72651723027375203 + 'no discount derivaite'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = np.eye(5, dtype=theano.config.floatX)\n",
    "y = 2 * np.ones((5,5), dtype=theano.config.floatX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[473, 2819, 2819, 2819, 2819, 2819, 2819, 2819, 2819, 2819, 2819, 2819, 2819, 2819, 2819, 2819, 2819, 2819, 2819, 2819, 2819, 2819, 2819, 2819, 2819, 2819, 2819, 2819, 2819, 2819, 2819, 2819, 2819, 2819, 2819, 2819, 2819, 2819, 2819, 2819, 2819, 2819, 2819, 2819, 2819, 2819, 2819, 2819, 2819, 2819, 2819, 2819, 2819, 2819, 2819, 2819, 2819, 2819, 2819, 2819, 2819, 2819, 2819, 2819, 2819, 2819, 2819, 2819, 2819, 2819, 2819, 2819, 2819, 2819, 2819, 2819, 2819, 2819, 2819, 2819, 2819, 2819, 2819, 2819, 2819, 2819, 2819, 2819]\n"
     ]
    }
   ],
   "source": [
    "print sgd_pos_items[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class BPR_Buncle1(object):\n",
    "\n",
    "    def __init__(self, rank, bundle_size, n_users, n_items, \\\n",
    "                 itemID_price, user_price_means, user_price_stds, bunlde_max_id,\\\n",
    "                 lambda_u = 0.0025, lambda_i = 0.0025, lambda_j = 0.00025, lambda_d = 0.0025, lambda_p = 0.00025, lambda_bias = 0.0, learning_rate = 0.05):\n",
    "        \n",
    "        self._rank = rank\n",
    "        self._bundle_rank = bundle_size + 1\n",
    "        self._n_bundles = bunlde_max_id + 1\n",
    "        self._n_users = n_users\n",
    "        self._n_items = n_items\n",
    "        self._lambda_u = lambda_u\n",
    "        self._lambda_i = lambda_i\n",
    "        self._lambda_j = lambda_j\n",
    "        self._lambda_d = lambda_d\n",
    "        self._lambda_p = lambda_p\n",
    "        self._lambda_bias = lambda_bias\n",
    "        self.itemID_price = theano.shared(np.array(itemID_price).astype('float32'), name='itemID_price')\n",
    "        self.user_price_mean = theano.shared(np.array(user_price_means).astype('float32'), name='user_price_mean')\n",
    "        self.user_price_std= theano.shared(np.array(user_price_stds).astype('float32'), name='user_price_std')\n",
    "        self._learning_rate = learning_rate\n",
    "        self._configure_theano()\n",
    "        self._generate_train_model_item_function()\n",
    "        self._generate_test_model_function()\n",
    "\n",
    "    def _configure_theano(self):\n",
    "        theano.config.mode = 'FAST_RUN'\n",
    "        theano.config.floatX = 'float32'\n",
    "    \n",
    "    def _generate_train_model_item_function(self):\n",
    "        u = T.lvector('u')\n",
    "        i = T.lmatrix('i')\n",
    "        j = T.lmatrix('j')\n",
    "        n1 = T.lvector('n1')\n",
    "        n2 = T.lvector('n2')\n",
    "        di = T.dvector('di')\n",
    "        dj = T.dvector('dj')\n",
    "        #itmes price\n",
    "        pi = T.dvector('pi')\n",
    "        pj = T.dvector('pj')\n",
    "        #bundle discount rate\n",
    "        bdi = T.dvector('bdi')\n",
    "        bdj = T.dvector('bdj')\n",
    "        #bundle id\n",
    "        bi = T.lvector('bi')\n",
    "        bj = T.lvector('bj')\n",
    "        \n",
    "        self.W1 = bpr_item.W\n",
    "        self.H1 = theano.shared(H_item.astype('float32'), name='H')\n",
    "        self.B1 = theano.shared(B_item.astype('float32'), name='B')\n",
    "        self.P = theano.shared(numpy.random.random(self._n_users).astype('float32'), name='P')\n",
    "        self.pbu = theano.shared(numpy.random.random(self._n_users).astype('float32'), name='Pbu')\n",
    "        self.pbx = theano.shared(numpy.random.random(self._n_bundles).astype('float32'), name='Pbx')\n",
    "        \n",
    "        self.M1 = theano.shared(numpy.random.random((self._rank, self._rank)).astype('float64'), name='M1')\n",
    "        self.M2 = theano.shared(numpy.random.random((self._rank, self._rank)).astype('float64'), name='M2')\n",
    "        self.K = theano.shared(numpy.random.rand(), name='K')\n",
    "        self.D = theano.shared(numpy.random.rand(), name='D')\n",
    "        self.N = theano.shared(numpy.random.random(self._bundle_rank).astype('float32'), name='N')\n",
    "\n",
    "        \n",
    "        x_ui = T.dot(T.dot(self.W1[u],self.M2), T.dot(self.M1, self.H1[i].sum(axis=1).T/n1)).diagonal() +\\\n",
    "                self.K*(self.B1[i].T/n1).T.sum(axis=1) + self.N[n1] + self.D*di+\\\n",
    "                (self.pbu[u] * self.pbx[bi])* bdi\n",
    "        x_uj = T.dot(T.dot(self.W1[u],self.M2), T.dot(self.M1, self.H1[j].sum(axis=1).T/n2)).diagonal() +\\\n",
    "                self.K*(self.B1[j].T/n2).T.sum(axis=1) + self.N[n2] + self.D*dj+\\\n",
    "                (self.pbu[u] * self.pbx[bj])* bdj\n",
    "        \n",
    "        x_uij = T.nnet.sigmoid(x_ui -x_uj - self.P[u] * (pi  - pj))\n",
    "        \n",
    "        obj = T.sum(T.log(x_uij) - self._lambda_u * (self.M1 ** 2).sum() - \\\n",
    "                    self._lambda_u * (self.M2 ** 2).sum()  - self._lambda_d * (self.K**2) - self._lambda_d * (self.D**2)\\\n",
    "                    -self._lambda_p * (self.N[n2]**2) - self._lambda_p * (self.N[n1]**2)\\\n",
    "                    -self._lambda_p * (self.P[u]**2 + self.pbu[u]**2 + self.pbx[bi]**2 + self.pbx[bj]**2  ))\n",
    "        cost = - obj\n",
    "\n",
    "        g_cost_M1 = T.grad(cost=cost, wrt=self.M1)\n",
    "        g_cost_M2 = T.grad(cost=cost, wrt=self.M2)\n",
    "        g_cost_K = T.grad(cost=cost, wrt=self.K)\n",
    "        g_cost_N = T.grad(cost=cost, wrt=self.N)\n",
    "        g_cost_D = T.grad(cost=cost, wrt=self.D)\n",
    "        g_cost_P = T.grad(cost=cost, wrt=self.P)\n",
    "        g_cost_pbu = T.grad(cost=cost, wrt=self.pbu)\n",
    "        g_cost_pbx = T.grad(cost=cost, wrt=self.pbx)\n",
    "        \n",
    "        updates = [(self.M1, self.M1 - self._learning_rate * .001* g_cost_M1), (self.M2, self.M2 - self._learning_rate *.001* g_cost_M2), \n",
    "                   (self.K, self.K - self._learning_rate * .001*g_cost_K), (self.N, self.N - self._learning_rate *g_cost_N),\n",
    "                  (self.D, self.D - self._learning_rate * g_cost_D),\\\n",
    "                  (self.P, self.P - self._learning_rate * g_cost_P),\\\n",
    "                  (self.pbu, self.pbu - self._learning_rate * g_cost_pbu),\\\n",
    "                  (self.pbx, self.pbx - self._learning_rate * g_cost_pbx)]\n",
    "\n",
    "        self.train_model_item = theano.function(inputs=[u, i, j, n1, n2, di, dj, pi, pj, bdi, bdj, bi, bj], outputs=cost, updates=updates)\n",
    "\n",
    "    def _generate_test_model_function(self):\n",
    "        u = T.lvector('u')\n",
    "        i = T.lmatrix('i')\n",
    "        j = T.lmatrix('j')\n",
    "        n1 = T.lvector('n1')\n",
    "        n2 = T.lvector('n2')\n",
    "        di = T.dvector('di')\n",
    "        dj = T.dvector('dj')\n",
    "        pi = T.dvector('pi')\n",
    "        pj = T.dvector('pj')\n",
    "        bdi = T.dvector('bdi')\n",
    "        bdj = T.dvector('bdj')\n",
    "        #bundle id\n",
    "        bi = T.lvector('bi')\n",
    "        bj = T.lvector('bj')\n",
    "        \n",
    "        x_ui = T.dot(T.dot(self.W1[u],self.M2), T.dot(self.M1, self.H1[i].sum(axis=1).T/n1)).diagonal() +\\\n",
    "                self.K*(self.B1[i].T/n1).T.sum(axis=1) + self.N[n1] + self.D*di+\\\n",
    "                (self.pbu[u] * self.pbx[bi])* bdi\n",
    "        x_uj = T.dot(T.dot(self.W1[u],self.M2), T.dot(self.M1, self.H1[j].sum(axis=1).T/n2)).diagonal() +\\\n",
    "                self.K*(self.B1[j].T/n2).T.sum(axis=1) + self.N[n2] + self.D*dj+\\\n",
    "                (self.pbu[u] * self.pbx[bj])* bdj\n",
    "        x_uij = x_ui -x_uj - self.P[u] * (pi - pj)\n",
    "        \n",
    "        self.test_model = theano.function(inputs=[u, i, j, n1, n2, di, dj, pi, pj, bdi, bdj, bi, bj], outputs=x_uij)\n",
    "    \n",
    "    def train(self, s_users=None, s_pos_items=None, s_neg_items=None, s_pos_len=None, s_neg_len=None,\\\n",
    "              s_pos_diversity=None, s_neg_diversity=None, items_pos_price = None, items_neg_price = None,\\\n",
    "              bd_pos_discount = None, bd_neg_discount = None, pos_bundles = None, neg_bundles = None,\\\n",
    "              pos_bundle = None, neg_bundle = None, batch_size=1000):\n",
    "        \n",
    "        if len(s_users) < batch_size:\n",
    "            sys.stderr.write(\"WARNING: Batch size is greater than number of training samples, switching to a batch size of %s\\n\" % str(len(train_data)))\n",
    "            batch_size = len(s_users)\n",
    "        \n",
    "        sgd_users, sgd_pos_items, sgd_neg_items = s_users, s_pos_items, s_neg_items\n",
    "        n_sgd_samples = len(s_users)\n",
    "\n",
    "        z = 0\n",
    "        t2 = t1 = t0 = time.time()\n",
    "        while (z+1)*batch_size < n_sgd_samples:\n",
    "            self.train_model_item(\n",
    "                sgd_users[z*batch_size: (z+1)*batch_size],\n",
    "                sgd_pos_items[z*batch_size: (z+1)*batch_size],\n",
    "                sgd_neg_items[z*batch_size: (z+1)*batch_size],\n",
    "                s_pos_len[z*batch_size: (z+1)*batch_size],\n",
    "                s_neg_len[z*batch_size: (z+1)*batch_size],\n",
    "                s_pos_diversity[z*batch_size: (z+1)*batch_size],\n",
    "                s_neg_diversity[z*batch_size: (z+1)*batch_size],\n",
    "                items_pos_price[z*batch_size: (z+1)*batch_size],\n",
    "                items_neg_price[z*batch_size: (z+1)*batch_size],\n",
    "                bd_pos_discount[z*batch_size: (z+1)*batch_size],\n",
    "                bd_neg_discount[z*batch_size: (z+1)*batch_size],\n",
    "                pos_bundle[z*batch_size: (z+1)*batch_size],\n",
    "                neg_bundle[z*batch_size: (z+1)*batch_size]\n",
    "            )\n",
    "            z += 1\n",
    "            t2 = time.time()\n",
    "            sys.stderr.write(\"\\rProcessed %s ( %.2f%% ) in %.4f seconds\" %(str(z*batch_size), 100.0 * float(z*batch_size)/n_sgd_samples, t2 - t1))\n",
    "            sys.stderr.flush()\n",
    "            t1 = t2\n",
    "        if n_sgd_samples > 0:\n",
    "            sys.stderr.write(\"\\nTotal training time %.2f seconds; %e per sample\\n\" % (t2 - t0, (t2 - t0)/n_sgd_samples))\n",
    "            sys.stderr.flush()\n",
    "        \n",
    "    def test_bundle(self, sgd_users, sgd_pos_items, sgd_neg_items, s_pos_len, s_neg_len, s_pos_diversity, s_neg_diversity,\\\n",
    "                    items_pos_price, items_neg_price, bd_pos_discount, bd_neg_discount, \\\n",
    "                    pos_bundle, neg_bundle,batch_size=1000):\n",
    "        \n",
    "        auc_values = []\n",
    "        z = 0\n",
    "        t2 = t1 = t0 = time.time()\n",
    "        n_sgd_samples = len(sgd_users)\n",
    "        while (z+1)*batch_size < n_sgd_samples:\n",
    "            pref_list=self.test_model(\n",
    "                sgd_users[z*batch_size: (z+1)*batch_size],\n",
    "                sgd_pos_items[z*batch_size: (z+1)*batch_size],\n",
    "                sgd_neg_items[z*batch_size: (z+1)*batch_size],\n",
    "                s_pos_len[z*batch_size: (z+1)*batch_size],\n",
    "                s_neg_len[z*batch_size: (z+1)*batch_size],\n",
    "                s_pos_diversity[z*batch_size: (z+1)*batch_size],\n",
    "                s_neg_diversity[z*batch_size: (z+1)*batch_size],\n",
    "                items_pos_price[z*batch_size: (z+1)*batch_size],\n",
    "                items_neg_price[z*batch_size: (z+1)*batch_size],\n",
    "                bd_pos_discount[z*batch_size: (z+1)*batch_size],\n",
    "                bd_neg_discount[z*batch_size: (z+1)*batch_size],\n",
    "                pos_bundle[z*batch_size: (z+1)*batch_size],\n",
    "                neg_bundle[z*batch_size: (z+1)*batch_size]\n",
    "            )\n",
    "            z += 1\n",
    "            t2 = time.time()\n",
    "            sys.stderr.write(\"\\rProcessed %s ( %.2f%% ) in %.4f seconds\" %(str(z*batch_size), 100.0 * float(z*batch_size)/n_sgd_samples, t2 - t1))\n",
    "            t1 = t2\n",
    "            \n",
    "            auc = np.sum([1.0 if a>0.0 else 0.0 for a in pref_list])\n",
    "            auc /= batch_size\n",
    "            \n",
    "            auc_values.append(auc)\n",
    "            sys.stderr.write(\"\\rCurrent AUC mean (%s samples): %0.5f\" % (str(z*batch_size), numpy.mean(auc_values)))\n",
    "            sys.stderr.flush()\n",
    "        \n",
    "        sys.stderr.write(\"\\n\")\n",
    "        sys.stderr.flush()\n",
    "        return numpy.mean(auc_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bunlde_max_id = max([x for x in sgd_pos_bundles])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bpr_bundle = BPR_Buncle1(10, max_bundle_size, len(user_bundle_map.keys()), num_items, \\\n",
    "                        itemID_price, user_price_means, user_price_stds, bunlde_max_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed 2101000 ( 99.97% ) in 0.1305 seconds\n",
      "Total training time 183.60 seconds; 8.736395e-05 per sample\n"
     ]
    }
   ],
   "source": [
    "bpr_bundle.train(s_users=sgd_users, s_pos_items=sgd_pos_items, s_neg_items=sgd_neg_items, \n",
    "         s_pos_len=sgd_pos_len, s_neg_len=sgd_neg_len, s_pos_diversity=sgd_pos_diversity,\n",
    "               s_neg_diversity=sgd_neg_diversity, \\\n",
    "                 items_pos_price = sgd_pos_price, items_neg_price = sgd_neg_price,\\\n",
    "                 bd_pos_discount = sgd_pos_dicount, bd_neg_discount = sgd_neg_dicount,\\\n",
    "                 pos_bundle = sgd_pos_bundles, neg_bundle = sgd_neg_bundles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'test_pos_bundles' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-116-fe961003b888>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mprint\u001b[0m \u001b[0mtest_pos_bundles\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'test_pos_bundles' is not defined"
     ]
    }
   ],
   "source": [
    "print test_pos_bundles[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bpr_bundle.test_bundle(test_users, test_pos_items, test_neg_items, test_n1, test_n2,\\\n",
    "                       test_pos_diversity, test_neg_diversity, \\\n",
    "                       test_pos_price, test_neg_price, test_bd_pos_discount, test_bd_neg_discount\\\n",
    "                       test_pos_bundles, test_neg_bundles)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:py2]",
   "language": "python",
   "name": "conda-env-py2-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
